{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$('#menubar').toggle();\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$('#menubar').toggle();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science and Systems \n",
    "# 2020-2021\n",
    "\n",
    "## Lecture Week 2: Introduction, Vectors and Matricies (and a bit of `numpy`)\n",
    "\n",
    "\n",
    " ##### University of Glasgow - material prepared by John H. Williamson (adapted to IDSS by BSJ), <small>v20202021B</small>\n",
    " \n",
    "---- \n",
    " $$\\newcommand{\\vec}[1]{{\\bf #1}} \n",
    " \\newenvironment{examinable}{}{{}}\n",
    "\\newcommand{\\real}{\\mathbb{R}}\n",
    "\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "%\\vec{x}\n",
    "%\\real\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "By the end of this unit you should know:\n",
    "* what a vector is and a what a vector space is\n",
    "* the standard operations on vectors: addition and multiplication\n",
    "* what a norm is and how it can be used to measure vectors\n",
    "* what an inner product is and how it gives rise to geometry of vectors\n",
    "* how mathematical vectors map onto numerical arrays\n",
    "* the different p-norms and their uses\n",
    "* important computational uses of vector representations\n",
    "* how to characterise vector data with a mean vector and a covariance matrix\n",
    "* the properties of high-dimensional vector spaces\n",
    "* the basic notation for matrices\n",
    "* the view of matrices as linear maps\n",
    "* how basic geometric transforms are implemented using matrices\n",
    "* how matrix multiplication is defined and its algebraic properties\n",
    "* the basic anatomy of matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<script>\n",
       "  function code_toggle() {\n",
       "    if (code_shown){\n",
       "      $('div.input').hide('500');\n",
       "      $('#toggleButton').val('Show Code')\n",
       "    } else {\n",
       "      $('div.input').show('500');\n",
       "      $('#toggleButton').val('Hide Code')\n",
       "    }\n",
       "    code_shown = !code_shown\n",
       "  }\n",
       "\n",
       "  $( document ).ready(function(){\n",
       "    code_shown=false;\n",
       "    $('div.input').hide()\n",
       "  });\n",
       "</script>\n",
       "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display\n",
    "IPython.display.HTML(\"\"\"\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T12:16:02.400120Z",
     "start_time": "2020-10-01T12:16:00.980368Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jhwutils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjhwutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatrices\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_matrix_effect, print_matrix\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mrc(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfigure\u001b[39m\u001b[38;5;124m'\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6.0\u001b[39m, \u001b[38;5;241m5.0\u001b[39m), dpi\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m240\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'jhwutils'"
     ]
    }
   ],
   "source": [
    "# standard imports\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from jhwutils.matrices import show_matrix_effect, print_matrix\n",
    "plt.rc('figure', figsize=(6.0, 5.0), dpi=240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Text  and translation\n",
    "Text, as represented by strings in memory, has *weak structure*. There are **comparison functions** for strings (e.g. edit distance, Hamming distance) but the distance between two strings captures only character-level semantics. **String operations** are character-level operations like concatenation or reversal. These string operations are fine for building spell-checkers, but they aren't much use for building a machine translation system.\n",
    "\n",
    "\n",
    "<img src=\"imgs/dictionary.jpg\"> \n",
    "<br><br>*[[Image](https://flickr.com/photos/horiavarlan/4268897748 \"Spanish dictionary pages up into the air\") by [Horia Varlan](https://flickr.com/people/horiavarlan) shared [CC BY](https://creativecommons.org/licenses/by/2.0/)]*\n",
    "\n",
    "\n",
    "## Words aren't enough\n",
    "Looking up words in the dictionary doesn't work as translation model, no matter how brilliant your text comparison algorithms are:\n",
    "\n",
    "* *Original*\n",
    "\n",
    ">    \"The craft held fast to the bank of the burn.\"\n",
    ">\n",
    ">    *(the vessel stayed moored to the edge of the stream)*\n",
    "    \n",
    "*  *Dictionary lookup (naiive)*\n",
    "\n",
    "> French:\n",
    ">    \"L'artisanat tenu rapide à la Banque de la brûlure.\" \n",
    ">\n",
    ">    *(the artisanal skill held quickly to the financial institution of the burn wounds)*\n",
    ">\n",
    "> Danish:\n",
    ">    \"Håndværket holdt fast til banken af brænden\"  \n",
    ">\n",
    ">    *(The craftmanship held fast to the bank [financial institution] of fire)*\n",
    "   \n",
    "* *Correct(ish)*\n",
    "\n",
    "> French:\n",
    ">    \"Le bateau se tenait fermement à la rive du ruisseau.\" \n",
    ">\n",
    ">    *(the boat was firmly attached to the riverbank)*\n",
    "> \n",
    "> Danish:\n",
    ">    \"Farttøjet holdt fast i bredden af åen\" \n",
    ">\n",
    ">    *(The vessel held fast at the bank of the river)*\n",
    "\n",
    "\n",
    "An effective approach is to imbue text fragments with additional mathematical structure -- **to place them in a vector space**. Fragments might be words, partial words or whole sentences. This is called **embedding** and algorithms such as [Word2vec](https://en.wikipedia.org/wiki/Word2vec) can learn a transformation from strings to high dimensional vectors (typically >100D) simply by observing large amounts of text.\n",
    "<img src=\"imgs/word2vec.png\">\n",
    "*[Image: word2vec example, from https://www.tensorflow.org/tutorials/word2vec, licensed Apache 2.0]*\n",
    "\n",
    "Because this data has the structure of a (topological) vector space, it is possible answer computationally questions like:\n",
    "* what words are like `salamander`? (i.e. which vectors are in the neighbourhood of the vector corresponding to `salamander`), which might include words like `axolotl` or `waterdog`.\n",
    "* What is the equivalent of a `king`, but with a `woman` instead of a `man`? Famously, the original word2vec paper should that on their test data, the equation \n",
    "$$\\text{King} - \\text{Man} + \\text{Woman} = \\text{Queen}$$ held, where addition is defined as vector addition.\n",
    "\n",
    "Although each dimension of the space has no obvious meaning, the embedding means that **semantics are mapped onto spatial relations**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vector spaces\n",
    "In this course, we will consider vectors to be ordered tuples of real numbers $[x_1, x_2, \\dots x_n], x_i \\in \\mathbb{R}$ (the concept generalises to complex numbers, finite fields, etc. but we'll ignore that). A vector has a fixed dimension $n$, which is the length of the tuple. We can imagine each element of the vector as representing a distance in an **direction orthogonal** to all the other elements.\n",
    "\n",
    "For example, a length-3 vector might be used to represent a spatial position in Cartesian coordinates, with three orthogonal measurements for each vector. Orthogonal just means \"independent\", or, geometrically speaking \"at 90 degrees\".\n",
    "\n",
    "\n",
    "* Consider the 3D vector [5, 7, 3]. This is a point in $\\real^3$, which is formed of:\n",
    "\n",
    "            5 * [1,0,0] +\n",
    "            7 * [0,1,0] +\n",
    "            3 * [0,0,1]\n",
    "            \n",
    "Each of these vectors [1,0,0], [0,1,0], [0,0,1] is pointing in a independent direction (orthogonal direction) and has length one. The vector [5,7,3]\n",
    "can be thought of a weighted sum of these orthogonal unit vectors (called **\"basis vectors\"**). The vector space has three independent bases, and so is three dimensional.\n",
    "\n",
    "We write vectors with a bold lower case letter:\n",
    "$$\\vec{x} = [x_1, x_2, \\dots, x_d],\\\\\n",
    "\\vec{y} = [y_1, y_2, \\dots, y_d],$$ and so on.\n",
    "\n",
    "## Points in space\n",
    "\n",
    "### Notation: $\\real^n$\n",
    "\n",
    "* $\\real$ means the set of real numbers.  \n",
    "* $\\real_{\\geq 0}$ means the set of non-negative reals.  \n",
    "* $\\real^n$ means the set of tuples of exactly $n$ real numbers. \n",
    "* $\\real^{n\\times m}$ means the set of 2D arrays (matrix) of real numbers with exactly $n$ rows of $m$ elements.\n",
    "\n",
    "* The notation $(\\real^n, \\real^n) \\rightarrow \\real$ says that than operation defines a map from a pair of $n$ dimensional vectors to a real number.\n",
    "\n",
    "### Vector spaces\n",
    "Any vector of given dimension $n$ lies in a **vector space**, called $\\real^n$ (we will only deal with finite-dimensional real vector spaces with standard bases), which is the set of possible vectors of length $n$ having real elements, along with the operations of: \n",
    "*  **scalar multiplication** so that $a{\\bf x}$  is defined for any scalar $a$. For real vectors, $a{\\bf x} = [a x_1, a x_2, \\dots a x_n]$, elementwise scaling.\n",
    "    * $(\\real, \\real^n) \\rightarrow \\real^n$\n",
    "* **vector addition** so that ${\\bf x} + {\\bf y}$ vectors ${\\bf x, y}$ of equal dimension. For real vectors, ${\\bf x} + {\\bf y} = [x_1 + y_1, x_2 + y_2, \\dots x_d + y_d]$ the elementwise sum\n",
    "    * $(\\real^n, \\real^n) \\rightarrow \\real^n$\n",
    "\n",
    "\n",
    "We will consider vector spaces which are equipped with two additional operations:\n",
    "* a **norm** $||{\\bf x}||$ which allows the length of vectors to be measured.\n",
    "    * $\\real_n \\rightarrow \\real_{\\geq 0}$\n",
    "* an **inner product** $\\langle {\\bf x} | {\\bf y} \\rangle$ or ${\\bf x \\bullet y}$  which allows the angles of two vectors to be compared. The inner product of two orthogonal vectors is 0. For real vectors ${\\bf x}\\bullet{\\bf y} = x_1 y_1 + x_2 y_2 + x_3 y_3 \\dots x_d y_d$\n",
    "    * $(\\real^n, \\real^n) \\rightarrow \\real$\n",
    "\n",
    "All operations between vectors are defined within a vector space. We cannot, for example, add two vectors of different dimension, as they are elements of different spaces.\n",
    "\n",
    "#### Topological and inner product spaces\n",
    "With a norm a vector space is a **topological vector space**. This means that the space is continuous, and it makes sense to talk about vectors being \"close together\" or a vector having a neighbourhood around it. With an inner product, a vector space is an **inner product space**, and we can talk about the angle between two vectors.\n",
    "\n",
    "<img src=\"imgs/vectors.png\" width=\"60%\">\n",
    "\n",
    "### Are vectors points in space, arrows pointing from the origin, or tuples of numbers?\n",
    "These are all valid ways of thinking about vectors. Most high school mathematics uses the \"arrows\" view of vectors. Computationally, the tuple of numbers is the *representation* we use. The \"points in space\" mental model is probably the most useful, but some operations are easier to understand from the alternative perspectives. \n",
    "\n",
    "The points mental model is the most useful *because* we tend to view:\n",
    "* vectors to represent *data*; data lies in space\n",
    "* matrices to represent *operations* on data; matrices warp space.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relation to arrays\n",
    "These vectors of real numbers can be represented by the 1D floating point arrays we called \"vectors\" in the self-study. But be careful; the representation and the mathematical element are different things, just as floating point numbers are not real numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T12:29:52.390253Z",
     "start_time": "2020-10-01T12:29:52.381278Z"
    }
   },
   "outputs": [],
   "source": [
    "# two 3D vectors (3 element ordered tuples)\n",
    "x = np.array([0,1,1])\n",
    "y = np.array([4,5,6])\n",
    "a = 2\n",
    "print_matrix(\"a\", a)\n",
    "print_matrix(\"x\", x)\n",
    "print_matrix(\"y\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T12:30:31.389133Z",
     "start_time": "2020-10-01T12:30:31.379160Z"
    }
   },
   "outputs": [],
   "source": [
    "print_matrix(\"ax\", a*x) \n",
    "print_matrix(\"ay\", a*y) \n",
    "print_matrix(\"x+y\", x+y) \n",
    "print_matrix(\"\\|x\\|_2\", np.linalg.norm(x)) # norm\n",
    "print_matrix(\"x\\cdot y\", np.dot(x,y))      # inner product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uses of vectors\n",
    "Vectors, despite their apparently simple nature, are enormously important throughout data science. They are a *lingua franca* for data. Because vectors can be \n",
    "* **composed** (via addition), \n",
    "* **compared** (via norms/inner products) \n",
    "* and **weighted** (by scaling), \n",
    "\n",
    "they can represent many of the kinds of transformations we want to be able to do to data.\n",
    "\n",
    "On top of this, they map onto the efficient **ndarray** data structure, so we can operate on them efficiently and concisely.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector data\n",
    "Datasets are commonly stored as 2D **tables**. These can be seen as lists of vectors. Each **row** is a vector representing an \"observation\" (e.g. the fluid flow reading in 10 pipes might become a 10 element vector). Each observation is then stacked up in a 2D matrix. Each **column** represents one element of the vector across many observations.\n",
    "\n",
    "We have seen many datasets like this (synthetic) physiological dataset:\n",
    "\n",
    "    heart_rate systolic diastolic vo2 \n",
    "     67           110    72       98\n",
    "     65           111    70       98\n",
    "     64           110    69       97\n",
    "     ..\n",
    "     \n",
    "Each **row** can be seen as a vector in $\\real^n$ (in $\\real^4$ for this set of physiological measurements). The whole matrix is a sequence of vectors in the same vector space. This means we can make **geometric** statements about tabular data.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric operations\n",
    "The most obvious use of vectors is to represent 2D or 3D geometric data. Almost all the computation in a modern computer game or 3D rendering engine is made up of low dimensional vector operations (2D, 3D, or 4D) repeated at enormous scale. \n",
    "    \n",
    "\n",
    "<img src=\"imgs/spaceship.png\">\n",
    "\n",
    "*Image: geometry made up of faces (*blue*), defined by edges (*white*) which connect together vertices (*reddish*). Vertices are points, or rather vectors in a vector space. The whole model can be moved and rotated by applying an identical operation to each vertex.*\n",
    "\n",
    "The *Cobra Mk. III* spaceship model above is defined by these vectors specifying the vertices in 3D space:\n",
    "\n",
    "         [[ -0.   15.    0. ]\n",
    "         [ 16.   -0.5  32.5]\n",
    "         [-16.   -0.5  32.5]\n",
    "         [ 16.  -15.  -32.5]\n",
    "         [-16.  -15.  -32.5]\n",
    "         [-44.   10.  -32.5]\n",
    "         [-60.   -3.  -13. ]\n",
    "         [-65.   -3.  -32.5]\n",
    "         [ 44.   10.  -32.5]\n",
    "         [ 60.   -3.  -13. ]\n",
    "         [ 65.   -3.  -32.5]\n",
    "         [ -0.   15.  -32.5]]\n",
    "\n",
    "---\n",
    "\n",
    "Standard transformations in 3D space include:\n",
    "\n",
    "* scaling\n",
    "* rotation\n",
    "* flipping (mirroring)\n",
    "* translation (shifting)\n",
    "\n",
    "as well as more specialised operations like color space transforms or estimating the surface normals of a triangle mesh (which way the triangles are pointing). \n",
    "\n",
    "GPUs evolved from devices designed to do these geometric transformations extremely quickly. A vector space formulation lets all geometry have a common representation, and *matrices* (which we will see later) allow for efficient definition of operations on portions of that geometry.\n",
    "\n",
    "Graphical pipelines process everything (spatial position, surface normal direction, texture coordinates, colours, and so on) as large arrays of vectors. Programming for graphics on GPUs largely involves packing data into a low-dimensional vector arrays (on the CPU) then processing them quickly on the GPU using a **shader language**.\n",
    "\n",
    "Shader languages like HLSL and GLSL have special data types and operators for working with low dimensional vectors:\n",
    "\n",
    "    # some GLSL\n",
    "    vec3 pos = vec3(1.0,2.0,0.0);\n",
    "    vec3 vel = vec3(0.1,0.0,0.0);\n",
    "    \n",
    "    pos = pos + vel;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning applications\n",
    "Machine learning relies heavily on vector representation. A typical machine learning process involves:\n",
    "\n",
    "* transforming some data onto **feature vectors** \n",
    "* creating a function that transforms **feature vectors** to a prediction (e.g. a class label)\n",
    "\n",
    "The **feature vectors** are simply an encoding of the data in vector space, which could be as simple as the tabular data example above, and feature transforms (the operations that take data in its \"raw\" form and output feature vectors) range from the very simple to enormously sophisticated. \n",
    "\n",
    "*Most machine learning algorithms can be seen as doing geometric operations: comparing distances, warping space, computing angles, and so on.*\n",
    "\n",
    "One of the simplest effective machine learning algorithms is **k nearest neighbours**. This involves some *training set* of data, which consists of pairs $\\vec{x_i}, y_i$: a feature vector $\\vec{x_i}$ and a label $y_i$. \n",
    "\n",
    "When a new feature needs classified to make a prediction, the $k$ *nearest* vectors in this training set are computed, using a **norm** to compute distances. The output prediction is the class label that occurs most times among these $k$ neighbours ($k$ is preset in some way; for many problems it might be around 3-12).\n",
    "\n",
    "The idea is simple; nearby vectors ought to share common properties. So to find a property we don't know for a vector we do know, look at the properties that nearby vectors have.\n",
    "\n",
    "<img src=\"imgs/iris.png\">\n",
    "*[Image: the measurements of the dimensions of the sepals and petals of irises allows classification of species]*\n",
    "\n",
    "In a classic ML example, the feature vector is the physical dimensions of the parts of a flower; four measurements like above gives a 4D vector. The class to predict is the *species* of the flower. \n",
    "\n",
    "<img src=\"imgs/knn.png\" width=\"70%\">\n",
    "\n",
    "*Image: pairplot of the four dimensions of the iris dataset against each other. A new point (blue) is tested, and the 5 nearest neighbours (small blue circles) are shown. The result of classification is the majority label of the neighbours; here, most of the neighbours belong to the green class*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image compression\n",
    "Images have a straightforward representation as 2D arrays of brightness, as we have seen already. But, just like text, this representation is rather empty in terms of the operations that can be done to it. A single pixel, on its own, has as little meaning as a single letter.\n",
    "\n",
    "Groups of pixels -- for example, rectangular patches -- can be unraveled into a vector. An 8x8 image patch would be unraveled to a 64-dimensional vector. These vectors can be treated as elements of a vector space.\n",
    "\n",
    "<img src=\"imgs/img_closeup.png\"> <br>\n",
    "<img src=\"imgs/img_vector.png\"> <br>\n",
    "<br><br>*.[Original image](https://flickr.com/photos/fsadykov/28389929340 \"homeless\") by [red line highway](https://flickr.com/people/fsadykov) [CC BY](https://creativecommons.org/licenses/by/2.0/)*\n",
    "\n",
    "Many image compression algorithms take advantage of this view. One common approach involves splitting images into patches, and treating each patch as a vector $\\vec{x_1}, \\dots, \\vec{x_n}$. The vectors are **clustered** to find a small number of vectors $\\vec{y_1}, \\dots, \\vec{y_m},\\ m << n$ that are a reasonable approximation of nearby vectors. Instead of storing the whole image, the vectors for the small number of representative vectors $\\vec{y_i}$ are stored (the **codebook**), and the rest of the image is represented as the *indices* of the \"closest\" matching vector in the codebook (i.e. the vector $\\vec{y_j}$ that minimises $||x_i - y_j||$. \n",
    "\n",
    "This is **vector quantisation**, so called because it quantises the vector space into a small number of discrete regions. This process maps **visual similarity onto spatial relationships.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Basic vector operations\n",
    "There are several standard operations defined for vectors, including getting the length of vectors,  and computing dot (inner), outer and cross products.\n",
    "<br>\n",
    "\n",
    "### Addition and multiplication\n",
    "Elementwise addition and scalar multiplication on arrays already implement the mathematical vector operations. Note that these ideas let us form **weighted sums** of vectors:\n",
    "$$\\lambda_1 \\vec{x_1} + \\lambda_2 \\vec{x_2} + \\dots + \\lambda_n \\vec {x_n}$$\n",
    "\n",
    "This applies **only** to vectors of the same dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:02:28.077282Z",
     "start_time": "2020-10-01T13:02:27.845395Z"
    }
   },
   "outputs": [],
   "source": [
    "v1 = np.array([2.0, 1.0])\n",
    "v2 = np.array([-1.0, 0.5])\n",
    "v3 = np.array([-0.5, 0.0])\n",
    "\n",
    "origin = np.array([0,0])\n",
    "\n",
    "def show_vector(ax, start, end, label='', color=None, **kwargs):\n",
    "    vec = np.stack([start, end])    \n",
    "    lines = ax.plot(end[0], end[1], 'o', label=label, color=color, **kwargs)\n",
    "    if color is None:\n",
    "        color = lines[0].get_color()\n",
    "    ax.arrow(start[0], start[1], end[0]-start[0], end[1]-start[1], \n",
    "             head_width=0.1, width=0.02, overhang=0.2, length_includes_head=True,\n",
    "            color=color, **kwargs)\n",
    "    \n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "# show the original vectors\n",
    "show_vector(ax, origin, v1, 'V1')\n",
    "show_vector(ax, origin, v2, 'V2')\n",
    "show_vector(ax, origin, v3, 'V3')\n",
    "# show some sums of vectors\n",
    "show_vector(ax, v1, v1+v2, 'V1+V2')\n",
    "show_vector(ax, v1+v2, v1+v2+v3, 'V1+V2+V3')\n",
    "show_vector(ax, v1+v2+v3, v1+v2+v3-v2, 'V1+V2+V3-V2')\n",
    "\n",
    "ax.set_frame_on(False)\n",
    "ax.set_xlim(-3,3)\n",
    "ax.set_ylim(-3,3)\n",
    "ax.set_aspect(1.0)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that because we have defined addition and scalar multiplication, many standard statistics and operations can be directly applied.\n",
    "\n",
    "For example, we can **linearly interpolate** between two vectors. Linear interpolation of two values is governed by a parameter $\\alpha$, and is just:\n",
    "\n",
    "$$\\text{lerp}(\\vec{x}, \\vec{y}, \\alpha) = (1-\\alpha) \\vec{x} + (\\alpha) \\vec{y}$$\n",
    "\n",
    "This lets us move along the line between two vectors: as $\\alpha$ goes from 0 to 1, the result goes in a smooth straight line from $\\vec{x}$ to $\\vec{y}$.\n",
    "\n",
    "We can see this visually:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:03:39.158787Z",
     "start_time": "2020-10-01T13:03:38.935357Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# show the original vectors\n",
    "show_vector(ax, origin, v1, 'V1')\n",
    "show_vector(ax, origin, v2, 'V2')\n",
    "\n",
    "\n",
    "# show a range of weighted vectors in between\n",
    "alphas = np.linspace(0, 1, 15)\n",
    "for alpha in alphas:\n",
    "    show_vector(ax, origin, \n",
    "                alpha * v1 + (1 - alpha) * v2, color='k', alpha=0.25, label='')\n",
    "    \n",
    "ax.set_xlim(-2, 3)\n",
    "ax.set_ylim(-2, 3)\n",
    "ax.set_title(\"Linear interpolation between two 2D vectors\")\n",
    "ax.legend()\n",
    "ax.set_aspect(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How big is that vector?\n",
    "Vector spaces do not necessarily have a concept of distance, but the spaces we will consider can have a distance *defined*. It is not an inherent property of the space, but something that we define such that it gives us useful measures.\n",
    "\n",
    "The Euclidean length of a vector $\\bf x$ (written as $||{\\bf x}||$) can be computed directly using `np.linalg.norm()`. This is equal to:\n",
    "\n",
    "$$ \\|{\\bf x}\\|_2 = \\sqrt{x_0^2 + x_1^2 + x_2^2 + \\dots + x_n^2  } $$\n",
    "\n",
    "and corresponds to the radius of a (hyper)sphere that would just touch the position specified by the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:04:54.451814Z",
     "start_time": "2020-10-01T13:04:54.440335Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([1.0, 10.0, -5.0])\n",
    "y = np.array([1.0, -4.0, 8.0])\n",
    "print_matrix(\"x\", x)\n",
    "print_matrix(\"y\", y)\n",
    "\n",
    "print_matrix(\"\\|x\\|\", np.linalg.norm(x))\n",
    "print_matrix(\"\\|y\\|\", np.linalg.norm(y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different norms\n",
    "The default `norm` is the **Euclidean norm** or **Euclidean distance measure**; this corresponds to the everyday meaning of the word \"length\". A vector space of real vectors with the Euclidean norm is called a **Euclidean space**. The distance between two vectors is just the norm of the difference of two vectors: $$||{\\bf x-y}||_2$$ is the distance from $\\bf x$ to $\\bf y$\n",
    "\n",
    "But there are multiple ways of measuring the length of a vector, some of which are more appropriate in certain contexts. These include the $L_p$-norms or Minkowski norms, which generalise Euclidean distances, written $$\n",
    "\\|{\\bf x}\\|_p$$.\n",
    "\n",
    "The $L_p$ norm is defined by: \n",
    "    \n",
    "$$\\|\\vec{x}\\|_p = \\left(\\sum_i x_i^p\\right)^\\frac{1}{p}$$\n",
    "\n",
    "\n",
    "<img src=\"imgs/pnorm_triangle.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| p         | Notation              | Common name                    | Effect                 | Uses                                          | Geometric view                     |\n",
    "|-----------|-----------------------|--------------------------------|------------------------|-----------------------------------------------|------------------------------------|\n",
    "| 2         | $\\|x\\|$ or $\\|x\\|_2$  | Euclidean norm                 | Ordinary distance      | Spatial distance measurement                  | Sphere just touching point         |\n",
    "| 1         | $\\|x\\|_1$             | Taxicab norm; Manhattan norm   | Sum of absolute values | Distances in high dimensions, or on grids     | Axis-aligned steps to get to point |\n",
    "| 0         | $\\|x\\|_0$             | Zero pseudo-norm; non-zero sum | Count of non-zero values | Counting the number of \"active elements\"    | Numbers of dimensions not touching axes                                |\n",
    "| $\\infty$  | $\\|x\\|_\\inf$          | Infinity norm; max norm        | Maximum element        | Capturing maximum \"activation\" or \"excursion\" | Smallest cube enclosing point      |\n",
    "| $-\\infty$ | $\\|x\\|_{-\\inf}$       | Min norm                       | Minimum element        |    Capturing minimum excursion                                           |                         Distance of point to closest axis           |  \n",
    "\n",
    "#### Unit \"spheres\"\n",
    "The figure below shows the contours of a \"sphere\" in $\\real^2$ in several $L_p$ norms; a series of rings with increasing size as measured by a $L_p$ norm over a 2D space. Every dashed line has the same distance to the origin as measured in that norm. The points of equal distance in that norm appear as a connected line.\n",
    "\n",
    "<img src=\"imgs/pnorms.png\">\n",
    "\n",
    "*Image: isocontours of the $L_p$ norm for various $p$. This shows contours of equal distance to the centre for each norm. The top row shows standard norms. The bottom row shows pseudo-norms.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:07:34.784266Z",
     "start_time": "2020-10-01T13:07:34.760313Z"
    }
   },
   "outputs": [],
   "source": [
    "test_vector = np.array([1, 0, 2, 0.1, -4, 0])\n",
    "print_matrix(\"x\", test_vector)\n",
    "for norm in [1, 2, np.inf, -np.inf, 0, 0.5, 3, -1]:\n",
    "    print_matrix(\"\\|x\\|_{{{norm}}}\".format(norm=norm), np.linalg.norm(test_vector,norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unit vectors and normalisation\n",
    "A unit vector has norm 1 (the definition of a unit vector depends on the norm used). Normalising for the Euclidean norm can by done by scaling the vector ${\\bf x}$ by $\\frac{1}{||{\\bf x}||_2}$. A unit vector nearly always refers to a vector with Euclidean norm 1.\n",
    "\n",
    "If we think of vectors in the physics sense of having a **direction** and **length**, a unit vector is \"pure direction\". If normalised using the $L_2$ norm, for example, a unit vector always lies on the surface of the unit sphere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:08:47.143812Z",
     "start_time": "2020-10-01T13:08:47.136830Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.normal(0,5,(5,)) # a random vector\n",
    "x_norm = x / np.linalg.norm(x) # a random unit vector\n",
    "print_matrix(\"x\", x)\n",
    "print_matrix(\"x_n\", x_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:09:05.176031Z",
     "start_time": "2020-10-01T13:09:05.171045Z"
    }
   },
   "outputs": [],
   "source": [
    "def connect_plot(ax, a, b):\n",
    "    \n",
    "    for a1,b1 in zip(a,b):        \n",
    "        #ax.plot([a1[0], b1[0]],[a1[1], b1[1]], 'k-', lw=0.25)\n",
    "        ax.arrow(a1[0], a1[1], b1[0]-a1[0], b1[1]-a1[1],\n",
    "              head_width=0.05, length_includes_head=True, facecolor='k', zorder=10)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:09:07.151987Z",
     "start_time": "2020-10-01T13:09:06.871710Z"
    }
   },
   "outputs": [],
   "source": [
    "# show that 2D unit vectors lie on the unit circle\n",
    "x = np.random.normal(0,1,(100,2)) # 100 2D vectors\n",
    "unit_x = (x.T / np.linalg.norm(x, axis=1)).T # a random unit vector\n",
    "\n",
    "\n",
    "# plot the results\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(x[:,0], x[:,1], '.', label=\"Original\")\n",
    "ax.plot(unit_x[:,0], unit_x[:,1], 'o', label=\"Normalised\")\n",
    "ax.legend()\n",
    "connect_plot(ax, x, unit_x)\n",
    "ax.set_aspect(1.0)\n",
    "ax.set_frame_on(False)\n",
    "ax.set_title(\"Euclidean normalisation of vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:09:25.925742Z",
     "start_time": "2020-10-01T13:09:25.536278Z"
    }
   },
   "outputs": [],
   "source": [
    "# in a different norm\n",
    "x = np.random.normal(0,1,(100,2)) # 100 2D vectors\n",
    "unit_x = (x.T / np.linalg.norm(x, axis=1, ord=np.inf)).T # a random L_inf unit vector\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "connect_plot(ax, x, unit_x)\n",
    "ax.set_title(\"$L_\\infty$ normalisation of vectors\")\n",
    "ax.plot(x[:,0], x[:,1], '.', label=\"Original\")\n",
    "\n",
    "ax.set_frame_on(False)\n",
    "ax.plot(unit_x[:,0], unit_x[:,1], 'o', label=\"Normalised\")\n",
    "ax.legend()\n",
    "ax.set_aspect(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner products of vectors\n",
    "An inner product $(\\real^N \\times \\real^N) \\rightarrow \\real$ measures the *angle* between two real vectors. It is related to the **cosine distance**:\n",
    "    \n",
    "$$\n",
    "\\cos \\theta = \\frac{ {\\bf x} \\bullet {\\bf y}} {||{\\bf x}|| \\  ||{\\bf y}||}.$$\n",
    "\n",
    "For **unit vectors**, we can forget about the denominator, since $||{\\bf x}||=1, ||{\\bf y}||=1$, so $\\cos \\theta = {\\bf x} \\bullet {\\bf y}$.\n",
    "\n",
    "\n",
    "The computation of the dot product, for real-valued vectors in $\\real^N$, is simply the sum of the elementwise products:\n",
    "\n",
    "   \n",
    "$$\n",
    "\\vec{x}\\bullet \\vec{y} = \\sum_i x_i y_i.\n",
    "$$\n",
    "\n",
    "\n",
    "The inner product is only defined between vectors of the same dimension, and only in inner product spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:10:27.249601Z",
     "start_time": "2020-10-01T13:10:27.239121Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3, 4])\n",
    "y = np.array([4, 0, 1, 4])\n",
    "print_matrix(\"x\", x)\n",
    "print_matrix(\"y\", y)\n",
    "\n",
    "print_matrix(\"x\\cdot y\", np.inner(x, y))  # inner product is same as dot product for vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:10:59.627230Z",
     "start_time": "2020-10-01T13:10:59.467657Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.inner(x,[1,1,1])) # inner product is not defined for vectors of differing dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inner product is a useful operator for comparing vectors that might be of very different magnitudes, since it does not depend on the magnitude of the vectors, just their directions. For example, it is widely used in information retrieval to compare **document vectors** which represent terms present in a document as large, sparse vectors which might have wildly different magnitudes for documents of different lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic vector statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our straightforward definition of vectors, we can define some  **statistics** that generalise the statistics of ordinary real numbers. These just use the definition of vector addition and scalar multiplication, along with the outer product.\n",
    "\n",
    "The **mean vector** of a collection of $N$ vectors is the sum of the vectors multiplied by $\\frac{1}{N}$:\n",
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    "$$\\text{mean}(\\vec{x_1}, \\vec{x_2}, \\dots, \\vec{x_n}) = \\frac{1}{N} \\sum_i \\vec{x_i}$$\n",
    "</div>\n",
    "\n",
    "The mean vector is the **geometric centroid** of a set of vectors and can be thought of as capturing \"centre of mass\" of those vectors. \n",
    "\n",
    "If we have vectors stacked up in a matrix $X$, one vector per row, `np.mean(x, axis=0)` will calculate the mean vector for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:11:32.133686Z",
     "start_time": "2020-10-01T13:11:32.125708Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.normal(2,1, (30,4)) \n",
    "# 20 rows, 4 columns\n",
    "\n",
    "mu = np.mean(x, axis=0) # the mean vector\n",
    "print_matrix(\"{\\\\bf \\mu}\", mu)\n",
    "\n",
    "# verify the computation is the same as doing it \"by hand\"\n",
    "print_matrix(\"{\\\\bf \\mu}_{\\\\text{hand}}\", \n",
    "             np.sum(x, axis=0)/x.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can **center** a dataset stored as an array of vectors to **zero mean** by just subtracting the mean vector from every row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:12:00.291150Z",
     "start_time": "2020-10-01T13:12:00.285167Z"
    }
   },
   "outputs": [],
   "source": [
    "x_center = x - mu\n",
    "mu_c = np.mean(x_center, axis=0) # verify the mean is now all zeros\n",
    "print_matrix(\"\\mu_c\", mu_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:12:07.185915Z",
     "start_time": "2020-10-01T13:12:06.974458Z"
    }
   },
   "outputs": [],
   "source": [
    "# show the effect of centering a collection of vectors\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "# original\n",
    "ax.scatter(x[:,0], x[:,1], c='C0', label=\"Original\", s=10)\n",
    "# original mean\n",
    "ax.scatter(mu[0], mu[1], c='C2', label=\"Mean\", s=40)\n",
    "\n",
    "# centered\n",
    "ax.scatter(x_center[:,0], x_center[:,1], c='C1', label=\"Centered\", s=10)\n",
    "ax.plot([0, mu[0]], [0, mu[1]], c='C2', label='Shift')\n",
    "\n",
    "# draw origin and fix axes\n",
    "ax.set_xlim(-4,4)\n",
    "ax.set_ylim(-4,4)\n",
    "ax.set_frame_on(False)\n",
    "ax.axhline(0, c='k', ls=':')\n",
    "ax.axvline(0, c='k', ls=':')\n",
    "ax.set_aspect(1.0)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median is harder\n",
    "Note that other statistical operations like the median can be generalised to higher dimensions, but it is much more complex to do so, and there is no simple direct algorithm for computing the **geometric median**. This is because the are *not* just combined operations of scalar multiplication and vector addition.\n",
    "\n",
    "-----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-dimensional vector spaces\n",
    "Vectors in low dimensional space, such as 2D and 3D are familiar in their operation. However, data science often involves **high dimensional vector spaces**, which obey the same mathematical rules as we have defined, but whose properties are sometimes unintuitive.\n",
    "\n",
    "Many problems in machine learning, optimisation and statistical modelling involve using *many measurements*, each of which has a simple nature; for example, an image is just an array of luminance measurements. A 512x512 image could be considered a single vector of 262144 elements. We can consider one \"data point\" to be a vector of measurements. The dimension $d$ of these \"feature vectors\" has a massive impact on the performance and behaviour of algorithmics and many of the problems in modelling are concerned with dealing with high-dimensional spaces.\n",
    "\n",
    "High-dimensional can mean any $d>3$; a 20-dimensional feature set might be called medium-dimensional; a 1000-dimensional might be called high-dimensional; a 1M-dimensional dataset might be called extremely high-dimensional. These are loose terms, and vary from discipline to discipline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geometry in high-D\n",
    "The geometric properties of high-d spaces are very counter-intuitive. The volume of space increases exponentially with $d$ (e.g. the volume of a hypersphere or hypercube). There is *a lot* of empty space in high-dimensions, and where data is sparse it can be difficult to generalise in high-dimensional spaces. Some research areas, such as genetic analysis often have $n<<d$; i.e. many fewer samples than measurement features (we might have 20000 vectors, each with 1 million dimensions). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "### Curse of dimensionality    \n",
    "Many algorithms that work really well in low dimensions break down in higher dimensions. This problem is universal in data science and is called the **curse of dimensionality**. Understanding the curse of dimensionality is critical to doing any kind of data science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: sailing weather station\n",
    "\n",
    "Imagine we build a weather station for mounting on a sailing craft, to measure local atmospheric conditions during voyages. This seems like an innocuous problem.\n",
    "\n",
    "<img src=\"imgs/sailing.jpg\"> <br><br>*.[Image](https://flickr.com/photos/16633132@N04/33932787224 \"Departing Grutness IMG_2486\") by [Ronnierob](https://flickr.com/people/16633132@N04) license [CC BY-SA](https://creativecommons.org/licenses/by-sa/2.0/)*\n",
    "\n",
    "We want to be able to summarise the weather conditions encountered.  Every few minutes we measure a number of variables: wind speed, temperature, humidity, sunshine hours, etc. As a simple visualisation, we could use a histogram to count the number of data points falling into bins. This might let us do simple predictions, such as: *is it likely to be above 30C tomorrow?*\n",
    "\n",
    "Let's say we measure temperatures and have 10,000 measurements. We might have a histogram with division into 10 bins; each bin would receive hundreds if not thousands of data points. So our histogram will be a fairly reliable summary of the weather. The size of each bin will have lots of evidence to support it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:24:56.066541Z",
     "start_time": "2020-10-01T13:24:55.659124Z"
    }
   },
   "outputs": [],
   "source": [
    "time = np.linspace(0,120,10000)\n",
    "season = np.sin(2*np.pi*time/12.0)\n",
    "temps = np.random.normal(14.0, 4.0, time.shape) + season * 5\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(time, temps)\n",
    "ax.set_xlabel(\"Time (months)\")\n",
    "ax.set_ylabel(\"Temp (deg. C)\")\n",
    "ax.set_frame_on(False)\n",
    "ax.set_title(\"Simulated temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:25:02.769370Z",
     "start_time": "2020-10-01T13:25:02.484101Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.hist(temps, bins=20)\n",
    "ax.set_frame_on(False)\n",
    "ax.set_title(\"Histogram of temperatures\")\n",
    "ax.set_xlabel(\"Temp (deg. C)\")\n",
    "ax.set_ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now maybe we also measure humidity, and compute a 2D histogram for each of our 10,000 (temp, humidity) pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:25:23.761251Z",
     "start_time": "2020-10-01T13:25:23.451551Z"
    }
   },
   "outputs": [],
   "source": [
    "humidity = np.random.normal(np.tanh(temps*0.15)*60+30, 5, time.shape)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.set_xlabel(\"Time (months)\")\n",
    "ax.set_ylabel(\"Relative humidity (%)\")\n",
    "ax.set_title(\"Simulated humidity\")\n",
    "ax.set_frame_on(False)\n",
    "ax.plot(time, humidity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we plot a 2D histogram representing the *combinations* of temperature and humidity experienced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:25:28.296659Z",
     "start_time": "2020-10-01T13:25:28.026827Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "bar = plt.hist2d(temps, humidity, bins=(20,20));\n",
    "ax.set_xlabel(\"Temp (deg. C)\")\n",
    "ax.set_ylabel(\"Relative humidity (%)\")\n",
    "fig.colorbar(bar[-1])\n",
    "ax.set_title(\"2D histogram of temperature and humidity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many bins left?\n",
    "Now there are 20 bins in each dimension, for 400 bins total. Each bin only gets ~500 or so measurements at most, and in practice most bins are empty and a few are heavily populated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### High-D histograms don't work\n",
    "If we had 10 different measurements (air temperature, air humidity, latitude, longitude, wind speed, wind direction, precipitation, time of day, solar power, sea temperature) and we wanted to subdivide them into 20 bins each, we would need a histogram with $20^{10}$ bins -- over ***10 trillion*** bins. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we only have 10,000 measurements; so we'd expect that virtually every bin would be empty, and that a tiny fraction of bins (about 1 in a billion in this case) would have probably one measurement each. Not to mention a naive implementation would need memory space for 10 trillion counts -- even using 8 bit unsigned integers this would be 10TB of memory!\n",
    "\n",
    "This is the problem of sparseness in high-dimensions. There is a lot of volume in high-D, and geometry does not work as you might expect generalising from 2D or 3D problems.\n",
    "\n",
    "* **Curse of dimensionality: as dimension increases generalisation gets harder *exponentially***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The dark heart of Africa\n",
    "<img src=\"imgs/africa_heart.jpg\" width=80%>\n",
    "\n",
    "*Africa in 1861. Public domain. Everything is empty in the middle -- apparently*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paradoxes of high dimensional vector spaces\n",
    "Here are some high-d \"paradoxes\":\n",
    "#### Think of a box\n",
    "* Imagine an empty box in high-D (a hyper cube). (good luck imagining it!)\n",
    "    * Fill it with random points. For any given point, in high enough dimension, the boundaries of the box will be closer than any other point in the box.\n",
    "    * Not only that, but every point will be nearly the same (Euclidean, $L_2$) distance away from any other point.\n",
    "    * The box will have $2^d$ corners. For example, a 20D box has more than 1 million corners.\n",
    "    * For $d>5$ more of the volume is in the areas close to the corners than anywhere else; by $d=20$  the overwhelming volume of the space is in the corners.\n",
    "    * Imagine a sphere sitting in the box so the sphere's surface just touches the edges of the box (an inscribed sphere). As D increases, the sphere takes up less and less of the box, until it is a vanishingly small proportion of the space.\n",
    "    * Fill the inner sphere with random points. **Almost all of them** are within in a tiny shell near the surface of the sphere, with virtually none in the centre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spheres in boxes\n",
    "Although humans are terrible at visualising high dimensional problems, we can see some of the properties of high-d spaces visually by analogy.\n",
    "<img src=\"imgs/ballbox.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The volume of a n-D sphere with diameter $1$  is $$ V_n(R) = \\frac{\\pi^{n/2}}{\\Gamma({n/2}+1)}\\frac{1}{2}^n$$ (you definitely don't need to know this formula -- it's just to show how this is computed).\n",
    "\n",
    "The volume of a unit cube is just $$1^n=1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:28:58.726732Z",
     "start_time": "2020-10-01T13:28:58.722743Z"
    }
   },
   "outputs": [],
   "source": [
    "import scipy.special # for the gamma function\n",
    "\n",
    "def sphere_volume(n):\n",
    "    return 0.5**n * np.pi**(n/2.0) / scipy.special.gamma(n/2.0+1)\n",
    "\n",
    "# this one is easy...\n",
    "def cube_volume(n):\n",
    "    return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:29:00.616374Z",
     "start_time": "2020-10-01T13:29:00.421895Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.arange(1,50)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(x, [sphere_volume(xi) for xi in x], 'o-')\n",
    "ax.set_xlabel(\"Dimension\")\n",
    "ax.set_ylabel(\"Volume\")\n",
    "ax.set_frame_on(False)\n",
    "ax.set_title(\"Volume of sphere as fraction of circumscibed cube vs. dimension\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:29:08.156472Z",
     "start_time": "2020-10-01T13:29:07.856226Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.semilogy(x, [sphere_volume(xi) for xi in x])\n",
    "ax.set_xlabel(\"Dimension\")\n",
    "ax.set_ylabel(\"Volume\")\n",
    "ax.set_frame_on(False)\n",
    "ax.set_title(\"Volume of sphere as fraction of circumscibed cube vs. dimension (log scale)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A box in space\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:31:55.230438Z",
     "start_time": "2020-10-01T13:31:54.693321Z"
    }
   },
   "outputs": [],
   "source": [
    "# let's do an experiment\n",
    "d = 1  # dimensions\n",
    "n = 100_000 # number of points\n",
    "\n",
    "# cube, at origin, n-dimensional\n",
    "points_in_box = np.random.uniform(-1, 1, (n,d))\n",
    "\n",
    "# select all points with radius < 1 (inside sphere in that box)\n",
    "points_in_sphere = points_in_box[\n",
    "    np.linalg.norm(points_in_box,axis=1)<1]\n",
    "\n",
    "# how many points are in the sphere compared to the box?\n",
    "print(\"Points in box: {n_pts}; Points in sphere:{n_sphere}, ratio:{ratio:.4f}%\".format(n_pts=len(points_in_box), \n",
    "                                                                                       n_sphere=len(points_in_sphere), \n",
    "                                                                ratio=100*len(points_in_sphere)/len(points_in_box)))\n",
    "\n",
    "# How far away are the points in the sphere from the origin (mean radius)\n",
    "print(\"Mean radius of points in sphere\", np.mean(np.linalg.norm(points_in_sphere, axis=1)))\n",
    "\n",
    "# How far away are the points in the box from the edges of the box\n",
    "distance_to_walls = 1.0 - np.linalg.norm(points_in_box, np.inf, axis=1)\n",
    "\n",
    "print(\"Mean distance to nearest wall of the box\", np.mean(distance_to_walls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distances don't work so well\n",
    "If we compute the distance between two random high-dimensional vectors in the Euclidean norm, the results will be *almost the same*. Almost all points will be a very similar distance apart from each other. \n",
    "\n",
    "Other norms like the $L_{\\inf}$ or $L_1$ norm, or the cosine distance (normalised dot product) between two vectors $\\vec{x}$ and $\\vec{y}$ are less sensitive to high dimensional spaces, though still not perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:34:01.462298Z",
     "start_time": "2020-10-01T13:34:01.029366Z"
    }
   },
   "outputs": [],
   "source": [
    "# Show that random points are in fact almost all the same distance away from each other!\n",
    "import scipy.spatial as sp # just allows us to compute inter-point distances quickly\n",
    "\n",
    "d = 10\n",
    "p = 2\n",
    "\n",
    "# 100 random points in a length 2 cube in d-dimensional space\n",
    "pts = np.random.uniform(-1,1,(1000,d))\n",
    "\n",
    "# plot the distances\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "distances = sp.distance.pdist(pts, metric='euclidean').ravel()\n",
    "distances = distances[distances!=0] # remove the zero distances of a vector to itself\n",
    "m = np.mean(distances)\n",
    "ax.hist(distances, bins=100, normed=True)\n",
    "ax.set_xlim(np.max((0,m-10.0)),m+20)\n",
    "ax.set_xlabel(\"Euclidean distance $||x-y||_{p}$\".format(p=p))\n",
    "ax.set_ylabel(\"Frequency (normalized)\")\n",
    "ax.set_frame_on(False)\n",
    "ax.set_title(\"Distribution of distance of random points in {d}D hypercube\".format(d=d))\n",
    "\n",
    "print(\"Mean: \",np.mean(distances))\n",
    "print(\"Var: \",np.var(distances))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine living in a world where *every* city was less than 20 miles away, but there were no cities at all less than 15 miles away!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrices and linear operators\n",
    "\n",
    "## Uses of matrices\n",
    "We have seen that (real) vectors represent elements of a vector space as 1D arrays of real numbers (and implemented as ndarrays of floats). \n",
    "\n",
    "Matrices represent **linear maps** as 2D arrays of reals; $\\real^{m\\times n}$.\n",
    "\n",
    "* Vectors represent \"points in space\"\n",
    "* Matrices represent *operations* that do things to those points in space. \n",
    "\n",
    "The operations represented by matrices are a particular class of functions on vectors -- \"rigid\" transformations. Matrices are a very compact way of writing down these operations.\n",
    "\n",
    "### Operations with matrices\n",
    "There are many things we can do with matrices:\n",
    "\n",
    "* They can be added and subtracted $C=A+B$ \n",
    "    *  $(\\real^{n\\times m},\\real^{n\\times m}) \\rightarrow \\real^{n\\times m}$\n",
    "* They can be scaled with a scalar $C = sA$\n",
    "    * $(\\real^{n\\times m},\\real) \\rightarrow \\real^{n\\times m}$\n",
    "* They can be transposed $B = A^T$; this exchanges rows and columns\n",
    "    * $\\real^{n\\times m} \\rightarrow \\real^{m\\times n}$\n",
    "* They can be *applied to vectors* $\\vec{y} = A\\vec{x}$; this **applies** a matrix to a vector.\n",
    "    * $(\\real^{n\\times m}, \\real^{m}) \\rightarrow \\real^{n}$\n",
    "* They can be *multiplied together* $C = AB$; this **composes** the effect of two matrices \n",
    "    * $(\\real^{p\\times q}, \\real^{q\\times r})\\rightarrow \\real^{p\\times r}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Intro to matrix notation\n",
    "We write matrices as a capital letter: \n",
    "\n",
    "$$A \\in \\real^{n \\times m}=  \\begin{bmatrix}\n",
    "a_{1,1}  & a_{1,2} & \\dots & a_{1,m}  \\\\\n",
    "a_{2,1}  & a_{2,2}  & \\dots & a_{2,m}  \\\\\n",
    "\\dots \\\\\n",
    "a_{n,1} + & a_{n,2}  & \\dots & a_{n,m} \\\\\n",
    "\\end{bmatrix},\\  a_{i,j}\\in \\real$$\n",
    "\n",
    "(although we don't usually write matrices with capital letters in code -- they follow the normal rules for variable naming like any other value)\n",
    "\n",
    "A matrix with dimension $n \\times m$ has $n$ rows and $m$ columns (remember this order -- it is important!). Each element of the matrix $A$ is written as $a_{i,j}$ for the $i$th row and $j$th column.\n",
    "\n",
    "Matrices correspond to the 2D arrays / rank-2 tensors we are familiar with from earlier. But they have a very rich mathematical structure which makes them of key importance in computational methods. *Remember to distinguish 2D arrays from the mathematical concept of matrices. Matrices (in the linear algebra sense) are represented by 2D arrays, as real numbers are represented by floating-point numbers*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:41:40.772185Z",
     "start_time": "2020-10-01T13:41:40.765204Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, 3],\n",
    "              [4, 5, 6],\n",
    "              [7, 8, 9]])\n",
    "\n",
    "print_matrix(\"A\", a)\n",
    "# note that code indexes from 0, whereas mathematical notation indexes from 1!\n",
    "\n",
    "print_matrix(\"A_{1,3}\", a[0,2]) # index row i=0, column j=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrices as maps\n",
    "We saw vectors as **points in space**. Matrices represent **linear maps** -- these are functions applied to vectors which outputs vectors. In the standard notation, matrices are *applied* to vectors by multiplying them:\n",
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    "    \n",
    "$$ A{\\bf x} = f({\\vec x}) $$\n",
    "</div>\n",
    "\n",
    "This is equivalent to applying some function $f({\\vec x})$ to the vectors. Matrices represent functions mapping vectors to vectors in a very compact form, and they capture a special set of functions that preserve important properties of the vectors they act on. We'll see how matrix-vector multiplication is defined algorithmically shortly.\n",
    "\n",
    "#### Effect of matrix transform\n",
    "Specifically, a $n \\times m$ matrix $A$ represents a function $f({\\bf x})$ taking $m$ dimensional vectors to $n$ dimensional vectors, ($\\real^m \\rightarrow \\real^n$) such that all straight lines remain straight and all parallel lines remain parallel, and the origin does not move (i.e. that the zero vector $\\vec{0^m} [0,0,\\dots,0]\\rightarrow \\vec{0^n}[0,0,\\dots,0]$).\n",
    "\n",
    "##### Linearity\n",
    "This is equivalent to saying that:\n",
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    "    \n",
    "$$ \n",
    "f(\\vec{x}+\\vec{y}) = f(\\vec{x}) + f(\\vec{y}) \\quad =  A({\\bf x}+{\\bf y}) = A{\\bf x} + A{\\bf y}, \\\\\n",
    "f(c\\vec{x}) = cf(\\vec{x}) \\quad = A(c{\\bf x}) = cA{\\bf x},\n",
    "$$\n",
    "    \n",
    "</div>\n",
    "\n",
    "i.e. the transform of the sum of two vectors is the same as the sum of the transform of two vectors, and the transform of a scalar multiple of a vector is the same as the scalar multiple of the transform of a vector. This property is **linearity**, and matrices represent **linear maps** or **linear functions**.\n",
    "\n",
    "> Anything which is linear is easy. Anything which isn't linear is hard.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geometric intuition (cube -> parallelepiped)\n",
    "An intuitive way of understanding matrix operations is to consider a matrix to transform a cube of vector space centered on the origin in one space to a **parallelotope** in another space, with the origin staying fixed. This is the *only* kind of transform a matrix can apply.\n",
    "\n",
    "A parallelotope is the generalisation of a parallelogram to any finite dimensional vector space, which has parallel faces but edges which might not be at 90 degrees.\n",
    "\n",
    "<img src=\"imgs/parallel.png\" width=80%>\n",
    "\n",
    "#### Transforms and projections\n",
    "* A linear map is any function $f$ $R^m \\rightarrow R^n$ which satisfies the linearity requirements.\n",
    "* If the map represented by the matrix is $n\\times n$ then it maps from a vector space onto the *same* vector space (e.g. from $\\real^n \\rightarrow \\real^n$), and it is called a **linear transform**.\n",
    "* If the map has the property $Ax = AAx$ or equivalently $f(x)= f(f(x))$ then the operation is called a **linear projection**; for example, projecting 3D points onto a plane; applying this transform to a set of vectors twice is the same as applying it once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keeping it real\n",
    "We will only consider **real matrices** in this course, although the abstract definitions above apply to linear maps across any vector space (e.g complex numbers, finite fields, polynomials).\n",
    "\n",
    "#### Linear maps are representable as matrices\n",
    "*Every linear map of real vectors can be written as a real matrix.* In other words, if there is a function $f(\\vec{x})$ that satisfies the linearity conditions above, it can be expressed as a matrix $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "It's easiest to see the effect of matrix operations in low-dimensional vector spaces. Let's visualise some examples of linear transforms (linear maps $A \\in \\real^{2\\times 2}, \\real^2 \\rightarrow \\real^2$), on the 2D plane. \n",
    "\n",
    "We will take collections of vectors $\\vec{x_1}, \\vec{x_2}, \\dots$ and then apply various matrices to them. We forms the product $A\\vec{x}$, which \"applies\" the matrix to the vector $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:45:12.702540Z",
     "start_time": "2020-10-01T13:45:12.495056Z"
    }
   },
   "outputs": [],
   "source": [
    "show_matrix_effect(np.array(\n",
    "        [[1,0], \n",
    "         [0,1]]), suptitle=\"Identity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:45:20.284439Z",
     "start_time": "2020-10-01T13:45:20.080983Z"
    }
   },
   "outputs": [],
   "source": [
    "# uniform scaling\n",
    "show_matrix_effect(np.array(\n",
    "        [[0.5, 0], \n",
    "         [0,   0.5]]), suptitle=\"Uniform scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:45:29.433491Z",
     "start_time": "2020-10-01T13:45:29.234992Z"
    }
   },
   "outputs": [],
   "source": [
    "# non-uniform scaling\n",
    "show_matrix_effect(np.array(\n",
    "        [[0.5, 0], \n",
    "         [0,   1.0]]), suptitle=\"Non-uniform scale\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:45:42.051069Z",
     "start_time": "2020-10-01T13:45:41.845597Z"
    }
   },
   "outputs": [],
   "source": [
    "# rotation by 90 degrees\n",
    "show_matrix_effect(np.array(\n",
    "        [[0, 1], \n",
    "         [-1, 0]]), suptitle=\"Rotate 90\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:45:53.872134Z",
     "start_time": "2020-10-01T13:45:53.663187Z"
    }
   },
   "outputs": [],
   "source": [
    "# rotation by 30 degrees\n",
    "# don't worry about how this matrix is constructed just yet\n",
    "# but observe its effect\n",
    "d30 = np.radians(30)\n",
    "cs = np.cos(d30)\n",
    "ss = np.sin(d30)\n",
    "\n",
    "show_matrix_effect(np.array(\n",
    "        [[cs, ss], \n",
    "         [-ss, cs]]), suptitle=\"Rotate 30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:46:04.237162Z",
     "start_time": "2020-10-01T13:46:04.038168Z"
    }
   },
   "outputs": [],
   "source": [
    "# rotation by 45 degrees, scale by 0.5\n",
    "d30 = np.radians(45)\n",
    "cs = np.cos(d30) * 0.5\n",
    "ss = np.sin(d30) * 0.5\n",
    "\n",
    "show_matrix_effect(np.array([[cs, ss], \n",
    "                             [-ss, cs]]), \"Rotate 45, Scale 0.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:46:12.289423Z",
     "start_time": "2020-10-01T13:46:12.093424Z"
    }
   },
   "outputs": [],
   "source": [
    "# flip x\n",
    "show_matrix_effect(np.array([[-1, 0], \n",
    "                             [0, 1]]), \"Flip x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:46:20.888420Z",
     "start_time": "2020-10-01T13:46:20.687432Z"
    }
   },
   "outputs": [],
   "source": [
    "# shear\n",
    "show_matrix_effect(np.array([[0.15, 0.75], \n",
    "                             [0.5, 0.8]]), \"Shear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:46:38.018225Z",
     "start_time": "2020-10-01T13:46:37.818232Z"
    }
   },
   "outputs": [],
   "source": [
    "# random!\n",
    "show_matrix_effect(np.random.uniform(-1, 1, (2, 2)), \"Random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix operations\n",
    "There is an **algebra** of matrices; this is **linear algebra**. In particular, there is a concept of addition of matrices of *equal size*, which is simple elementwise addition:\n",
    "\n",
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    "    \n",
    "$$  A + B = \\begin{bmatrix}\n",
    "a_{1,1} + b_{1,1} & a_{1,2} + b_{1,2} & \\dots & a_{1,m} + b_{1,m} \\\\\n",
    "a_{2,1} + b_{2,1} & a_{2,2} + b_{2,2} & \\dots & a_{2,m} + b_{2,m} \\\\\n",
    "\\dots \\\\\n",
    "a_{n,1} + b_{n,1} & a_{n,2} + b_{n,2} & \\dots & a_{n,m} + b_{n,m} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "along with scalar multiplication $cA$, which multiplies each element by $c$.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-box alert-success\">\n",
    " \n",
    "$$  cA = \\begin{bmatrix}\n",
    "ca_{1,1}  & ca_{1,2} & \\dots & ca_{1,m}  \\\\\n",
    "ca_{2,1} & ca_{2,2}  & \\dots & ca_{2,m} \\\\\n",
    "\\dots \\\\\n",
    "ca_{n,1}  & ca_{n,2} & \\dots & ca_{n,m} \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "</div>\n",
    "\n",
    "These correspond exactly to addition and scalar multiplication in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:47:08.886256Z",
     "start_time": "2020-10-01T13:47:08.870300Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.arange(9).reshape(3, 3)\n",
    "b = np.array([[1, 0, 1], \n",
    "              [-1, -1, -1], \n",
    "              [1, -1, 0]])\n",
    "\n",
    "print_matrix(\"A\", a)\n",
    "print_matrix(\"B\", b)\n",
    "print_matrix(\"A+B\", a + b)  # matrix addition\n",
    "print_matrix(\"2A=A+A\", a * 2)  # scalar multiplication\n",
    "print_matrix(\"0.5A\", a * 0.5)  # scalar multiplication\n",
    "print_matrix(\"A-B = A+(-1)B\", a - b)  # equal to (-1) * a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application to vectors\n",
    "We can apply a matrix to a vector. We write it as a product $A\\vec{x}$, to mean the matrix $A$ applied to the vector $\\vec{x}$.  This is equivalent to applying the function $f(\\vec{x})$, where $f$ is the corresponding function.\n",
    "\n",
    "If $A$ is $\\real^{n \\times m}$, and $\\vec{x}$ is $\\real^m$, then this will map from an $m$ dimensional vector space to an $n$ dimensional vector space.\n",
    "\n",
    "**All application of a matrix to a vector does is form a weighted sum of the elements of the vector**. This is a linear combination (equivalent to a \"weighted sum\") of the components.\n",
    "\n",
    "In particular, we take each element of $\\vec{x}, x_1, x_2, \\dots, x_m$, multiply it with the corresponding *column* of $A$, and sum these columns together.\n",
    "\n",
    "* Set $\\vec{y}=[0,0,0,\\dots]=0^n$ (the n-dimensional zero vector)\n",
    "* For each column $1\\leq i \\leq m$ in $A$\n",
    "    *    $\\vec{y} = \\vec{y} + x_iA_i$. Note that $x_iA_i$ is scalar times vector, and has $n$ elements. $A_i$ here means the $i$th column of $A$.\n",
    "\n",
    "\n",
    "    1 2     1 \n",
    "    3 4     2\n",
    "    5 6\n",
    "    7 8\n",
    "                 \n",
    "    = 1 * [1,3,5,7] + 2 * [2,4,6,8]\n",
    "    = [1*1+2*2, 1*3+2*5, 1*4+2*6, 1*7+2*8], \n",
    "    = [5, 11, 17, 23]     \n",
    "    \n",
    "    We can use @ to form products of vectors and matrices in Numpy:\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:48:51.872411Z",
     "start_time": "2020-10-01T13:48:51.863437Z"
    }
   },
   "outputs": [],
   "source": [
    "A = np.array([[1, 2], [3, 4], [5, 6], [7, 8]])\n",
    "x = np.array([1, 2])\n",
    "\n",
    "print_matrix(\"A\", A)\n",
    "print_matrix(\"\\\\bf x\", x)\n",
    "print_matrix(\"A\\\\bf x\", A @ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:49:08.826408Z",
     "start_time": "2020-10-01T13:49:08.820424Z"
    }
   },
   "outputs": [],
   "source": [
    "# we'd never do this by hand\n",
    "def apply_matrix_vector(A, x):\n",
    "    y = np.zeros(A.shape[0])\n",
    "    for i in range(A.shape[1]):\n",
    "        y += x[i] * A[:, i]\n",
    "    return y\n",
    "\n",
    "print_matrix(\"A\\\\bf x\", apply_matrix_vector(A, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:49:12.367725Z",
     "start_time": "2020-10-01T13:49:12.360773Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([1, 2, 3])\n",
    "A = np.array([[1, -1, 1], [1, 0, 0], [0, 1, 1]])  # 3x3\n",
    "print_matrix(\"x\", x)  # note: written horizontally, but interpreted vertically!\n",
    "print_matrix(\"A\", A)\n",
    "print_matrix(\"Ax\", A @ x)  # linear transform -- output dimension == input dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:49:24.024261Z",
     "start_time": "2020-10-01T13:49:24.016773Z"
    }
   },
   "outputs": [],
   "source": [
    "B = np.array([[2, -5, 5], [1, 0, 0]])  # 2x3 -- OK\n",
    "print_matrix(\"B\", B)\n",
    "print_matrix(\"Bx\", B @ x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:49:34.579093Z",
     "start_time": "2020-10-01T13:49:34.566125Z"
    }
   },
   "outputs": [],
   "source": [
    "### shape error\n",
    "C = np.array([[2, -5], [1, 0], [3, 3]])  # 3x2 -- not OK\n",
    "print_matrix(\"C\", C)\n",
    "print_matrix(\"Cx\", C @ x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix multiplication\n",
    "Multiplication is the interesting matrix operation. Matrix multiplication defines the product $C=AB$, where $A,B,C$ are all matrices.\n",
    "\n",
    "Matrix multiplication is defined such that if $A$ represents linear transform $f(\\vec{x})$ and\n",
    "$B$ represents linear transform $g(\\vec{x})$, then $BA\\vec{x} = g(f(\\vec{x}))$.\n",
    "\n",
    "**Multiplying two matrices is equivalent to composing the linear functions they represent, and it results in a matrix which has that affect.**\n",
    "\n",
    "*Note that the composition of linear maps is read right to left. To apply the transformation $A$, **then** $B$, we form the product $BA$, and so on.*\n",
    "\n",
    "### Multiplication algorithm\n",
    "This gives rise to many important uses of matrices: for example, the product of a scaling matrix and a rotation matrix is a scale-and-rotate matrix. It also places some requirements on the matrices which form a valid product. Multiplication is *only* defined for two matrices $A, B$ if:\n",
    "* $A$ is $p \\times q$ and\n",
    "* $B$ is $q \\times r$.\n",
    "\n",
    "This follows from the definition of multiplication: $A$ represents a map $\\real^q \\rightarrow \\real^p$ and $B$ represents a map $\\real^r \\rightarrow \\real^q$. The output of $A$ must match the dimension of the input of $B$, or the operation is undefined. \n",
    "\n",
    "Matrix multiplication is defined in a slightly surprising way, which is easiest to see in the form of an algorithm:\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "   \n",
    "If $C=AB$ then $$C_{ij}=\\sum_k a_{ik} b_{kj}$$\n",
    "\n",
    "The element at $C_{ij}$ is the sum of the elementwise product of the $i$th row and the $j$th column, which will be the same size by the requirement above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:58:12.701063Z",
     "start_time": "2020-10-01T13:58:12.696076Z"
    }
   },
   "outputs": [],
   "source": [
    "def matmul(a, b):\n",
    "    p, q_a = a.shape\n",
    "    q_b, r = b.shape\n",
    "    # we can only multiply two matrices if A is p x q and B in q x r\n",
    "    assert q_a == q_b\n",
    "    # the result is a matrix of size p x r\n",
    "    c = np.zeros((p, r))\n",
    "    for i in range(p):\n",
    "        for j in range(r):\n",
    "            # Note that this can be seen as a simple *weighted sum*\n",
    "            # the sum of the ith row of A weighted by the jth column of B\n",
    "            c[i, j] = np.sum(a[i, :] * b[:, j])\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:58:13.896012Z",
     "start_time": "2020-10-01T13:58:13.887036Z"
    }
   },
   "outputs": [],
   "source": [
    "a = np.array([[1, 2, -3]])\n",
    "b = np.array([[1, -1, 1], \n",
    "              [2, -2, 2], \n",
    "              [3, -3, 3]])\n",
    "\n",
    "print_matrix(\"{\\\\bf A}\", a)\n",
    "print_matrix(\"{\\\\bf B}\", b)\n",
    "c = matmul(a,b)\n",
    "print_matrix(\"{\\\\bf A}B\", c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication is of course built in to NumPy, and much more efficient than this algorthim. Matrix multiplication is applied by `np.dot(a,b)` or by the syntax `a @ b`\n",
    "\n",
    "We'll use `a @ b` as the standard syntax for matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:58:32.053691Z",
     "start_time": "2020-10-01T13:58:32.046710Z"
    }
   },
   "outputs": [],
   "source": [
    "# verify that this is the same as the built in matrix multiply\n",
    "c_numpy = np.dot(a, b)\n",
    "print_matrix(\"C_{\\\\text numpy}\", c_numpy)\n",
    "print(np.allclose(c, c_numpy))\n",
    "\n",
    "c_at = a @ b \n",
    "print_matrix(\"C_{\\\\text a @ b}\", a @ b)\n",
    "print(np.allclose(c_at, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time complexity of multiplication\n",
    "Matrix multiplication has, in the general case, of time complexity $O(pqr)$, or for multiplying two square matrices $O(n^3)$. This is apparent from the three nested loops above. However, there are many special forms of matrices for which this complexity can be reduced, such as diagonal, triangular, sparse and banded matrices. We will we see these **special forms** later.\n",
    "\n",
    "There are some accelerated algorithms for general multiplication. The time complexity of all of them is $>O(N^2)$ but $<O(N^3)$. Most accelerated algorithms are impractical for all but the largest matrices because they have enormous constant overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Apply matrices to vectors\n",
    "The same algorithm for multiplying two matrices applies to multiplying a matrix by a vector **if we assume a $m$ dimensional vector $\\vec{x} \\in \\real^m$ is represented as a $m \\times 1$ column vector**:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\dots\\\\\n",
    "x_m\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Then the product $Ax$ is application of the linear map defined by $A$ to vector $\\vec{x}$. $A$ must be of dimension $n\\times m$ for this operation to be defined. If $A$ is $m \\times m$ then it is a **linear transform** (as we defined it above), and the result is another vector of the same dimension.\n",
    "\n",
    "Note: this is a slight abuse of notation. $\\vec{x}$ is a vector, not a matrix, and it is neither a column vector nor a row vector -- it's just an element of a vector space. However, it's convenient to pretend it works like a $m \\times 1$ matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:59:04.319942Z",
     "start_time": "2020-10-01T13:59:04.313958Z"
    }
   },
   "outputs": [],
   "source": [
    "print_matrix(\"A\", A)\n",
    "print_matrix(\"x\", x)\n",
    "print_matrix(\"Ax\", A @ x)\n",
    "# force to a 2D array of m x 1 size\n",
    "print_matrix(\"Ax\", A @ x.reshape(-1, 1))\n",
    "# note this will still be a 2D array in NumPy\n",
    "# and will appear differently when printed. However, it\n",
    "# has the same *semantics* as a 1D array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transposition\n",
    "The **transpose** of a matrix $A$ is written $A^T$ and has the same elements, but with the rows and columns exchanged. Many matrix algorithms use transpose in computations.\n",
    "\n",
    "NumPy uses the `A.T` syntax to transpose any array by reversing its stride array, which corresponds to the mathematical transpose for matrices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T13:59:35.162985Z",
     "start_time": "2020-10-01T13:59:35.156004Z"
    }
   },
   "outputs": [],
   "source": [
    "### Transpose\n",
    "A = np.array([[2, -5], [1, 0], [3, 3]])\n",
    "print_matrix(\"A\", A)\n",
    "print_matrix(\"A^T\", A.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column and row vectors\n",
    "The transpose of a column vector $$\\vec{x}=\n",
    "\\begin{bmatrix}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "\\dots\\\\\n",
    "x_n\\\\\n",
    "\\end{bmatrix}\n",
    "$$ is a row vector  $$\\vec{x}^T=\n",
    "\\begin{bmatrix}\n",
    "x_1 &\n",
    "x_2 & \n",
    "\\dots & \n",
    "x_n\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Note that from our definition of matrix multiplication, the product of a Mx1 with a 1xN vector is an $M \\times N$ matrix. This is the **outer product** of two vectors, every possible combination of their elements:\n",
    "\n",
    "$$\\vec{x} \\otimes \\vec{y} = \\vec{x}^T \\vec{y}$$\n",
    "\n",
    "and the product of a 1xN with an Nx1 vector is a 1x1 matrix; a scalar. This is exactly the **inner product** of two vectors:\n",
    "\n",
    "$$\\vec{x} \\bullet \\vec{y} = \\vec{x}\\vec{y^T} ,$$\n",
    "and is only defined for vectors $\\vec{x}, \\vec{y}$ of the same length.\n",
    "\n",
    "[again, we abuse notation to make it meaningful to \"transpose\" a vector; this is assuming we treat it as a column vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:00:37.362543Z",
     "start_time": "2020-10-01T14:00:37.356559Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([[1,2,3]])\n",
    "y = np.array([[4,5,6]])\n",
    "\n",
    "print_matrix(\"{\\\\bf x}\", x)\n",
    "print_matrix(\"{\\\\bf y}\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:00:52.160410Z",
     "start_time": "2020-10-01T14:00:52.151434Z"
    }
   },
   "outputs": [],
   "source": [
    "print_matrix(\"{\\\\bf x} \\otimes {\\\\bf y}\", np.outer(x,y))\n",
    "print_matrix(\"{\\\\bf x}^T{\\\\bf y}\", x.T @ y)\n",
    "\n",
    "print_matrix(\"{\\\\bf x} \\\\bullet {\\\\bf y}\", np.inner(x,y))\n",
    "print_matrix(\"{\\\\bf x}{\\\\bf y}^T\", x @ y.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composed maps \n",
    "\n",
    "There is a very important property of matrices. If $A$ represents $f(x)$ and $B$ represents $g(x)$, then the product $BA$ represents $g(f(x))$. **Multiplication is composition.** Note carefully the order of operations. $BA\\vec{x} = B(A\\vec{x})$ means do $A$ to $\\vec{x}$, then do $B$ to the result.\n",
    "\n",
    "We can visually verify that composition of matrices by multiplication is the composition of their effects. For example, lets define a nonuniform scaling and a rotation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:01:52.238648Z",
     "start_time": "2020-10-01T14:01:52.235652Z"
    }
   },
   "outputs": [],
   "source": [
    "## Rotation\n",
    "d30 = np.radians(30)\n",
    "cs = np.cos(d30)\n",
    "ss = np.sin(d30)\n",
    "rot30 = np.array([[cs, ss], [-ss, cs]])\n",
    "\n",
    "## Scaling\n",
    "scale_x = np.array([[1,0], [0, 0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:01:53.974142Z",
     "start_time": "2020-10-01T14:01:53.765151Z"
    }
   },
   "outputs": [],
   "source": [
    "show_matrix_effect(rot30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:01:57.656121Z",
     "start_time": "2020-10-01T14:01:57.461626Z"
    }
   },
   "outputs": [],
   "source": [
    "show_matrix_effect(scale_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:02:10.086256Z",
     "start_time": "2020-10-01T14:02:09.876816Z"
    }
   },
   "outputs": [],
   "source": [
    "A = scale_x @ rot30 # product of scale and rotate\n",
    "print(A)\n",
    "show_matrix_effect(A, \"Rotate then scale\")   # rotate, then scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:02:39.746964Z",
     "start_time": "2020-10-01T14:02:39.535499Z"
    }
   },
   "outputs": [],
   "source": [
    "B = rot30 @ scale_x\n",
    "print(B)\n",
    "show_matrix_effect(B, \"Scale then rotate\")  # scale, then rotate\n",
    "# note: Not the same!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenation of transforms\n",
    "Many software operations take advantage of the definition of matrix multiplication as the composition of linear maps. In a graphics processing pipeline, for example, all of the operations to position, scale and orient visible objects are represented as matrix transforms. Multiple operations can be combined into *one single matrix operation*.\n",
    "\n",
    "The desktop UI environment you are uses linear transforms to represent the transformation from data coordinates to screen coordinates. Because multiplication composes transforms, only a single matrix for each object needs to be kept around. (actually for 3D graphics, at least two matrices are kept: one to map 3D -> 3D (the *modelview matrix*) and one to map 3D -> 2D (the *projection matrix*)).\n",
    "\n",
    "Rotating an object by 90 degrees computes product of the current view matrix with a 90 degree rotation matrix, which is then stored in place of the previous view matrix. This means that all rendering just needs to apply to relevant matrix to the geometry data to get the pixel coordinates to perform the rendering.\n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Commutativity\n",
    "The order of multiplication is important. Matrix multiplication does **not** commute; in general:\n",
    "\n",
    "$$AB \\neq BA$$\n",
    "\n",
    "This should be obvious from the fact that multiplication is only defined for matrices of dimension $p \\times q$ and $q \\times r$; unless $p=q=r$ then the multiplication is not even *defined* if the operands are switched, since it would involve a $q \\times r$ matrix by a $p \\times q$ one!\n",
    "\n",
    "Even if two matrices are compatible in dimension when permuted (i.e. if they are square matrices, so $p=q=r$), multiplication still does not generally commute and it matters which order operations are applied in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transpose order switching\n",
    "\n",
    "There is a very important identity which is used frequently in rearranging expressions to make computation feasible. That is:\n",
    "\n",
    "$$(AB)^T = B^TA^T$$\n",
    "    \n",
    "Remember that matrix multiplication doesn't commute, so $AB \\neq BA$ in general (though it can be true in some special cases), so this is the only way to algebraically reorder general matrix multiplication expressions (side note: inversion has the same effect, but only works on non-singular matrices). This lets us rearrange the order of matrix multiplies to \"put matrices in the right place\".\n",
    "\n",
    "It is also true that $$(A+B)^T = A^T+B^T$$ but this is less often useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left-multiply and right-multiply\n",
    "Because of the noncommutativity of multiplication of matrices, there are actually two different matrix multiplication operations: **left multiplication** and **right multiplication**.\n",
    "\n",
    "$B$ left-multiply $A$ is $AB$; $B$ right-multiply $A$ is $BA$. This becomes important if we have to multiply out a longer expression:\n",
    "\n",
    "$$B\\vec{x}+\\vec{y}\\\\\n",
    "\\text{left multiply by A}\\quad =A[B\\vec{x}+\\vec{y}] = AB\\vec{x} + A\\vec{y}\\\\\n",
    "\\text{right multiply by A}\\quad =[B\\vec{x}+\\vec{y}]A = B\\vec{x}A + \\vec{y}A\\\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## An example matrix for measuring spread: covariance matrices\n",
    "\n",
    "As well as the **mean vector** we saw earlier, we can also generalise the idea of **variance**, which measures the spread of a dataset, to the multidimensional case. Variance (in the 1D case) is the sum of squared differences of each element from the mean of the vector:\n",
    "$$\\sigma^2 =  \\frac{1}{N-1} \\sum_{i=0}^{N-1} (x_i - \\mu_i)^2$$\n",
    "\n",
    "This is a measure of how \"spread out\" a vector of values $\\vec{x}$ is. The **standard deviation** $\\sigma$ is the square root of the **variance** and is more often used because it is in the same units as the elements of $\\vec{x}$.\n",
    "\n",
    "In the multidimensional case, to get a useful measure of spread of a $N \\times d$ data matrix  $X$ ($N$ $d$-dimensional vectors) we need to compute the *covariance* of every dimension with every other dimension. This is the average squared difference of each column of data from the average of every column. This forms a 2D array $\\Sigma$, which has entries in element $i,j$:\n",
    "\n",
    "$$\\Sigma_{ij} = \\frac{1}{N-1} \\sum_{k=1}^{N} (X_{ki}-\\mu_i)(X_{kj}-\\mu_j) $$\n",
    "\n",
    "As we will discuss shortly, this is a *special form* of matrix: it is square, symmetric and positive semi-definite.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:08:18.306594Z",
     "start_time": "2020-10-01T14:08:18.299613Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.random.normal(0,1,(500, 5))\n",
    "\n",
    "mu = np.mean(x, axis=0)\n",
    "sigma_cross = ((x - mu).T @ (x - mu)) / (x.shape[0]-1)\n",
    "np.set_printoptions(suppress=True, precision=2)\n",
    "print_matrix(\"\\Sigma_{\\\\text{cross}}\", sigma_cross)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also directly provided by NumPy as `np.cov(x)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:08:16.570144Z",
     "start_time": "2020-10-01T14:08:16.565648Z"
    }
   },
   "outputs": [],
   "source": [
    "# verify it is close to the function provided by NumPy\n",
    "sigma_np = np.cov(x, rowvar=False)\n",
    "print_matrix(\"\\Sigma_{\\\\text{numpy}}\", \n",
    "             sigma_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance ellipses\n",
    "This matrix captures the spread of data, including any **correlations** between dimensions. It can be seen as capturing an **ellipse** that represents a dataset.  The **mean vector** represents the centre of the ellipse, and the **covariance matrix** represent the shape of the ellipse. This ellipse is often called the **error ellipse** and is a very useful summary of high-dimensional data.\n",
    "\n",
    "The covariance matrix represents a (inverse) transform of a unit sphere to an ellipse covering the data. Sphere->ellipse is equivalent to square->parallelotope and so can be precisely represented as a matrix transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are helper functions. You do not need to undersatnd these in detail. \n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def eigsorted(cov):\n",
    "    vals, vecs = np.linalg.eigh(cov)\n",
    "    order = vals.argsort()[::-1]\n",
    "    return vals[order], vecs[:, order]\n",
    "\n",
    "def cov_ellipse(ax, x, nstd=1, **kwargs):\n",
    "    cov = np.cov(x.T)\n",
    "    mu = np.mean(x, axis=0)\n",
    "    vals, vecs = eigsorted(cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n",
    "    w, h = 2 * nstd * np.sqrt(vals)\n",
    "    ell = Ellipse(xy=(mu[0], mu[1]), width=w, height=h, angle=theta, **kwargs)\n",
    "\n",
    "    ax.add_artist(ell)\n",
    "    return mu,cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:08:21.818973Z",
     "start_time": "2020-10-01T14:08:21.625475Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "x =  np.random.normal(0,1,(200,2)) @ np.array([[0.1, 0.5], [-0.9, 1.0]])\n",
    "\n",
    "ax.scatter(x[:,0], x[:,1], c='C0', label=\"Original\", s=10)\n",
    "cov_ellipse(ax, x[:,0:2], 1, facecolor='none', edgecolor='k')\n",
    "cov_ellipse(ax, x[:,0:2], 2, facecolor='none', edgecolor='k')\n",
    "mu, cov = cov_ellipse(ax, x[:,0:2], 3, facecolor='none', edgecolor='k')\n",
    "\n",
    "ax.set_xlim(-4,4)\n",
    "ax.set_ylim(-4,4)\n",
    "ax.axhline(0)\n",
    "ax.axvline(0)\n",
    "ax.set_frame_on(False)\n",
    "ax.set_aspect(1.0)\n",
    "ax.legend()\n",
    "\n",
    "print_matrix(\"\\mu\",mu)\n",
    "print_matrix(\"\\Sigma\",cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean vector and covariance matrix capture the idea of \"centre\" and \"spread\" of a collection of points in a vector space, the way the mean and the standard deviation do for real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anatomy of a matrix\n",
    "The **diagonal** entries of a matrix ($A_{ii}$) are important \"landmarks\" in the structure of a matrix. Matrix elements are often referred to as being \"diagonal\" or \"off-diagonal\" terms. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Matrices which are all zero except for a single diagonal entry are called... *diagonal* matrices.\n",
    "They represent a transformation that is an independent scaling of each dimension. Such matrices transform cubes to cuboids (i.e. all angles remain unchanged, and no rotation occurs).\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\n",
    "0 & 2 & 0 \\\\\n",
    "0 & 0 & 3 \\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "`np.diag(x)` will return a diagonal matrix for a given vector `x`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:09:56.517282Z",
     "start_time": "2020-10-01T14:09:56.511299Z"
    }
   },
   "outputs": [],
   "source": [
    "print_matrix(\"\\\\text{diag}(x)\", np.diag([1,2,3,3,2,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The **anti-diagonal** is the set of elements $A_{i[N-i]}$ for an $NxN$ matrix, for example a 3x3 binary anti-diagonal matrix:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    " 0 & 0 & 1 \\\\\n",
    " 0 & 1 & 0 \\\\\n",
    " 1 & 0 & 0 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:10:09.850111Z",
     "start_time": "2020-10-01T14:10:09.845127Z"
    }
   },
   "outputs": [],
   "source": [
    "# an anti-diagonal array is the diagonal of the flipped array\n",
    "print_matrix(\"\\\\text{anti-diagonal}\", np.fliplr(np.diag([1,2,3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Special matrix forms\n",
    "There are *many* different special types of matrices, with different properties (for example, matrices that permute dimensions, or matrices that represent *invertible* transformations). \n",
    "\n",
    "[Wikipedia's enormous list of matrices](https://en.wikipedia.org/wiki/List_of_matrices) gives a fairly comprehensive overview. We will only deal with a few special kinds of matrix in IDSS(M). In particular, we will deal with **real matrices** only, and primarily with **real square matrices**.\n",
    "\n",
    "\n",
    "### Identity\n",
    "The identity matrix is denoted $I$ and is a $n$ square matrix, where all values are zero except 1 along the diagonal:\n",
    "$$\\begin{bmatrix}\n",
    "1 & 0 & 0 & \\dots & 0 \\\\\n",
    "0 & 1 & 0 & \\dots & 0 \\\\\n",
    "0 & 0 & 1 & \\dots & 0 \\\\\n",
    "\\dots \\\\\n",
    "0 & 0 & 0 & \\dots & 1\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The identity matrx *has no effect* when multiplied by another matrix or vector. (Obviously, it must be dimension compatible to be multiplied at all)\n",
    "\n",
    "$IA=A=AI$ and $I{\\bf x}={\\bf x}$.\n",
    "\n",
    "It is generated by `np.eye(n)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:11:07.161029Z",
     "start_time": "2020-10-01T14:11:07.156042Z"
    }
   },
   "outputs": [],
   "source": [
    "print_matrix(\"I\", np.eye(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:11:14.259191Z",
     "start_time": "2020-10-01T14:11:14.253208Z"
    }
   },
   "outputs": [],
   "source": [
    "# your identity never changes anything\n",
    "print_matrix(\"Ix\", np.eye(3) @ np.array([4,5,6]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:11:24.898759Z",
     "start_time": "2020-10-01T14:11:24.890781Z"
    }
   },
   "outputs": [],
   "source": [
    "print_matrix(\"AI\", np.array([[1,2,3],[4,5,6],[7,8,9]]) @ \n",
    "             np.eye(3))\n",
    "print_matrix(\"IA\", np.eye(3) @ np.array([[1,2,3],[4,5,6],[7,8,9]]) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any scalar multiple of the identity corresponds to a function which uniformly scales vectors:\n",
    "$$(cI){\\bf x} = c{\\bf x}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:11:45.466948Z",
     "start_time": "2020-10-01T14:11:45.457467Z"
    }
   },
   "outputs": [],
   "source": [
    "a = 0.5\n",
    "x = np.array([4,5,6]) \n",
    "aI = np.eye(3) * a\n",
    "print(\"c=\",a)\n",
    "print_matrix(\"cI\", aI)\n",
    "print_matrix(\"(cI){\\\\bf x}\\n\", aI @ x)\n",
    "\n",
    "# the same thing:\n",
    "print_matrix(\"c{\\\\bf x}\\n\",a*x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero\n",
    "The zero matrix is all zeros, and is defined for any matrix size $m\\times n$. It is written as $0$. Multiplying any vector or matrix by the zero matrix results in a result consisting of all zeros. The 0 matrix maps all vectors onto the zero vector (the origin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:11:58.749576Z",
     "start_time": "2020-10-01T14:11:58.745587Z"
    }
   },
   "outputs": [],
   "source": [
    "z = np.zeros((4,4))\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:12:09.368510Z",
     "start_time": "2020-10-01T14:12:09.358536Z"
    }
   },
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])\n",
    "y = np.array([[1,-1,1], [1,1,-1], [1,1,1], [-1,-1,-1]])\n",
    "print_matrix(\"x\", x)\n",
    "print_matrix(\"y\", y)\n",
    "print_matrix(\"0x\", z @ x)\n",
    "print()\n",
    "print_matrix(\"0y\", z @ y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Square\n",
    "A matrix is square if it has size $n\\times n$. Square matrices are important, because they apply transformations *within* a vector space; a mapping from $n$ dimensional space to $n$ dimensional space; a map from $\\real^n \\rightarrow \\real^n$. \n",
    "\n",
    "They represent functions mapping one domain to itself. Square matrices are the only ones that:\n",
    "* have an inverse \n",
    "* have determinants\n",
    "* have an eigendecomposition\n",
    "\n",
    "which are all ideas we will see in the following unit.\n",
    "\n",
    "## Triangular\n",
    "A square matrix is triangular if it has non-zero elements only above (**upper triangular**) or below the diagonal (**lower triangular**), *inclusive of the diagonal*.\n",
    "\n",
    "**Upper triangular**\n",
    "$$\\begin{bmatrix}\n",
    "1 & 2 & 3 & 4 \\\\\n",
    "0 & 5 & 6 & 7 \\\\\n",
    "0 & 0 & 8 & 9 \\\\\n",
    "0 & 0 & 0 & 10 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "**Lower triangular**\n",
    "$$\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 \\\\\n",
    "2 & 3 & 0 & 0 \\\\\n",
    "4 & 5 & 6 & 0 \\\\\n",
    "7 & 8 & 9 & 10 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "These represent particularly simple to solve sets of simultaneous equations. For example, the lower triangular matrix above can be seen as the system of equations:\n",
    "\n",
    "$$x_1 = y_1\\\\\n",
    "2x_1 + 3x_2 = y_2\\\\ \n",
    "4x_1 + 5x_2 + 6x_3 = y_3\\\\ \n",
    "7x_1 + 8x_2 + 9x_3 + 10x_4 = y_4\\\\ \n",
    "$$\n",
    "\n",
    "which, for a given $y_1, y_2, y_3, y_4$ is trivial to solve by substitution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-01T14:13:36.995921Z",
     "start_time": "2020-10-01T14:13:36.987436Z"
    }
   },
   "outputs": [],
   "source": [
    "# tri generates an all ones lower triangular matrix\n",
    "upper = np.tri(4) \n",
    "print_matrix(\"T_u\", upper)\n",
    "\n",
    "# transpose changes a lower triangular to an upper triangular\n",
    "lower = np.tri(4).T \n",
    "print_matrix(\"T_l\", lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "* [**3blue1brown Linear Algebra series**](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)  (**strongly recommended**)\n",
    "\n",
    "* [**Introduction to applied linear algebra**](http://stanford.edu/~boyd/vmls/vmls.pdf ) *by S. Boyd and L. Vandenberghe* \n",
    " \n",
    " \n",
    "## Beyond this course\n",
    "* **Linear Algebra Done Right** *by Sheldon Axler* (excellent introduction to the \"pure math\" side of linear algebra) ISBN-13: 978-0387982588\n",
    "\n",
    "* **Coding the Matrix: Linear Algebra through Applications to Computer Science** *by Philip N Klein* (top quality textbook on how linear algebra is implemented, all in Python) ISBN-13: 978-0615880990\n",
    "\n",
    "* **Linear Algebra and Learning from Data** *Gilbert Strang*, ISBN-13: 978-069219638-0, explains many detailed aspects of linear algebra and how they relate to data science.\n",
    "\n",
    "## Way beyond this course\n",
    "* [**The Matrix Cookbook**](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) *by Kaare Brandt Petersen and Michael Syskind Pedersen*. If you need to do a tricky calculation with matrices, this book will probably tell you how to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "998px",
    "left": "0px",
    "right": "1740px",
    "top": "93px",
    "width": "180px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
