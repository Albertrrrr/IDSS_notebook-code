{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$('#menubar').toggle();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Data Science and Systems (M)\n",
    "\n",
    "## Lecture Week 3: Computational linear algebra - *linear systems, inversion and matrix decompositions*\n",
    "----\n",
    " ##### University of Glasgow - material prepared by John H. Williamson (adapted to IDSS by BSJ, MZ and NP), <small>v20222023a</small>\n",
    " \n",
    " \n",
    " $$\\newcommand{\\vec}[1]{{\\bf #1}} \n",
    " \\newenvironment{examinable}{}{{}}\n",
    "\\newcommand{\\real}{\\mathbb{R}}\n",
    "\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "%\\vec{x}\n",
    "%\\real\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "By the end of this unit you should know:\n",
    "\n",
    "* how discrete problems can be modelled using continuous mathematics, i.e. using matrices\n",
    "    * how graphs can be represented as matrices\n",
    "    * how flows on graphs can be represented as matrix operations\n",
    "* what eigenvectors and eigenvalues are \n",
    "    * how the power iteration method can compute them \n",
    "    * how they can be used to decompose matrices\n",
    "* what the trace and determinant are, and the geometric intuition underlying them\n",
    "* what positive (semi-)definiteness means and why it is important\n",
    "* what the singular value decomposition (SVD) is and how it can can be used to compute functions of matrices\n",
    "* what a linear system of equations is and how it can be represented by a matrix\n",
    "    * what matrix inversion is and how it relates to solving linear systems of equations\n",
    "    * the numerical problems with direct inversion\n",
    "    * what the pseudo-inverse is, how it is derived from the SVD, and how it can be used\n",
    "* how to normalise data by using matrix operations to \"whiten\" it\n",
    "* what a low-rank approximation is and why you might use it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\newcommand{\\vec}[1]{{\\bf #1} } \n",
    "\\newenvironment{examinable}{}{\\ \\ [\\spadesuit]}\n",
    "\\newcommand{\\real}{\\mathbb{R}}\n",
    "\\newcommand{\\expect}[1]{\\mathbb{E}[#1]}\n",
    "\\DeclareMathOperator*{\\argmin}{arg\\,min}\n",
    "%\\begin{examinable}\n",
    "%\\vec{x}\n",
    "%\\in\n",
    "%\\real\n",
    "%\\end{examinable}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "IPython.display.HTML(\"\"\"\n",
    "<script>\n",
    "  function code_toggle() {\n",
    "    if (code_shown){\n",
    "      $('div.input').hide('500');\n",
    "      $('#toggleButton').val('Show Code')\n",
    "    } else {\n",
    "      $('div.input').show('500');\n",
    "      $('#toggleButton').val('Hide Code')\n",
    "    }\n",
    "    code_shown = !code_shown\n",
    "  }\n",
    "\n",
    "  $( document ).ready(function(){\n",
    "    code_shown=false;\n",
    "    $('div.input').hide()\n",
    "  });\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" id=\"toggleButton\" value=\"Show Code\"></form>\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from jhwutils.matrices import print_matrix, show_matrix_effect\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rc('figure', figsize=(8.0, 8.0), dpi=140)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphs as matrices\n",
    "\n",
    "## Example: distributing packages\n",
    "\n",
    "<img src=\"imgs/distribution.jpg\" width=80%> <br>*[[Image](https://flickr.com/photos/nseika/8096899965 \"P1070904_DxO\") by [nSeika](https://flickr.com/people/nseika) shared [CC BY](https://creativecommons.org/licenses/by/2.0/)]*.\n",
    "\n",
    "You run a large logistics company. You have to route packages between distributions centres efficiently, so they will be ready for local delivery. To do this, you need to be able to predict which warehouses are going to receive lots of packages (maybe they are connected to other sites by several direct motorways) and which will receive few packages (maybe they are remote).\n",
    "\n",
    "How can this problem be modelled? If we can make the assumption that the flow from site to site is **linear** -- that the packages arriving at one site is a weighted sum of the packages currently at each of the other sites -- then we can model the problem with linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might model the connectivity of distribution centres as a **graph**.\n",
    "A **directed graph** connects **vertices** by **edges**. \n",
    "The definition of a graph is $G=(V,E)$, where $V$ is a set of vertices and $E$ is a set of edges connecting pairs of vertices.\n",
    "\n",
    "<img src=\"imgs/graph.png\">\n",
    "\n",
    "The graph above has 8 vertices $(A,B,C,D,E,F,G,H)$ and 11 edges:\n",
    "\n",
    "$$    A \\rightarrow B \\\\\n",
    "      A \\rightarrow C \\\\\n",
    "      A \\rightarrow D \\\\\n",
    "      B \\rightarrow D \\\\\n",
    "      B \\rightarrow E \\\\\n",
    "      C \\rightarrow B \\\\\n",
    "      C \\rightarrow D \\\\\n",
    "      D \\rightarrow F \\\\\n",
    "      D \\rightarrow G \\\\\n",
    "      D \\rightarrow H \\\\\n",
    "      H \\rightarrow A $$\n",
    "\n",
    "We can write this as an **adjacency matrix**. We number each vertex $0, 1, 2, 3, \\dots$. We then create a square matrix $A$ whose elements are all zero, except where there is an edge from $V_i$ to $V_j$, in which case we set $A_{ij} = 1$. The graph shown above has the adjacency matrix:\n",
    "\n",
    "         A B C D E F G H\n",
    "      \n",
    "     A   0 1 1 1 0 0 0 0\n",
    "     B   0 0 0 1 1 0 0 0\n",
    "     C   0 1 0 1 0 0 0 0\n",
    "     D   0 0 0 0 0 1 1 1\n",
    "     E   0 0 0 0 0 0 0 0\n",
    "     F   0 0 0 0 0 0 0 0 \n",
    "     G   0 0 0 0 0 0 0 0\n",
    "     H   1 0 0 0 0 0 0 0\n",
    "\n",
    "(the letters aren't part of the matrix and are just shown for clarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing graph properties\n",
    "\n",
    "There are some graph properties which we can compute easily from this binary matrix:\n",
    "* The *out-degree* of each vertex (number of edges leaving a vertex) is the sum of each row.\n",
    "* The *in-degree* of each vertex (number of edges entering a vertex) is the sum of each column.\n",
    "* If the matrix is symmetric it represents an undirected graph; this is the case if it is equal to its transpose.\n",
    "* A directed graph can be converted to an undirected graph by computing $A^\\prime = A + A^T$. This is equivalent to making all the arrows bi-directional.\n",
    "* If there are non-zero elements on the diagonal, that means there are edges connecting vertices to themselves (self-transitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our adjacency matrix:\n",
    "adj = np.array([[0, 1, 1, 1, 0, 0, 0, 0],\n",
    "                [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "                [0, 1, 0, 1, 0, 0, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 1, 1, 1],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                [0, 0, 0, 0, 0, 0, 0, 0], \n",
    "                [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                [1, 0, 0, 0, 0, 0, 0, 0]])\n",
    "\n",
    "# compute in-degrees and out-degrees\n",
    "in_degrees = np.sum(adj, axis=0)\n",
    "out_degrees = np.sum(adj, axis=1)\n",
    "print('In degrees: ', list(zip('ABCDEFGH', in_degrees)))\n",
    "print('Out degrees:', list(zip('ABCDEFGH', out_degrees)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the graph undirected?\n",
    "print(np.allclose(adj, adj.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to *force* our adjacency matrix to be symmetric,\n",
    "# i.e. convert the graph from directed to undirected, \n",
    "# we can add it to its transpose\n",
    "adj_sym = adj + adj.T\n",
    "print(adj_sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any self transitions?\n",
    "print(np.diag(adj))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edge-weighted graphs\n",
    "\n",
    "If the some of the connections between distibution centres are stronger than others, e.g. if they are connected by bigger roads, we can model this using edge weights. Now the entry at $A_{ij}$ represents the weight of the connection from vertex $V_i$ to $V_j$.\n",
    "\n",
    "For example, the graph below has weighted edges:\n",
    "\n",
    "<img src=\"imgs/graph_weighted.png\" width=20%>\n",
    "\n",
    "And the adjacency matrix is:\n",
    "\n",
    "          A     B     C     D     E     F     G     H\n",
    "         \n",
    "     A   0.00  2.00  1.00  0.50  0.00  0.00  0.00  0.00\n",
    "     B   0.00  0.00  0.00  0.25  3.00  0.00  0.00  0.00\n",
    "     C   0.00  2.00  0.00  4.00  0.00  0.00  0.00  0.00\n",
    "     D   0.00  0.00  0.00  0.00  0.00  0.50  2.00  0.50\n",
    "     E   0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
    "     F   0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
    "     G   0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
    "     H   3.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00\n",
    "     \n",
    "The easiest way to visualise this matrix is by plotting it as an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [[0.0, 2.0, 1.0, 0.5, 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.25, 3.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 2.0, 0.0, 4.0 , 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0 , 0.0, 0.5, 2.0, 0.5],\n",
    "    [0.0, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0],\n",
    "    [0.0, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0],\n",
    "    [3.0, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0]]\n",
    "\n",
    "# plot the adjacency matrix A as an image plot\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "img = ax.imshow(A)\n",
    "ax.set_xticks(np.arange(8))\n",
    "ax.set_xticklabels(\"ABCDEFGH\")\n",
    "ax.set_yticks(np.arange(8))\n",
    "ax.set_yticklabels(\"ABCDEFGH\")\n",
    "fig.colorbar(img)\n",
    "ax.set_title(\"Adjacency matrix as an image plot\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think of graphs as representing flows of \"mass\" through a network of vertices.\n",
    "\n",
    "* If the total flow out of a vertex is $>1$, i.e. its row sums to $>1$, then it is a **source** of mass; for example it is *manufacturing* things.\n",
    "* If the total flow out of a vertex is $<1$, i.e. its row sums to $<1$, then it is a **sink**; for example it is *consuming* things.\n",
    "* If the total flow out of the vertex is $1$ exactly, i.e. its row sums to $1$ exactly, then it conserves mass; it only ever *re-routes* things.\n",
    "\n",
    "Can you see which vertices are sources and sinks in the above graph?\n",
    "\n",
    "If the whole graph consists of vertices whose total outgoing weight is 1.0, and all weights are positive or zero, then the whole graph preserves mass under flow. Nothing is produced or consumed. Every row in the adjacency matrix $A$ sums to 1. This is called a **conserving adjacency matrix**. We can normalise the rows of any positive matrix $A$ (so long as each vertex has at least some flow out of it) to form a conserving adjacency matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(A,axis=0)) # along rows\n",
    "print(np.sum(A,axis=1)) # along collums\n",
    "print(np.sum(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flow analysis: using matrices to model discrete problems\n",
    "\n",
    "Previously, we have talked about how matrices perform geometrical transformations on vectors. Matrices are sometimes referred to as *linear maps* because they map from one vector to another, using only linear operations. The adjacency matrix does this too.\n",
    "\n",
    "What does this mean? What vectors does it operate on? What does the geometrical transformation (linear mapping) represent?\n",
    "\n",
    "At any point in time, we can write down the proportion of packages at each depot as a vector $\\vec{x_t} \\in \\real^{V}$, where $V$ is the number of vertices in the graph (number of depots), e.g.\n",
    "\n",
    "$$\\vec{x_t} = \\begin{bmatrix} 0.05 & 0.15 & 0.30 & 0.20 & 0.03 & 0.12 & 0.08 & 0.07 \\end{bmatrix}$$\n",
    "\n",
    "The flow of packages (per day) between depots is a linear map $\\real^{V} \\rightarrow \\real^{V}$. This is represented by the adjacency matrix $A \\in \\real^{V \\times V}$ (a square matrix).\n",
    "\n",
    "This allows us to analyse an apparently *discrete* problem (connectivity of graphs) with tools from *continuous* mathematics (vectors and matrices).\n",
    "\n",
    "The switch of viewpoints from discrete to continuous (and vice versa) is a very powerful and fundamental step in data analysis. It might not seem like it, but the flow of packages can be modelled as some rigid rotation and scaling in a high-dimensional space.\n",
    "\n",
    "We saw the same kind of application in Topic 1, when we transformed the translation problem from a problem of strings (discrete) to a problem in vector spaces (continuous) and made it solvable.\n",
    "\n",
    "A great many problems can be represented this way:\n",
    "\n",
    "* packages moving between depots (how many packages at each depot at an instant in time)\n",
    "* users moving between web pages (how many users on each webpage)\n",
    "* cancer cells moving between tumour sites (how many cancer cells at each tumour site)\n",
    "* trade between states (how many items in each state)\n",
    "* shoppers walking between retailers (how many shoppers in each shop)\n",
    "* traffic across a load-balancing network (how many cars at each junction)\n",
    "* NPCs (non-player characters) moving between towns in a game (how many NPCs in each town)\n",
    "* fluid moving between regions (how much fluid in each tank)\n",
    "* blood flowing between organs (how much blood in each organ)\n",
    "* beliefs moving among hypotheses (how much we believe in each hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation of changes in package distribution over time\n",
    "\n",
    "Suppose we start with an *initial distribution* of packages: a vector $\\vec{x_{t=0}}$. How many packages will be at each depot tomorrow? This will be given by the (row) vector $\\vec{x_{t=1}}$, which can be computed as follows:\n",
    "\n",
    "$$\\vec{x_{t=1}} = \\vec{x_{t=0}}\\,A = A^T \\vec{x_{t=0}}^T $$\n",
    "\n",
    "We can simulate the flow *over the whole network* in one go with just one matrix multiplication. This \"rotates\" the distribution of packages from today to tomorrow. The advantage of vectorised operations is that they can be accelerated using hardware such as a GPU (Graphics Processing Unit).\n",
    "\n",
    "N.B. Because we are working with row vectors, the adjacency matrix post-multiplies the package-state vector. This is due to the way the adjacency matrix is defined. For other applications we often work with column vectors, in which case the matrix pre-multiplies the vector.\n",
    "    \n",
    "-----    \n",
    "    \n",
    "## Important questions    \n",
    "There are some harder questions we can ask:\n",
    "* What about in a week's time? What will $\\vec{x_{t=7}}$ be?\n",
    "* What about in *one hour's* time (i.e. a 24th of a day)? What will be $\\vec{x_{t=1/24}}$ be?\n",
    "* What about at time infinity $\\vec{x_{t=\\infty}}$? What is the long term behaviour? Will the system reach a steady state (an **equilibrium**)? Or will it oscillate forever?\n",
    "* What about if we wanted to go backwards in time? If we know $\\vec{x_{t=0}}$, can we predict yesterday $\\vec{x_{t=-1}}$?\n",
    "\n",
    "We will solve these problems today, using some new operations that we can do with *certain kinds* of matrices:\n",
    "\n",
    "---\n",
    "\n",
    "## New matrix operations\n",
    "* Matrices can be exponentiated: $C = A^n$; this \"repeats\" the effect of matrix (e.g. $C= AAAA$)\n",
    "* Matrices can be inverted: $C = A^{-1}$; this undoes the effect of a matrix \n",
    "* We can find eigenvalues: $A\\vec{x} = \\lambda \\vec{x}$; this identifies specific vectors $\\vec{x}$ that are *only* scaled by a factor $\\lambda$ (not rotated) when transformed by matrix $A$.\n",
    "* Matrices can be factorised: $A = U\\Sigma V^T$; any matrix can expressed as the product of three other matrices with special forms. \n",
    "* We can measure some properties of $A$ numerically, including the **determinant**, **trace** and **condition number**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix powers (exponentiation)\n",
    "\n",
    "Since we have already defined matrix multiplication, we can now define $A^2=AA$, $A^3=AAA$, $A^4=AAAA$, etc. These are the **powers** of a matrix, and are only defined for square matrices. Matrix exponentiation is the **repeated application** of a matrix, and it can only be defined for square matrices because we'd otherwise change the dimensions after the first step and be unable to reapply the same matrix.\n",
    "\n",
    "This answers the question, **\"What about in a week's time? What will $\\vec{x_{t=7}}$ be?\"** We can simply apply the matrix seven times; raising it to the power of 7.\n",
    "\n",
    "Matrix powers are very easy to compute for positive, whole powers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that raises a matrix A to the power n\n",
    "def powm(A, n):    \n",
    "    B = np.eye(A.shape[0]) # start with identity    \n",
    "    for i in range(n):\n",
    "        B = A @ B  # @ performs matrix multiplication\n",
    "    return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate raising a matrix A to powers from 0 to 5\n",
    "A = np.array([[1.5, 0.0], [-1.1, 0.1]])\n",
    "print_matrix(\"A\", A)\n",
    "\n",
    "for i in range(5):\n",
    "    print_matrix(\"A^%d\" % i, powm(A, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix powers correspond to applying a matrix multiple times, compounding the effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the effect of applying a 7-degree rotation matrix repeatedly,\n",
    "# i.e. raising it to powers from 0 to 5\n",
    "\n",
    "rad = np.radians(7) # converts 7 degrees into radians.\n",
    "c = np.cos(rad)\n",
    "s = np.sin(rad)\n",
    "rot = np.array([[c, s], [-s, c]])\n",
    "print_matrix(\"A\", rot)\n",
    "\n",
    "# display plots of original and transformed matrix for several different powers\n",
    "for i in range(7):\n",
    "    show_matrix_effect(powm(rot, i), 'A^%d' % i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stable point: all roads lead to Rome\n",
    "\n",
    "Here is a simple experiment. Imagine we have a conserving adjacency matrix modelling the flow of packages around a network of depots. If we start with different package distributions and then let the \"natural\" flow begin, what will happen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_nodes = 30\n",
    "\n",
    "# generate a random transition matrix\n",
    "adjacency = np.random.uniform(0, 1, (n_nodes, n_nodes))**50\n",
    "\n",
    "# normalise rows to make a conserving adjacency matrix !\n",
    "adjacency = (adjacency.T / np.sum(adjacency, axis=1)).T\n",
    "\n",
    "# show the adjacency matrix\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "img = ax.imshow(adjacency, cmap='magma')\n",
    "ax.set_title(\"Adjacency matrix\")\n",
    "fig.colorbar(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we do some experiments where we try starting the system in different states and see how it evolves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we do some experiments where we try starting the system in different states \n",
    "# and see how it evolves (we can imagine a system of tanks, where we put dye in \n",
    "# one tank to begin with, and see where it ends up after we turn the pump on)\n",
    "timesteps = 60\n",
    "\n",
    "# run the experiment a few times with different starting states\n",
    "for j in range(5):\n",
    "    test_vector = np.zeros((1, n_nodes))  # initialise a row vector\n",
    "    \n",
    "    # choose a random node to start all the packages in\n",
    "    node = np.random.randint(0, n_nodes)\n",
    "    test_vector[0][node] = 1\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "    # initialise a list to store vectors at every time point\n",
    "    vectors = []\n",
    "    vectors.append(test_vector.T) # store as column vector\n",
    "    \n",
    "    # let the system evolve over a number of timesteps\n",
    "    for i in range(timesteps):\n",
    "        test_vector = test_vector @ adjacency\n",
    "        vectors.append(test_vector.T)        \n",
    "\n",
    "    # plot the state vector as a function of time\n",
    "    all_vectors = np.hstack(vectors)\n",
    "    ax.imshow(all_vectors)\n",
    "    ax.set_xlabel(\"Time steps\")\n",
    "    ax.set_ylabel(\"Depot\")\n",
    "    ax.set_title(\"Starting at depot %d\" % node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprised...? No matter what the initial package distribution is, after several days the distribution will settle down to a **steady state** (or stable point). This vector is one of the *eigenvectors* of the adjacency matrix. Applying the matrix again does not change the distrbution of the packages, i.e. $\\vec{x}_{t=60}=\\vec{x}_{t=59}A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigenvalues and eigenvectors\n",
    "\n",
    "A matrix represents a special kind of function: a **linear transform**; an operation that performs rotation and scaling on vectors. However, there are certain vectors which don't get rotated when multiplied by the matrix. \n",
    "\n",
    "Special vectors: They only get scaled (stretched or compressed). These vectors are called **eigenvectors**, and they can be thought of as the \"fundamental\" or \"characteristic\" vectors of the matrix, as they have some stability. The prefix **eigen** just means **characteristic** (from the German for \"own\"). The scaling factors that the matrix applies to its eigenvectors are called **eigenvalues**.\n",
    "\n",
    "We can visualise the effect of a matrix transformation by imagining a parallelepiped (whose edges are vectors) being rotated, stretched and compressed. If the edges of the parallelelpiped are the eigenvectors of the matrix, the parallelepiped will only be stretched or compressed, not rotated. If the edges of this parallelepiped have unit length, then after the transformation their lengths will be equal to the eigenvalues. The youtube video series on linear algebra by [3blue1brown](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) illustrates this particularly well.\n",
    "\n",
    "<img src=\"imgs/eigs.png\">\n",
    "\n",
    "When we are dealing with $n$-dimensional matrices, we have to imagine an $n$-dimensional parallelotope being transformed in the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to find the leading eigenvector: the power iteration method\n",
    "\n",
    "Our example, \"All roads lead to Rome\", illustrates how to find one of the eigenvectors of a matrix. We simply apply the matrix repeatedly to a random initial vector and wait until the result converges to a steady state. However, there is one important caveat ...\n",
    "\n",
    "What happens if we apply a square matrix $A$ to a vector $\\vec{x}$ of any length, then take the resulting vector and apply $A$ again, and so on?\n",
    "\n",
    "Let's work with column vectors now, so that $A$ pre-multiplies the vector. If we compute\n",
    "\n",
    "$$\\vec{x_n} = AAAA\\dots AA\\vec{x_0} \\\\ = A^{n}\\vec{x_0}$$\n",
    "\n",
    "this **will generally either explode in value or collapse to zero**. However, we can fix the problem by **normalizing** the resulting vector after each application of the matrix:\n",
    "\n",
    "$$\\begin{examinable} \\vec{x_n} = \\frac{A \\vec{x_{n-1}}}{\\|A\\vec{x_{n-1}}\\|_\\infty} \\end{examinable}$$\n",
    "\n",
    "The vector will be forced back to unit norm at each step, using the $L_\\infty$ norm. This process is called **power iteration**.\n",
    "\n",
    "N.B. You can use any norm for normalisation. The infinity norm has been used traditionally, probably for reasons of efficiency.\n",
    "\n",
    "Let's demonstrate power iteration with a random $3 \\times 3$ matrix $A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_iterate(A, x, n):        \n",
    "    for i in range(n):\n",
    "        x = A @ x # matrix multiplication\n",
    "        x = x / np.linalg.norm(x, np.inf) # normalize x using the L-infinity norm during power iteration\n",
    "        \n",
    "    return x / np.linalg.norm(x, 2) # finally, divide by the L2 norm so x can be compared with result of np.linalg.eig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random 3 x 3 matrix\n",
    "A = np.random.normal(0, 1, (3, 3))\n",
    "# make it symmetric to ensure all eigenvalues are real (not complex)\n",
    "A = A + A.T\n",
    "print_matrix(\"A\", A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a random 3-element column vector\n",
    "random_vec = np.random.normal(0, 1, (3, 1))\n",
    "print_matrix(\"\\\\bf{x_0}\", random_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do power iteration on this vector for enough iterations to ensure it converges to the leading eigenvector\n",
    "xn = power_iterate(A, random_vec, n=500)\n",
    "print_matrix(\"\\\\bf{x_{n}}\", xn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of which vector $\\vec{x_0}$ we start with (the vector above is chosen randomly), the power iteration method always approaches a fixed vector (though possibly with sign flips). This is true for almost every square matrix. \n",
    "\n",
    "The vector that results from power iteration is known as the **leading eigenvector**. It satisfies the definition of an eigenvector because the matrix $A$ performs only scaling on this vector (no rotation). We know this **must** be true, because the scaling effect is eliminated by the normalisation step in the power iteration, but any other effects pass through. We can write the scaling effect of the $A$ on an eigenvector $\\vec{x}$ as follows:\n",
    "\n",
    "$$A\\vec{x} = \\lambda \\vec{x}$$\n",
    "\n",
    "where $\\lambda$ is the eigenvalue. We can calculate $\\lambda$ simply by dividing the vector $A\\vec{x}$ element-wise by the vector $\\vec{x}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = (A @ xn) / xn # elementwise\n",
    "print_matrix(\"\\\\frac{A\\\\bf x}{\\\\bf x}\", ratio)\n",
    "eigenvalue = ratio[0][0] # all elements of ratio will be the same\n",
    "print_matrix(\"\\lambda\", eigenvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print((A @ xn))\n",
    "print(eigenvalue * xn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing eigenvectors and eigenvalues with Numpy\n",
    "\n",
    "The power iteration method enables us to calculate the leading eigenvector and eigenvalue, but if we want to know **all** the (linearlly independent) eigenvectors and eigenvalues of a matrix, we can use `np.linalg.eig`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute eigenvalues and eigenvectors with Numpy\n",
    "print(\"A=\\n%s\\n\" % A)\n",
    "evals, evecs = np.linalg.eig(A)  # evecs is a matrix whose columns are the unit eigenvectors\n",
    "print(\"Eigenvalues:\\n\", evals)\n",
    "print()\n",
    "print(\"Unit Eigenvectors (columns):\\n\", evecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leading eigenvector and eigenvalue should match the results we obtained by power iteration.\n",
    "\n",
    "Notice that our $3 \\times 3$ matrix has 3 eigenvalues and 3 eigenvectors. In general, for an $n \\times n$ matrix, `np.linalg.eig` will yield $n$ eigenvalues and $n$ eigenvectors. The eigenvectors are **orthogonal**, i.e. the dot product of any pair of eigenvectors is zero.\n",
    "\n",
    "For very large matrices, if you just want to compute the leading eigenvector, power iteration is much faster than using `np.linalg.eig`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formal definition of eigenvectors and eigenvalues\n",
    "\n",
    "Now that we have explored the concepts of eigenvectors and eigenvalues through practical examples, we can move on to a formal definition:\n",
    "\n",
    "Consider a vector function $f(\\vec{x})$. There may exist vectors such that $f(\\vec{x}) = \\lambda\\vec{x}$. The function maps these vectors to scaled versions of themselves. No rotation or skewing is applied, just pure scaling.\n",
    "\n",
    "Any square matrix $A$ represents a function $f(\\vec{x})$ and may have vectors like this, such that \n",
    "\n",
    "$$\n",
    "A\\vec{x}_i = \\lambda_i \\vec{x}_i\n",
    "$$\n",
    "\n",
    "Each vector $\\vec{x}_i$ satisfying this equation is known as an **eigenvector** and each corresponding factor $\\lambda_i$ is known as an **eigenvalue**. \n",
    "\n",
    "For any matrix, the **eigenvalues** are uniquely determined, but the eigenvectors are not. There may be many eigenvectors corresponding to any given eigenvalue (for example, pairs of vectors that point in opposite directions).\n",
    "\n",
    "**Eigenproblems** are problems that can be tackled using eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The eigendecomposition: revisiting the package distribution problem\n",
    "\n",
    "Let's revisit our package distribution example and compute all the eigenvectors and eigenvalues of an adjacency matrix. \n",
    "\n",
    "We will work with an *undirected* (symmetric) graph to ensure that the eigenvalues are all real (an adjacency matrix whose eigenvalues are all complex would have no steady state, but instead would converge to an oscillatory state).\n",
    "\n",
    "N.B. Making the adjacency matrix symmetric also means we don't have to worry about transposing it before using `np.linalg.eig` (the adjacency matrix is defined such that it operates on row vectors, but `np.linalg.eig` returns column eigenvectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up an adjacency matrix A to represent package flow\n",
    "A = np.array([[0.0, 0.7, 0.9, 1.0, 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.5, 0.9, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.6, 0.0, 0.4 , 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0 , 0.0, 0.8, 0.6, 0.3],\n",
    "              [0.0, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.0, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0],\n",
    "              [0.8, 0.0, 0.0, 0.0 , 0.0, 0.0, 0.0, 0.0]])\n",
    "\n",
    "# make A symmetric (undirected)\n",
    "A = (A + A.T) * 0.5\n",
    "\n",
    "# plot adjacency matrix as image\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "img = ax.imshow(A)\n",
    "ax.set_xticks(np.arange(8))\n",
    "ax.set_xticklabels(\"ABCDEFGH\")\n",
    "ax.set_yticks(np.arange(8))\n",
    "ax.set_yticklabels(\"ABCDEFGH\")\n",
    "fig.colorbar(img)\n",
    "ax.set_title(\"Adjacency matrix, as an image plot\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# compute eigendecomposition of A\n",
    "evals, evecs = np.linalg.eig(A)\n",
    "print(evals)\n",
    "\n",
    "# plot eigenvectors as an image, in order of decreasing eigenvalue\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "order = np.argsort(-np.abs(evals))\n",
    "img = ax.imshow(((evecs[:, order] * evals[order])), cmap='seismic', vmin=-1, vmax=1)\n",
    "ax.set_yticklabels(\"ABCDEFGH\")\n",
    "ax.set_yticks(np.arange(8))\n",
    "ax.set_xlabel(\"Eigenvector rank\")\n",
    "ax.set_ylabel(\"Depot\")\n",
    "fig.colorbar(img)#, orientation='horizontal')\n",
    "ax.set_title(\"Eigenvectors of A, scaled by eigenvalue\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The leading eigenvector tells us what the steady state of the package distribution will be if we keep the system running for long enough. For this distribution network, most of the packages will pile up at A, B, C, D and H.\n",
    "\n",
    "From the eigendecomposition we can get a feel for which eigenvectors are large and which are small. In the case of package distribution, small eigenvalues correspond to route patterns that are rarely used, and large eigenvalues to the dominant path of packages moving through the system.\n",
    "\n",
    "## The eigenspectrum\n",
    "\n",
    "The **eigenspectrum** is just the sequence of absolute eigenvalues, ordered by magnitude $|\\lambda_1|>|\\lambda_2|>|>\\dots>|\\lambda_n|$. This *ranks* the eigenvectors in order of \"importance\". As we shall see later, this can be useful in finding \"simplified\" versions of linear transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort eigenvalues by absolute value \n",
    "print_matrix(\"|\\lambda_i|\", np.abs(evals[np.argsort(-np.abs(evals))]))\n",
    "\n",
    "# plot the eigenspectrum (magnitudes of eigenvalues in decreaing order)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.bar(np.arange(len(evals)), np.abs(evals[np.argsort(-np.abs(evals))]))\n",
    "ax.set_title(\"Eigenspectrum of A\")\n",
    "ax.set_xlabel(\"Eigenvalue rank\")\n",
    "ax.set_ylabel(\"$|\\lambda|$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warning: Numerical instability of eigendecomposition algorithms\n",
    "\n",
    "A word of warning: `np.linalg.eig` can suffer from numerical instabilities due to rounding errors resulting from limitations on floating point precision. This means that sometimes the smallest eigenvectors are not completely orthogonal. `np.linalg.eig` is often sufficient for most purposes, but be careful how you use it. \n",
    "\n",
    "If your matrix satisfies certain special conditions, you might be able to use a more stable algorithm. For example, if it is real and symmetric (or Hermitian, in the case of a complex matrix), you can use `np.linalg.eigh`. \n",
    "\n",
    "Here we will just demonstarte that you need to be careful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute eigendecomposition of A using np.linalg.eig\n",
    "evals, evecs = np.linalg.eig(A)\n",
    "\n",
    "# Get the indices of the eigenvalues in order of largest to smallest (absolute value)\n",
    "order = np.argsort(-np.abs(evals))\n",
    "\n",
    "# Reorder eigenvalues and eigenvectors from largest to smallest eigenvalue (absolute value)\n",
    "evals = evals[order]\n",
    "evecs = evecs[:, order]\n",
    "\n",
    "# Display eigenvectors x_i (columns) in order from largest to smallest eigenvalue (absolute)\n",
    "print(\"Eigenvectors computed with np.linalg.eig:\")\n",
    "print_matrix(\"\\\\bf{x_i}\", evecs)\n",
    "\n",
    "print()\n",
    "\n",
    "# compute eigendecomposition of A using np.linalg.eigh (we can use this because A is real and symmetric)\n",
    "evalsh, evecsh = np.linalg.eigh(A)\n",
    "\n",
    "# Get the indices of the eigenvalues in order of largest to smallest (absolute value)\n",
    "order = np.argsort(-np.abs(evalsh))\n",
    "\n",
    "# Reorder eigenvalues and eigenvectors from largest to smallest eigenvalue (absolute value)\n",
    "evalsh = evalsh[order]\n",
    "evecsh = evecsh[:, order]\n",
    "\n",
    "# Display eigenvectors x_i (columns) in order from largest to smallest eigenvalue (absolute)\n",
    "print(\"Eigenvectors computed with np.linalg.eigh:\")\n",
    "print_matrix(\"\\\\bf{x_i}\", evecsh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the first six eigenvectors are very similar for both algorithms, but the last two are quite different. These correspond to the smallest eigenvalues. Let's compute the dot product of the last two eigenvectors for both algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dot product of last two eigenvectors obtained with np.linalg.eig:\")\n",
    "print(np.dot(evecs[:,6], evecs[:,7]))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Dot product of last two eigenvectors obtained with np.linalg.eigh:\")\n",
    "print(np.dot(evecsh[:,6], evecsh[:,7]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how rounding errors have caused the last two eigenvectors from `np.linalg.eig` to be non-orthogonal, whereas the more stable `np.linalg.eigh` yields orthogonal eigenvectors with a dot product close to zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvectors and linear maps - more intuition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1,0.5],[0.5,1]])\n",
    "#A = np.array([[1,0],[0,1]])\n",
    "#A = np.array([[1,0],[1,0]])\n",
    "\n",
    "show_matrix_effect(A, \"Symmetric\")   # rotate, then scale\n",
    "\n",
    "evals, evecs = np.linalg.eig(A) # compute eigendecomposition of A using np.linalg.eig\n",
    "order = np.argsort(-np.abs(evals)) # Get the indices of the eigenvalues in order of largest to smallest (absolute value)\n",
    "evals = evals[order] # Reorder eigenvalues and eigenvectors from largest to smallest eigenvalue (absolute value)\n",
    "evecs = evecs[:, order] # # Reorder eigenvalues and eigenvectors from largest to smallest eigenvalue (absolute value)\n",
    "print(\"Eigenvectors computed with np.linalg.eig:\") # Display eigenvectors x_i (columns) in order from largest to smallest eigenvalue (absolute)\n",
    "print_matrix(\"\\\\bf{x_i}\", evecs)\n",
    "print_matrix(\"\\lambda_i\", evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA) - analysing data with linear matrices\n",
    "\n",
    "<br>\n",
    "Let's generate a dataset in $\\mathbb{R}^2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a dataset, collect it in a matrix of 200 random data points.vectors (x_1, x_2), where x_1 and x_2 are correlated\n",
    "x = np.random.normal(0, 1, (200, 2)) @ np.array([[0.05, 0.4], [-0.9, 1.0]])  # don't try to understand this now it is just a dataset\n",
    "\n",
    "print(\"First 10 observations...\\n%s\" % x[0:10,:])\n",
    "\n",
    "# plot the dataset\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(x[:, 0], x[:, 1], c='C0', label=\"Original data\", s=3, alpha=0.8)\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-4, 4)\n",
    "#\n",
    "ax.grid()\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Mean vector\n",
    "Given our straightforward definition of vectors, we can define some  **statistics** that generalise the statistics of ordinary real numbers. These just use the definition of vector addition and scalar multiplication, along with the outer product.\n",
    "\n",
    "The **mean vector** of a collection of $N$ vectors is the sum of the vectors multiplied by $\\frac{1}{N}$:\n",
    "\n",
    "$$\\text{mean}(\\vec{x_1}, \\vec{x_2}, \\dots, \\vec{x_n}) = \\frac{1}{N} \\sum_i \\vec{x_i}$$\n",
    "\n",
    "The mean vector is the **geometric centroid** of a set of vectors and can be thought of as capturing \"centre of mass\" of those vectors. \n",
    "\n",
    "If we have vectors stacked up in a matrix $X$, one vector per row, `np.mean(x, axis=0)` will calculate the mean vector for us.\n",
    "\n",
    "#### Covariance matrix\n",
    "As well as the **mean vector**, we can also generalise the idea of **variance**, which measures the spread of a dataset, to the multidimensional case. Variance (in the 1D case, i.e. $N \\times 1$) is the sum of squared differences of each element from the mean of the vector:\n",
    "$$\\sigma^2 =  \\frac{1}{N-1} \\sum_{i=0}^{N-1} (\\vec{x_i} - \\mu)^2$$,\n",
    "\n",
    "where $\\mu$ is the mean of this $N \\times 1$ data. This is a measure of how \"spread out\" a vector of values $\\vec{x}$ is. The **standard deviation** $\\sigma$ is the square root of the **variance** and is more often used because it is in the same units as the elements of $\\vec{x}$.\n",
    "\n",
    "In the multidimensional case, to get a useful measure of spread of a $N \\times d$ data matrix  $X$ ($N$ $d$-dimensional vectors) we need to compute the *covariance* of every dimension with every other dimension. This is the average squared difference of each column of data from the average of every column. This forms a 2D array $\\Sigma$, which has entries in element $i,j$:\n",
    "\n",
    "$$\\Sigma_{ij} = \\frac{1}{N-1} \\sum_{k=1}^{N} (X_{ki}-\\mu_i)(X_{kj}-\\mu_j) $$,\n",
    "\n",
    "where $\\mu_i$ is the mean of dimension $i$. It is a *special form* of matrix: it is square, symmetric and positive semi-definite.\n",
    "\n",
    "\n",
    "It tells us *how much correlation* there is between the variables in a data set. We can plot a representation of the covariance matrix as an ellipse which aligns with the distribution of the data points. Uncorrelated data is represented by a circle, whereas strongly correlated data is represented by a long thin ellipse. \n",
    "\n",
    "Hint: The eigenvectors of the covariance matrix, scaled by their eigenvalues, form the principal axes of the ellipse.\n",
    "\n",
    "\n",
    "## Decomposition of the covariance matrix into its eigenvectors and eigenvalues\n",
    "\n",
    "The eigenvectors of the covariance matrix are called the **principal components**, and they tell us the directions in which the data varies most. This is **an incredibly useful thing to be able to do**, particularly with high-dimensional data sets where the variables may be correlated in complicated ways.\n",
    "\n",
    "The direction of principal component $i$ is given by the eigenvector $\\vec{x}_i$, and the length of the component is given by $\\sqrt{\\lambda_i}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate the covariance matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the covariance matrix for x and display it\n",
    "print_matrix(\"\\Sigma\", np.cov(x, rowvar=False))  # rowvar=False means variables are stored as columns in x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the eigenvectors and eigenvalues of the covariance matrix and plot these as the principal axes of an ellipse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are helper functions. You do not need to undersatnd these in detail. \n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def eigsorted(cov):\n",
    "    vals, vecs = np.linalg.eigh(cov)\n",
    "    order = vals.argsort()[::-1]\n",
    "    return vals[order], vecs[:, order]\n",
    "\n",
    "def cov_ellipse(ax, x, nstd=1, **kwargs):\n",
    "    cov = np.cov(x.T)\n",
    "    mu = np.mean(x, axis=0)\n",
    "    vals, vecs = eigsorted(cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[:, 0][::-1]))\n",
    "    w, h = 2 * nstd * np.sqrt(vals)\n",
    "    ell = Ellipse(xy=(mu[0], mu[1]), width=w, height=h, angle=theta, **kwargs)\n",
    "\n",
    "    ax.add_artist(ell)\n",
    "    return mu,cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from jhwutils import ellipse as ellipse\n",
    "\n",
    "# Compute the eigenvalues and eigenvectors of the 2D covariance matrix\n",
    "evals, evecs = np.linalg.eig(np.cov(x, rowvar=False))\n",
    "\n",
    "evals_sorted = evals[np.argsort(-np.abs(evals))]\n",
    "evecs_sorted = evecs[:, np.argsort(-np.abs(evals))]\n",
    "\n",
    "# Plot the dataset\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(x[:, 0], x[:, 1], c='C0', label=\"Original data\", s=3, alpha=0.8)\n",
    "plt.xlabel('$x_1$')\n",
    "plt.ylabel('$x_2$')\n",
    "\n",
    "# Plot ellipses to show covariance of data        \n",
    "cov_ellipse(ax, x[:, 0:2], 1, facecolor='none', edgecolor='C0', alpha=0.2)\n",
    "cov_ellipse(ax, x[:, 0:2], 2, facecolor='none', edgecolor='C0', alpha=0.2)\n",
    "cov_ellipse(ax, x[:, 0:2], 3, facecolor='none', edgecolor='C0', alpha=0.2)\n",
    "\n",
    "# Plot eigenvectors of covariance matrix * 3 (so they reach the radius of the 3rd ellipse)\n",
    "def plot_evec(ix):    \n",
    "    ax.plot([0, evecs_sorted[0, ix] * 3 * np.sqrt(evals_sorted[ix])], \n",
    "            [0, evecs_sorted[1, ix] * 3 * np.sqrt(evals_sorted[ix])], color='C'+str(ix+1),             \n",
    "             label='Principal component {ix}'.format(ix=ix+1))\n",
    "\n",
    "plot_evec(0)\n",
    "plot_evec(1)\n",
    "# fix details\n",
    "ax.set_xlim(-3, 3)\n",
    "ax.set_ylim(-4, 4)\n",
    "\n",
    "ax.axhline(0, lw=1, ls=':')\n",
    "ax.axvline(0, lw=1, ls=':')\n",
    "ax.set_aspect(1.0)\n",
    "ax.legend()\n",
    "ax.set_frame_on(False)\n",
    "ax.set_title(\"Principal components of a dataset\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Euclidean space, the principal components of a 2D dataset are its semi-major and semi-minor axes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigendecomposition: Reconstruction of the covariance matrix from its eigenvectors and eigenvalues\n",
    "\n",
    "We are now interested in reconstruct the covariance matrix using the eigenvectors and values. We can do this as follows:\n",
    "\n",
    "$$\\Sigma = Q\\Lambda Q^T$$\n",
    "\n",
    "where $Q$ is a matrix of unit eigenvectors $\\vec{x_i}$ (same as the output `np.linalg.eig`) and $\\Lambda$ is a diagonal matrix of eigenvalues ($\\lambda_i$ on the diagonal, zero elsewhere).\n",
    "\n",
    "Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Eigenvectors =\\n%s\\n\" % evecs)\n",
    "print(\"Eigenvalues =\\n%s\\n\" % evals)\n",
    "xx = np.cov(x, rowvar=False)\n",
    "print(xx)\n",
    "evals, evecs = np.linalg.eig(np.cov(x, rowvar=False))\n",
    "reconstructed_A = evecs @ np.diag(evals) @ evecs.T\n",
    "print_matrix(\"Q\", evecs)\n",
    "print_matrix(\"\\Lambda\", np.diag(evals))\n",
    "\n",
    "# Display reconstructed covariance matrix for comparison with original covariance matrix\n",
    "# (see previous cell)\n",
    "print_matrix(\"\\Sigma = Q\\Lambda Q^T\", reconstructed_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hey presto! We've recovered the covariance matrix. \n",
    "\n",
    "## Approximating a matrix\n",
    "\n",
    "Imagine we started with a very high dimensional data set, so $\\Sigma$ is a very large matrix. It's so large, we don't want to store it in memory. Instead, we just want to store the first few principal components and use these to reconstruct an *approximation* to $\\Sigma$. Providing we keep the largest principal components, we will probably retain most of the information. \n",
    "\n",
    "Let's demonstrate this with our 2D covariance matrix. We can approximate the matrix by *truncating* the list of eigenvalues and eigenvectors, i.e. setting the smallest of the eigenvalues and eigenvectors to 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Eigenvectors =\\n%s\\n\" % evecs)\n",
    "print(\"Eigenvalues =\\n%s\\n\" % evals)\n",
    "#print(np.linalg.norm(evecs[:,1]))\n",
    "\n",
    "# truncate small eigenvalues to 0\n",
    "approx_evals = np.where(np.abs(evals)<1e-1, 0, evals)\n",
    "print_matrix(\"\\hat{\\Lambda}\", np.diag(approx_evals))\n",
    "\n",
    "# construct an approximation to A using the truncated eigenvalue matrix\n",
    "approx_A = evecs @ np.diag(approx_evals) @ evecs.T\n",
    "print_matrix(\"\\hat{\\Sigma} = Q \\hat{\\Lambda}Q\", approx_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix approximation can be used to simplify transformations or to compress matrices for data transmission.\n",
    "\n",
    "The **eigenspectrum** gives us an idea of how simply a matrix could be approximated:\n",
    "* One large eigenvalue and many small ones - just one vector might approximate this matrix. \n",
    "* All eigenvalues similar magnitude? We will not be able to approximate this transform easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction\n",
    "\n",
    "We can also reduce the dimensionality of our original dataset by projecting it onto the few principal components of the covariance matrix that we've kept. We can do this by multiplying the dataset matrix by each component and saving the projected data into a new, lower-dimensional matrix.\n",
    "\n",
    "Let's use our highly correlated 2D data set as an example. Above, we discarded the smaller of the two eigenvectors, leaving us with a single eigenvalue/eigenvector pair. We can project the original 2D dataset onto the remaining eigenvector to yield a 1D dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_projected = x @ evecs[:, 0] # remember x is a matrix with row vectors\n",
    "\n",
    "print(\"The eigenvector (row vector) \\n%s\\n\" %evecs[:, 1])\n",
    "print(\"A few data points\\n%s\\n\" % x[0:3, :])\n",
    "\n",
    "# Plot the dataset\n",
    "fig = plt.figure(figsize=(9, 1))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.scatter(x_projected, np.zeros_like(x_projected), c='C0', label=\"Original data\", s=10, alpha=0.8)\n",
    "ax.set_xlabel('distance from origin in direction of 1st principal component', fontsize=12)\n",
    "ax.set_yticks(np.arange(0))\n",
    "ax.set_frame_on(False)\n",
    "plt.title('Data projected onto principal component', fontsize=14);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, we reduced a 2D dataset to 1D, but a more common application is the reduction of a high dimensional data set to 2D. If we can reduce a dataset to 2D, we can visualise it by plotting it in a 2D coordinate system. This often reveals structure in the data, such as clusters of data points. We will explore this in this week's lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uses of eigendecomposition\n",
    "\n",
    "Matrix decomposition is an *essential* tool in data analysis. It can be extremely powerful and is efficient to implement. Systems such as recommenders (e.g. Netflix, YouTube, Amazon, etc.), search engines (Google), image compression algorithms, machine learning tools and visualisation systems apply these decompositions *extensively*.\n",
    "\n",
    "Google was wholly built around matrix decomposition algorithms; that's what \"PageRank\" is. This is what allowed Google to race ahead of their competitors in the early days of the search wars.\n",
    "\n",
    "The eigendecomposition can be used *anywhere* there is a system modelled as a linear transform (any linear map $\\real^N \\rightarrow \\real^N$). It lets us predict behaviour over different time scales (e.g. very short term or very long term). For instance, we can:\n",
    "\n",
    "* Find \"modes\" or \"resonances\" in a system (e.g physical model of a bridge). \n",
    "    * For example, every room has a set of \"eigenfrequencies\" - the acoustic resonant modes. A linear model of the acoustics of the room could be written as a matrix, and the resonant frequencies extracted directly via the eigendecomposition. [Alvin Lucier's analogue power iterations](https://www.youtube.com/watch?v=fAxHlLK3Oyk)\n",
    "* Predict the behaviour of feedback control systems: is the autopilot going to be stable or unstable?\n",
    "* Partition graphs and cluster data (spectral clustering). \n",
    "* Predict flows on graphs.\n",
    "* Perform Principal Component Analysis on high-dimensional data sets for exploratory data analysis, 2D visualisation or data compression.\n",
    "\n",
    "As soon as we can write down a matrix $A$ we can investigate its properties with the eigendecomposition. ***It is the microscope of the linear algebraist.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix properties - and (some) relations to the the eigendecomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace\n",
    "\n",
    "The trace of a square matrix can be computed from the sum of its diagonal values:\n",
    "\n",
    "$$\\begin{examinable} \\text{Tr}(A) = a_{1,1} + a_{2,2} + \\dots + a_{n,n} \\end{examinable}$$\n",
    "\n",
    "It is also equal to the sum of the eigenvalues of $A$\n",
    "\n",
    "$$\\begin{examinable} \\text{Tr}(A) = \\sum_{i=1}^n \\lambda_i \\end{examinable}$$\n",
    "\n",
    "The trace can be thought of as measuring the  **perimeter** of the parallelotope of a unit cube transformed by the matrix. [Strictly, it is *proportional* to the perimeter, with the constant of proportionality being $\\text{Perimiter}(A)=2^{n-1} \\text{Tr}(A)$].\n",
    "\n",
    "### Determinant\n",
    "\n",
    "The determinant $\\text{det}(A)$ is an important property of square matrices. It can be thought of as the **volume** of the parallelotope  of a unit cube transformed by the matrix -- it measures how much the space expands or contracts after the linear transform.\n",
    "\n",
    "It is equal to the product of the eigenvalues of the matrix.\n",
    "\n",
    "$$ \\begin{examinable} \\text{det}(A) = \\prod_{i=1}^n \\lambda_i  \\end{examinable}$$\n",
    "\n",
    "If any eigenvalue $\\lambda_i$ of $A$ is 0, the determinant $\\det(A)=0$, and the transformation collapses at least one dimension to be completely flat. This means that the transformation **cannot be reversed**; information has been lost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definite and semi-definite matrices\n",
    "\n",
    "A matrix is called \n",
    "\n",
    "- **positive definite** if all of its eigenvalues are greater than zero: $\\lambda_i > 0$. \n",
    "\n",
    "- **positive semi-definite** if all of its eigenvalues are greater than or equal to zero: $\\lambda_i \\geq 0$. \n",
    "\n",
    "- **negative definite** if all of the eigenvalues are less than zero: $\\lambda_i < 0$, - \n",
    "\n",
    "- **negative semi-definite** if all the eigenvalues are less than or equal to zero: $\\lambda_i \\leq 0$.\n",
    "\n",
    "A positive definite matrix $A$ has the property $\\vec{x}^TA\\vec{x}>0$ for all (nonzero) $\\vec{x}$. This tells us that the dot product of $\\vec{x}$ with $A \\vec{x}$ must be positive (N.B. $A \\vec{x}$ is the vector obtained by transforming $\\vec{x}$ with $A$). This can only happen if the angle $\\theta$ between $\\vec{x}$ and $A \\vec{x}$ is less than $\\frac{\\pi}{2}$, since $\\vec{x}^TA\\vec{x} = |\\vec{x}||A \\vec{x}|\\cos{\\theta}$. That means that $A$ does not rotate $\\vec{x}$ through more than $90^{\\circ}$.\n",
    "\n",
    "Positive definiteness will be an important concept when we discuss **covariance matrices** (important in statistical data analysis) and **Hessian matrices**  (important in numerical optimisation).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of eigenproblems\n",
    "\n",
    "* Eigenvectors exist only for square matrices.\n",
    "\n",
    "* A matrix $A$ transforms a general vector by rotating and scaling it. However, the eigenvectors of $A$ are special because they can only be scaled, not rotated by the transform. \n",
    "\n",
    "* The eigenvalues of $A$ are the scaling factors $\\lambda_i$ that correspond to each unit eigenvector $\\vec{x_i}$.\n",
    "\n",
    "* Eigenvectors and eigenvalues can be computed algorithmically (e.g. by the power iteration algorithm for finding the **leading eigenvector**).\n",
    "\n",
    "* Eigendecomposition is the process of breaking a matrix down into its constituent eigenvalues and eigenvectors. These serve as a compact **summary** of the matrix. \n",
    "\n",
    "* The **eigenspectrum** is just the list of (absolute) eigenvalues of a matrix, in rank order, largest first. \n",
    "\n",
    "* If we have a complete set of eigenvectors and eigenvalues, we can reconstruct the matrix. \n",
    "\n",
    "* We can approximate a large matrix $A$ with a few leading eigenvectors; this is a simplified or **truncated** approximation to the original matrix.\n",
    "\n",
    "* If we repeatedly apply a matrix $A$ to some vector $\\vec{x}$, the vector will be stretched more and more along the largest eigenvectors.\n",
    "\n",
    "\n",
    "## Things we can tell from eigenvectors/values:\n",
    "\n",
    "* If a matrix has one or more zero eigenvalues, the transform it performs is one that collapses one or more dimensions in vector space. This type of operation is irreversible, and this tells us that $A$ is singular (un-invertible) - more on that in a moment.\n",
    "\n",
    "* Eigenvectors corresponding to larger (absolute) eigenvalues are more \"important\"; they are represent directions in which data will get stretched most.\n",
    "\n",
    "* If the eigenspectrum is nearly flat (eigenvalues all have similar values), then $A$ represents a transform that stretches vectors almost equally in all directions (like transforming a sphere to a sphere).\n",
    "\n",
    "* If the eigenspectrum has a few large eigenvalues and lots of small ones, then vectors will get stretched along a few directions, but shrink away to nothing along others (like transforming a sphere to a long, skinny ellipse). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Inversion\n",
    "\n",
    "We have seen four basic algebraic operations on matrices:\n",
    "\n",
    "* scalar multiplication $cA$;\n",
    "* matrix addition $A+B$;\n",
    "* matrix multiplication $BA$\n",
    "* matrix transposition $A^T$\n",
    "\n",
    "There is a further important operation: **inversion** $A^{-1}$, defined such that:\n",
    "\n",
    "* $A^{-1}(A\\vec{x}) = \\vec{x}$, \n",
    "* $A^{-1}A = I$ \n",
    "* $(A^{-1})^{-1} = A$\n",
    "* $(AB)^{-1} = B^{-1}A^{-1}$\n",
    "\n",
    "The equivalent of division for matrices is left-multiplication by the inverse. This has the effect reversing or undoing the effect of the original matrix. \n",
    "\n",
    "*Inversion is only defined for certain kinds of matrices, as we will see below.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the inverse of a matrix\n",
    "\n",
    "There are many ways to compute the inverse of a matrix. There is a standard recursive algorithm which you may have seen in Maths courses, but this is only useful for very small matrices. Instead, we often use the workhorse of matrix decompositions: the **singular value decomposition**, which we will discuss later.\n",
    "\n",
    "In the meantime, we can use the NumPy method `np.linalg.inv`, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix A of random numbers\n",
    "A = np.random.normal(0, 1, (3, 3))\n",
    "print_matrix(\"A\", A)\n",
    "# Now invert it\n",
    "print_matrix(\"A^{-1}\", np.linalg.inv(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inversion as \"undo\"\n",
    "\n",
    "Inversion of a matrix creates a new linear operator which reverses the original operation. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that left-multiplying by the inverse \"undoes\" the transformation applied by A to a random vector x\n",
    "# Create a random vector\n",
    "x = np.random.normal(0, 1, (3, 1))\n",
    "print_matrix(\"x\", x)\n",
    "# Tranform it\n",
    "print_matrix(\"Ax\", A @ x)\n",
    "# Left-multiply by the inverse to recover the original vector\n",
    "print_matrix(\"A^{-1}(Ax)\", np.linalg.inv(A) @ (A @ x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inversion creates a matrix that \"undoes\" the transformation performed by another matrix, so long as no information was lost in the transformation. We can illustrate this geometrically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix that performs a 30 degree rotation\n",
    "d30 = np.radians(30)\n",
    "cs = np.cos(d30)\n",
    "ss = np.sin(d30)\n",
    "rot_mat = np.array([[cs, -ss], [ss, cs]])\n",
    "# Display the effect of this matrix\n",
    "show_matrix_effect(rot_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the inverse of the 30 degree rotation matrix and display its effect\n",
    "show_matrix_effect(np.linalg.inv(rot_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the combined effect of the 30 degree rotation matrix, left-multiplied by its inverse\n",
    "show_matrix_effect(np.linalg.inv(rot_mat) @ rot_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BUT only square matrices can be inverted !\n",
    "\n",
    "Inversion is only defined for square matrices, representing a linear transform $\\real^n \\rightarrow \\real^n$. This is equivalent to saying that the determinant of the matrix must be non-zero: $\\det(A) \\neq 0$. Why?\n",
    "\n",
    "A matrix which is non-square maps vectors of dimension $m$ to dimension $n$. This means the transformation collapses or creates dimensions. Such a transformation is not uniquely reversible.\n",
    "\n",
    "For a matrix to be invertible it must represent a **bijection** (a function that maps every member of a set onto exactly one member of another set).\n",
    "\n",
    "## Singular and non-singular matrices\n",
    "\n",
    "A matrix with $\\det(A)=0$ is called **singular** and has no inverse.\n",
    "\n",
    "A matrix which is invertible is called **non-singular**. \n",
    "\n",
    "The geometric intuition for this is simple. Going back to the paralleogram model, a matrix with zero determinant has at least one zero eigenvalue. This means that at least one of the dimensions of the parallelepiped has been squashed to nothing at all. Therefore it is impossible to reverse the transformation, because information was lost in the forward transform. \n",
    "\n",
    "All of the original dimensions must be preserved in a linear map for inversion to be meaningful; this is the same as saying $\\det(A) \\neq 0$.\n",
    "\n",
    "<img src=\"imgs/inversion.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following matrix is singular\n",
    "sing_mat = np.array([[0.0, 1], \n",
    "                    [0, 0.0]])\n",
    "# Display its effect\n",
    "show_matrix_effect(sing_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's not run it \n",
    "\n",
    "## Attempting to invert a singular matrix throws an error\n",
    "#try:\n",
    "#    show_matrix_effect(np.linalg.inv(sing_mat))\n",
    "#except:\n",
    "#    print(\"LinAlgError: Singular matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical stability of matrix inversion algorithms\n",
    "\n",
    "Because matrix operations involve *lots* of repeated floating point operations there are many opportunities for roundoff to accumulate. There is a great deal of importance in finding matrix algorithms which are **numerically stable**; that is they converge to the right answer reliably. \n",
    "\n",
    "Inversion is particularly hard to compute in a stable form directly, and many matrices that *theoretically* could be inverted cannot be inverted using floating point representation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time complexity\n",
    "\n",
    "Matrix inversion, for a general $n \\times n$ matrix, takes $O(n^3)$ time. It is *provable* that no general matrix inversion algorithm can ever be faster than $O(n^3)$ (one of the few problems for which a tight polynomial time bound is known). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special cases\n",
    "\n",
    "Just as for multiplication, there are many special kinds of matrices for which much faster inversion is possible. These include, among many others:\n",
    "\n",
    "* orthogonal matrix (rows and columns are all orthogonal unit vectors): $O(1)$, $A^{-1}= A^T$\n",
    "* diagonal matrix (all non-diagonal elements are zero): $O(n)$, $A^{-1} = \\frac{1}{A}$ (i.e. the reciprocal of the diagonal elements of $A$).\n",
    "* positive-definite matrix: $O(n^2)$ via the *Cholesky decomposition*. We won't discuss this further.\n",
    "* triangular matrix (all elements either above or below the main diagonal are zero): $O(n^2)$, trivially invertible by **elimination algorithms**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special cases: orthogonal matrices\n",
    "\n",
    "An **orthogonal matrix** is a special matrix form that has $A^T=A^{-1}$; that is the transpose and the inverse are equivalent. All of its component eigenvectors are **orthogonal** to each other (at 90 degrees; have an inner product of 0), and all of its eigenvalues are 1 or -1. An orthogonal matrix transforms a cube to a cube. It has a determinant of 1 or -1. Any purely rotational matrix is an orthogonal matrix. \n",
    "\n",
    "Orthogonal matrices can be inverted trivially, since transposition is essentially free in computational terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an orthogonal matrix and show its effect\n",
    "A = np.array([[0, 1, 0], \n",
    "              [-1, 0, 0], \n",
    "              [0, 0, 1]])\n",
    "show_matrix_effect(A, \"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the effect of its transpose and show that the inverse is the same as the transpose\n",
    "show_matrix_effect(A.T, \"A^T\")\n",
    "print_matrix(\"A^{-1}\", np.linalg.inv(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Special cases: Diagonal matrices\n",
    "\n",
    "The inverse of a diagonal matrix is another diagonal matrix whose diagonal elements are the reciprocal of each of the diagonal elements of the original matrix:\n",
    "\n",
    "$$A^{-1}_{ii} = \\frac{1}{A_{ii}}$$\n",
    "\n",
    "This is $O(n)$ to compute, so much faster than standard inversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a diagonal matrix\n",
    "d = np.diag([-1, 2, -3, 4])\n",
    "print_matrix('D', d)\n",
    "# Compute the reciprocal, elementwise\n",
    "print_matrix('\\\\frac{1}{D}', np.diag(1/np.diag(d)))\n",
    "# Comput the inverse\n",
    "print_matrix('D^{-1}', np.linalg.inv(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue with inversion of sparse matrices\n",
    "\n",
    "The inverse of a **sparse matrix** is in general **not sparse**; it will (most likely) be dense. This means that sparse matrix algorithms virtually never involve a direct inverse, as a sparse matrix could easily be 1,000,000 x 1,000,000, but with maybe only a few million non-zero entries, and might be stored in a few dozen megabytes. The inverse form would have 1,000,000,000,000 entries and require a terabyte or more to store!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Revisting the package distribution problems: \"predicting\" the past with inversion\n",
    "\n",
    "Revisiting the problem of package distribution, we can use inversion to solve the problem of predicting the distribution at $\\vec{x_{t=-1}}$ given $\\vec{x_{t=0}}$. Applying $A$ to $\\vec{x}$ took us \"one step\" into the future. Applying $A^{-1}$ takes us one step into the past:\n",
    "\n",
    "$$\\vec{x_{t=-1}} = A^{-1} \\vec{x_{t=0}}$$\n",
    "\n",
    "We can compute any negative power of the matrix to \"undo\" any number of steps:\n",
    "\n",
    "$$A^{-k} = \\underbrace{A^{-1}A^{-1}A^{-1}\\dots A^{-1}}_{\\text{k repetitions}}$$\n",
    "\n",
    "\n",
    "However, be aware that inversion is highly problematic from a numerical point of view, and roundoff error will lead to severely distorted results with repeated negative powers of a matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "## Linear systems\n",
    "Imaginge a system where there is an input, x, and an output, y.\n",
    "\n",
    "One way of looking at matrices is as a collection of weights of components of a vector. Consider:\n",
    "\n",
    "$$A = \\begin{bmatrix}\n",
    "0.5 & 1.0 & 2.0\\\\\n",
    "1.0 & 0.5 & 0.0\\\\\n",
    "0.6 & 1.1 & -0.3\\\\\n",
    "\\end{bmatrix}$$    \n",
    "\n",
    "This represents a linear map operating on 3D vectors $\\vec{x}\\in\\real^3$. It produces a 3D vectors $\\vec{y}\\in\\real^3$, each component of which is a **weighted sum** of the components of the input vector.\n",
    "\n",
    "$$y_1 = 0.5x_1 + 1.0x_2 +2.0x_3\\\\\n",
    "y_2 = 1.0x_1 +0.5x_2 +0.0x_3\\\\\n",
    "y_3 = 0.6x_1 + 1.1x_2 - 0.3x_3$$\n",
    "\n",
    "The coefficients of the matrix represent the weighting to be applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[0.5, 1.0, 2.0], \n",
    "              [1.0, 0.5, 0.0],\n",
    "              [0.6, 1.1, -0.3]])\n",
    "\n",
    "x = np.array([[1], [-1], [1]])\n",
    "\n",
    "print_matrix(\"A\", A)\n",
    "print_matrix(\"\\\\bf x\", x)\n",
    "print_matrix(\"y=A{\\\\bf x}\", A @ x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the package distribution system:\n",
    "\n",
    "NB! This is exactly what happens in the package distribution example; the matrix represents the equations that define the flow between depots. You could interpret one row as follows:\n",
    "\n",
    "$$\\begin{bmatrix} 0.9 & 0.0 & 0.0 & 0.2 & 0.0 & 0.05 & 0.08 & 0.0 \\end{bmatrix}$$\n",
    "\n",
    "> Tomorrow, Depot A will have 90% of Depot A's stock, 20% of Depot D's stock, 5% of Depot F's stock and 8% of Depot G's stock.\n",
    "\n",
    "$$ y_1 = 0.9x_1 + 0.2x_4 + 0.05x_6 + 0.08x_7 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the form that simultaneous equations take. If we know $A$ and $\\vec{y}$, we can ask what $\\vec{x}$ will satisfy the following equation (or if there even exists an $\\vec{x}$ that would satisfy it):\n",
    "\n",
    "$$ A\\vec{x} = \\vec{y}$$\n",
    "\n",
    "This is a **linear system** or **linear system of equations**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving linear systems\n",
    "\n",
    "The solution of linear systems is apparently simple for cases where $A$ is ___square___. If $A\\vec{x}=\\vec{y}$, then left-multiplying both sides by $A^{-1}$ we get \n",
    "\n",
    "$$\n",
    "A^{-1}A\\vec{x}=A^{-1}\\vec{y}\\\\\n",
    "I \\vec{x} = A^{-1} \\vec{y}\\\\\n",
    "\\vec{x} = A^{-1}\\vec{y}\n",
    "$$\n",
    "\n",
    "This only works for square matrices, as $A^{-1}$ is not defined for non-square matrices. This means that $\\vec{x}$ and $\\vec{y}$ must have the same number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try for a different y and find what x is\n",
    "y = np.array([[10], [1], [12]])\n",
    "A_inv = np.linalg.inv(A)\n",
    "x_solved = A_inv @ y\n",
    "print_matrix(\"A\", A)\n",
    "print_matrix(\"{\\\\bf y}\", y)\n",
    "# find the correct x\n",
    "print_matrix(\"{\\\\bf x}=A^{-1}{\\\\bf y}\", x_solved)\n",
    "# verify that we have solved these equations by checking that Ax = y\n",
    "print_matrix(\"A{\\\\bf x}\", A @ x_solved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate solutions for linear systems\n",
    "\n",
    "In practice, linear systems are almost never solved with a direct inversion. The numerical problems in inverting high dimensional matrices will make the result highly unstable, and tiny variations in $\\vec{y}$ might lead to wildly different solutions for $\\vec{x}$.\n",
    "\n",
    "Instead, linear systems are typically solved iteratively, either using specialised algorithms based on knowledge of the structure of the system, or using **optimisation**, which will be the topic of the next Unit.\n",
    "\n",
    "These algorithms search the possible space of $\\vec{x}$ to find solutions that minimise\n",
    "$\\|A\\vec{x}-\\vec{y}\\|_2^2$ by adjusting the value of $\\vec{x}$ repeatedly.\n",
    "\n",
    "The reason these iterative approximation algorithms can work when inversion is numerically impossible is that they only have to solve for one *specific* pair of vectors $\\vec{x}, \\vec{y}$. They do not have to create an inversion $A^{-1}$ that inverts the problem for all possible values of $\\vec{y}$, just the specific $\\vec{y}$ seen in the problem. This problem is much more constrained and therefore much more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singular value decomposition\n",
    "\n",
    "Eigendecompositions only apply to diagonalizable matrices; which are a subset of square matrices. But the ability to \"factorise\" matrices in the way the eigendecomposition does is enormously powerful, and there are many problems which have non-square matrices which we would like to be able to decompose.\n",
    "\n",
    "The **singular value decomposition** (SVD) is a general approach to decomposing any matrix $A$. It is the powerhouse of computational linear algebra.\n",
    "\n",
    "The SVD produces a decomposition which splits ***ANY*** matrix up into three matrices:\n",
    "\n",
    "$$\\begin{examinable}A = U \\Sigma V^T, \\end{examinable}$$\n",
    "where \n",
    "* $A$ is any $m \\times n$ matrix, \n",
    "* $U$ is a **square unitary** $m \\times m$ matrix, whose columns contain the **left singular vectors**,\n",
    "* $V$ is an **square unitary** $n \\times n$ matrix, whose columns contain the **right singular vectors**,\n",
    "* $\\Sigma$ is a diagonal $m \\times n$ matrix, whose diagonal contains the **singular values**.\n",
    "\n",
    "A **unitary** matrix is one whose conjugate transpose is equal to its inverse. If $A$ is real, then $U$ and $V$ will be **orthogonal** matrices ($U^T = U^{-1}$), whose rows all have unit norm and whose columns also all have unit norm.\n",
    "\n",
    "The diagonal of the matrix $\\Sigma$ is the set of **singular values**, which are closely related to the eigenvalues, but are *not* quite the same thing (except for special cases like when $A$ is a positive semi-definite symmetric matrix)! The **singular values** are always positive real numbers.\n",
    "\n",
    "We can compute the SVD with `np.linalg.svd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_svd(A):\n",
    "    \n",
    "    # Print A\n",
    "    print_matrix(\"A\", A)\n",
    "    \n",
    "    # Take SVD\n",
    "    u, sigma, vt = np.linalg.svd(A)\n",
    "    \n",
    "    # Print U, then norms of rows and norms of columns in U\n",
    "    print_matrix(\"U\", u)\n",
    "    print_matrix(\"\\|U\\|_{rows}\", np.linalg.norm(u, axis=1))\n",
    "    print_matrix(\"\\|U\\|_{cols}\", np.linalg.norm(u, axis=0))\n",
    "    \n",
    "    # By default, sigma will be a vector of the diagonal values\n",
    "    # np.diag(sigma) converts the vector back to a diagonal matrix\n",
    "    sigma_diag = np.zeros_like(A)\n",
    "    sqr = min(A.shape[0], A.shape[1])\n",
    "    sigma_diag[:sqr, :sqr] = np.diag(sigma)\n",
    "    \n",
    "    # Print sigma (as a diagonal matrix)\n",
    "    print_matrix(\"\\Sigma\", sigma_diag)\n",
    "    \n",
    "    # Print V, then norms of rows and norms of columns in V\n",
    "    print_matrix(\"V\", vt.T)\n",
    "    print_matrix(\"\\|V\\|_{rows}\", np.linalg.norm(vt.T, axis=1))\n",
    "    print_matrix(\"\\|V\\|_{cols}\", np.linalg.norm(vt.T, axis=0))\n",
    "    \n",
    "    def recompose(u, sigma, vt):\n",
    "        return u @ sigma @ vt\n",
    "    \n",
    "    # Print recomposed version of A (A = U Sigma V)\n",
    "    print_matrix(\"U \\Sigma V^T\", recompose(u, sigma_diag, vt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a square matrix A\n",
    "A = np.array([[0.5, 2, 3],\n",
    "              [4,   5, 6],\n",
    "              [7,   8, 9]])\n",
    "# Take the SVD and print all the decomposed parts\n",
    "print_svd(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a non-square 2 x 3 matrix A\n",
    "A = np.array([[0.1, 0.2, 0.3],\n",
    "              [0.4, 0.5, 0.6]])\n",
    "# Take the SVD and print all the decomposed parts\n",
    "print_svd(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a non-square 3 x 2 matrix A\n",
    "A = np.array([[0.1, 0.2, 0.3],\n",
    "              [0.4, 0.5, 0.6]]).T\n",
    "# Take the SVD and print all the decomposed parts\n",
    "print_svd(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relation to eigendecomposition\n",
    "The SVD is the same as:\n",
    "* taking the eigenvectors of $A^TA$ to get $U$ (NB A @ A.T is a symetric matrix)\n",
    "* taking the square root of the *absolute* value of the eigenvalues $\\lambda_i$ of $A^TA$ to get $\\Sigma_i = \\sqrt{|\\lambda_i|}$\n",
    "* taking the eigenvectors of $AA^T$ to get $V^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3 x 2 matrix A\n",
    "A = np.array([[0.1, 0.2, 0.3],\n",
    "              [0.4, 0.5, 0.6]]).T\n",
    "\n",
    "print_matrix('A', A)\n",
    "print_matrix('AA^T', A @ A.T)\n",
    "print_matrix('A^TA ', A.T @ A)\n",
    "\n",
    "# Compute eigenvalues and eigenvectors of the matrices A A^T and A^T A\n",
    "l_evals, l_evecs = np.linalg.eig(A @ A.T)\n",
    "r_evals, r_evecs = np.linalg.eig(A.T @ A)\n",
    "\n",
    "# compare eigenvector matrices to U and V from SVD\n",
    "u, sigma, vt = np.linalg.svd(A)\n",
    "print_matrix(\"\\\\text{eigenvectors of}\\ AA^T\", l_evecs)\n",
    "print_matrix(\"U\", u)\n",
    "\n",
    "print_matrix(\"\\\\text{eigenvectors of}\\ A^TA\", np.fliplr(r_evecs)) # flip for comparison with V\n",
    "print_matrix(\"V^T\", vt)\n",
    "\n",
    "# compare eigenvalues to Sigma\n",
    "print_matrix(\"\\sqrt{|\\lambda|} \\; \\\\text{of} \\; AA^T\", np.sqrt(np.abs(l_evals)))\n",
    "print_matrix(\"\\sqrt{|\\lambda|} \\; \\\\text{of} \\; A^TA\", np.sqrt(np.abs(r_evals))[::-1]) # flip for comparison with Sigma\n",
    "print_matrix(\"\\Sigma\", sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Very special case: symmetric, positive semi-definite matrix\n",
    "\n",
    "For a symmetric, positive semi-definite matrix $A$,\n",
    "- the eigenvectors are the columns of $U$ or the columns of $V$. \n",
    "- the eigenvalues are in $\\Sigma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a symmetric positive semi-definite 3 x 3 matrix\n",
    "A = np.array([[2, 1, 0],\n",
    "              [1, 3, 1],\n",
    "              [0, 1, 4]])\n",
    "print_matrix(\"A\", A)\n",
    "\n",
    "# Find eigenvectors and eigenvalues\n",
    "evals, evecs = np.linalg.eig(A)\n",
    "\n",
    "print(\"------------------\")\n",
    "print_matrix(\"\\lambda (unordered)\", evals)\n",
    "print_matrix(\"\\\\bf{x} (unordered)\", evecs)\n",
    "print_matrix(\"\\lambda\", evals[::-1])       # Reverse order of eigenvalues to get them in size order\n",
    "print_matrix(\"\\\\bf{x}\", np.fliplr(evecs))  # Flip eigenvector matrix left right to correspond with ordered eigenvalues\n",
    "\n",
    "print(\"------------------\")\n",
    "# Take the SVD\n",
    "u, sigma, vt = np.linalg.svd(A)\n",
    "print_matrix(\"\\Sigma\", sigma)\n",
    "print_matrix(\"U\", u)\n",
    "print_matrix(\"V\", vt.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD decomposes *any* matrix into three matrices with special forms\n",
    "\n",
    "Special forms of matrices, like orthogonal matrices and diagonal matrices, are much easier to work with than general matrices. This is the power of the SVD.\n",
    "\n",
    "* $U$ is orthogonal, so is a pure rotation matrix,\n",
    "* $\\Sigma$ is diagonal, so is a pure scaling matrix, \n",
    "* $V$ is orthogonal, so is a pure rotation matrix.\n",
    "\n",
    "#### Rotate, scale, rotate\n",
    "\n",
    "*The SVD splits any matrix transformation into a rotate-scale-rotate operation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a matrix A that performs a rotation and scaling transformation\n",
    "A = np.array([[0.4, 0.75], \n",
    "              [0.15, -0.6]])\n",
    "\n",
    "show_matrix_effect(A, \"A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's break this transformation down into its consitutent parts: \n",
    "\n",
    "* $V^T$: rotation ,\n",
    "* $\\Sigma$: scaling,\n",
    "* $U$: rotation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, sigma, vt = np.linalg.svd(A)\n",
    "\n",
    "show_matrix_effect(vt, \"V^T\")\n",
    "show_matrix_effect(np.diag(sigma) @ vt, \"\\Sigma V^T\")\n",
    "show_matrix_effect(u @ np.diag(sigma) @ vt, \"U \\Sigma V^T\")\n",
    "\n",
    "#Sanity\n",
    "print(\"Determinant of vt:\" ,np.linalg.det(vt))\n",
    "print(\"Determinant of u:\" ,np.linalg.det(u))\n",
    "print(\"Determinant of Sigma:\" ,np.linalg.det(np.diag(sigma)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the SVD\n",
    "\n",
    "Many matrix operations are trivial once we have the factorisation of the matrix into the three components. But some can only be performed on *on certain types of matrices*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Fractional powers\n",
    "\n",
    "We can use the SVD to compute interesting matrix functions like the square root of a matrix $A^{1/2}$. In fact, we can use the SVD to raise a matrix to any power, *in a single operation*, provided it is a **square symmetric matrix**. \n",
    "\n",
    "We can use the SVD to:\n",
    "\n",
    "* raise a matrix to a fractional power, e.g. $A^{1/2}$, which will \"part do\" an operation,\n",
    "* invert a matrix: $A^{-1}$, which will \"undo\" the operation.\n",
    "\n",
    "The rule is simple: to do any of these operations, ignore $U$ and $V$ (which are just rotations), and apply the function to the singular values elementwise:\n",
    "\n",
    "$$ A^n = V \\Sigma^n U^T $$\n",
    "\n",
    "For a symmetric matrix, this is the same as:\n",
    "\n",
    "$$ A^n = U \\Sigma^n V^T $$\n",
    "\n",
    "**Note: $A^{1/2}$ is not the elementwise square root of each element of A!** \n",
    "\n",
    "Rather, we must compute the elementwise square root of $\\Sigma$, then compute $A^{1/2} = U \\Sigma^{1/2} V^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def powm(A, n): # generalised matrix power\n",
    "    u, sigma, vt = np.linalg.svd(A)\n",
    "    sigma_n = np.diag(sigma**n)\n",
    "    return u @ sigma_n @ vt\n",
    "    #return vt.T @ sigma_n @ u.T  # gives the same result as (u @ sigma_n @ vt) for a symmetric matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a square, symmetric matrix\n",
    "A = np.array([[0.5, -1], \n",
    "              [-1, 3]])\n",
    "\n",
    "print_matrix(\"A\", A)\n",
    "\n",
    "# Compute A^(1/2)\n",
    "a_half = powm(A, 0.5)\n",
    "print_matrix(\"A^{1/2}\", a_half)\n",
    "\n",
    "# A^(1/2) A^(1/2) should be equal to A\n",
    "print_matrix(\"A^{1/2} A^{1/2}\", a_half @ a_half)\n",
    "\n",
    "# Compute A^2\n",
    "print_matrix(\"A^2\", powm(np.array(A), 2))\n",
    "\n",
    "# AA should be equal to A^2\n",
    "print_matrix(\"AA\", A @ A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inversion - relation to SVD\n",
    "\n",
    "\n",
    "We can efficiently invert a matrix once it is in SVD form. For a non-symmetric matrix, we use:\n",
    "\n",
    "$$\\begin{examinable}A ^{-1} = V \\Sigma^{-1} U^T\\end{examinable}$$\n",
    "\n",
    "This can be computed in $O(n)$ time because $\\Sigma^{-1}$ can be computed simply by taking the reciprocal of each of the diagonal elements of $\\Sigma$. \n",
    "\n",
    "*N.B. No free lunch! As a consequence, we now know that computing the SVD must take $O(n^3)$ time for square matrices, since inversion cannot be achieved faster than $O(n^3)$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inversion using the SVD\n",
    "def invert_by_svd(u, sigma, vt):\n",
    "    sigma_inv = np.zeros((u.shape[0], vt.shape[0]))\n",
    "    sqr = min(vt.shape[0], u.shape[0])\n",
    "    sigma_inv[:sqr, :sqr] = np.diag(1.0/sigma)    \n",
    "    return vt.T @ sigma_inv @ u.T\n",
    "\n",
    "# Create a non-symmetric, square matrix that we want to invert\n",
    "A = np.array([[1,  2, 3], \n",
    "              [4, -5, 6], \n",
    "              [7, -8, 9.5]])\n",
    "\n",
    "# Take the SVD\n",
    "u, sigma, vt = np.linalg.svd(A)\n",
    "\n",
    "print_matrix(\"A\", A)\n",
    "\n",
    "# Comput the inverse of A using two different methods, and compare\n",
    "print_matrix(\"V \\Sigma^{-1} U^T\", invert_by_svd(u, sigma, vt))\n",
    "print_matrix(\"A^{-1}\", np.linalg.inv(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-inverse (non-symmetric)\n",
    "\n",
    "We can also pseudo-invert a matrix: $A^{+}$, which will approximately \"undo\" the operation, even when $A$ isn't square.\n",
    "\n",
    "This means we can solve (approximately) systems of equations where the number of input variables is different to the number of output variables. The **pseudo-inverse** or **Moore-Penrose pseudo-inverse** generalises inversion to (some) non-square matrices. \n",
    "\n",
    "We can find approximate solutions for $\\vec{x}$ in the equation:\n",
    "\n",
    "$$A\\vec{x} = \\vec{y},$$ \n",
    "\n",
    "or in fact simultaneous equations of the type\n",
    "\n",
    "$$AX = Y$$ \n",
    "\n",
    "The pseudo-inverse of $A$ is just \n",
    "\n",
    "$$A^{+} = V \\Sigma^{-1} U^T,$$ \n",
    "\n",
    "which is the same as the standard inverse computed via SVD, but taking care that $\\Sigma$ is the right shape - appropriate zero padding is required! Fortunately, this is taken care of by the Numpy method `np.linalg.pinv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a non-square matrix, so that we can find the pseudo-inverse\n",
    "A = np.array([[0.5, 1.0, 2.0], \n",
    "              [1.0, 0.5, 0.0]])\n",
    "\n",
    "print_matrix(\"A\", A)\n",
    "\n",
    "# Create a vector y\n",
    "y = np.array([[2], [5]])\n",
    "\n",
    "print_matrix(\"\\\\bf{y}\", y)\n",
    "\n",
    "# We want to know what x is, so we have to invert A using the pseudo-inverse\n",
    "A_pinv = np.linalg.pinv(A) # pseudo-inverse is called pinv in NumPy\n",
    "\n",
    "print_matrix(\"A^{+}\", A_pinv)\n",
    "\n",
    "# Compute x\n",
    "x = A_pinv @ y\n",
    "\n",
    "print_matrix(\"\\\\bf{x} = A^{+} \\\\bf{y}\", x)\n",
    "\n",
    "# check that Ax = y\n",
    "print_matrix(\"A \\\\bf{x}\", A @ x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color=red>Non-symmetric case</font>**: If the problem does not have an exact solution, the pseudo-inverse will give the closest result according to the $L_2$ norm. In other words, it will find the vector $\\vec{x}$ that minimises the distance $\\|A\\vec{x} - \\vec{y}\\|_2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a non-square matrix, so that we can find the pseudo-inverse\n",
    "A = np.array([[0.5, 1.0, 2.0], \n",
    "              [1.0, 0.5, 0.0]]).T\n",
    "\n",
    "print_matrix(\"A\", A)\n",
    "\n",
    "# Create a vector y\n",
    "y = np.array([[1], [2], [3]])\n",
    "\n",
    "print_matrix(\"\\\\bf{y}\", y)\n",
    "\n",
    "# We want to know what x is, so we have to invert A using the pseudo-inverse\n",
    "A_pinv = np.linalg.pinv(A) \n",
    "\n",
    "print_matrix(\"A^{+}\", A_pinv)\n",
    "\n",
    "# Compute x\n",
    "x = A_pinv @ y\n",
    "\n",
    "print_matrix(\"\\\\bf{x} = A^{+} \\\\bf{y}\", x)\n",
    "\n",
    "# check that Ax = y\n",
    "# won't be exact, as there are no exact solutions\n",
    "print_matrix(\"A \\\\bf{x}\", A @ x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the $\\|A\\vec{x} - \\vec{y}\\|_2$ distance plays a massive role in data science and machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few more matrix properties...\n",
    "\n",
    "## Rank of a matrix\n",
    "\n",
    "The **rank** of a matrix is equal to the number of non-zero singular values. \n",
    "\n",
    "* If the number of non-zero singular values is equal to the size of the matrix, then the matrix is **full rank**. \n",
    "* A full rank matrix has a non-zero determinant and will be invertible. \n",
    "* The rank tells us how many dimensions the parallelotope that the transform represents will have. \n",
    "* If a matrix does not have full rank, it is **singular** (non-invertible) and has **deficient rank**.\n",
    "* If the number of non-zero singular values is much less than the size of the matrix, the matrix is **low rank**.\n",
    "\n",
    "For example, a 4 x 4 matrix with rank 2 will take vectors in $\\real^4$ and output vectors in a $\\real^2$ subspace (a plane) of $\\real^4$. The orientation of that plane will be given by the first two eigenvectors of the matrix.\n",
    "\n",
    "## Condition number of a matrix\n",
    "\n",
    "The **condition number** number of a matrix is the ratio of the largest singular value to the smallest. \n",
    "\n",
    "* This is only defined for full rank matrices. \n",
    "* The condition number measures how sensitive inversion of the matrix is to small changes.\n",
    "* A matrix with a small condition number is called **well-conditioned** and is unlikely to cause numerical issues. \n",
    "* A matrix with a large condition number is **ill-conditioned**, and numerical issues are likely to be significant. \n",
    "\n",
    "An ill-conditioned matrix is almost singular, so inverting it will lead to invalid results dur to floating point roundoff errors.\n",
    "\n",
    "### Relation to singularity\n",
    "\n",
    "A **singular** matrix $A$ is un-invertible and has $\\det(A)=0$. Singularity is a binary property, and is either true or false. \n",
    "\n",
    "**Rank** and **condition numbers** extend this concept.\n",
    "\n",
    "<img src=\"imgs/matrix_spectrum.png\">\n",
    "\n",
    "* We can think of **rank** as measuring \"how singular\" the matrix is, i.e. how many dimensions are lost in the transform.\n",
    "* We can think of the **condition number** as measuring how close a non-singular matrix is to being singular. A matrix which is nearly singular may become effectively singular due to floating point roundoff errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Well-conditioned matrix\")\n",
    "well_cond = np.random.normal(0, 1, (3, 3))\n",
    "print(\"Condition number:\", np.linalg.cond(well_cond))\n",
    "print_matrix(\"A\", well_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that condition number is ratio of largest to smallest singular values\n",
    "u, sigma, vt = np.linalg.svd(well_cond)\n",
    "print(sigma)\n",
    "print(\"Ratio of max to min singular values:\", np.max(sigma)/np.min(sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that, for a well-conditioned matrix, if we apply the matrix twice, \n",
    "# followed by the inverse twice, this results in the identity matrix I\n",
    "print_matrix(\"A^{-1}A^{-1}AA\", np.linalg.inv(well_cond) @ np.linalg.inv(well_cond) @ well_cond @ well_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ill-conditioned matrix\")\n",
    "ill_cond = np.random.normal(2000000, 0.002, (3, 3))\n",
    "print(\"Condition number:\", np.linalg.cond(ill_cond))\n",
    "print_matrix(\"A\", ill_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u, sigma, vt = np.linalg.svd(ill_cond)\n",
    "print(sigma)\n",
    "print(\"Ratio of max to min singular values:\", np.max(sigma)/np.min(sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for an ill-conditioned matrix, if we apply the matrix twice, \n",
    "# followed by the inverse twice, we don't get the identity matrix I,\n",
    "# due to floating point error accumulation\n",
    "print_matrix(\"A^{-1}A^{-1}AA\", np.linalg.inv(ill_cond) @ np.linalg.inv(ill_cond) @ ill_cond @ ill_cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this matrix is rank 2 and projects 3D space onto a plane\n",
    "A = np.array([[1,  2,  3], \n",
    "              [2,  4,  6], \n",
    "              [0, -1, -5]])\n",
    "\n",
    "u, sigma, vt = np.linalg.svd(A)\n",
    "\n",
    "# one of the singular values is zero\n",
    "print_matrix(\"\\Sigma\", sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying decompositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Whitening a data set (self-study in the lab)\n",
    "\n",
    "**Whitening** removes all linear correlations within a dataset. It is a *normalizing* step used to standardise data before analysis. This will be covered in the lab.\n",
    "\n",
    "Given a data set stored in matrix $X$, we can compute:\n",
    "\n",
    "$$\\begin{examinable}X^{\\text w} = (X - \\vec{\\mu}) \\Sigma^{-1/2} \\end{examinable}$$\n",
    "\n",
    "where $\\vec{\\mu}$ is the **mean vector**, i.e. a row vector containing the mean of each column in $X$, and $\\Sigma$ is the **covariance matrix** (not the matrix of singular values).\n",
    "\n",
    "This equation takes each column of $X$ and subtracts the mean of that column from every element in the column, so that each column is centred on zero. Then it multiplies by the inverse square root of the covariance matrix, which is a bit like dividing each column of $X$ by its standard deviation to normalise the spread of values in each column. However, it is more rigorous than that, because it also removes any correlations between the columns.\n",
    "\n",
    "To summarise, whitening does the following:\n",
    "\n",
    "* centers the data around its mean, so it has **zero mean**.\n",
    "* \"squashes\" the data so that its distribution is spherical and has **unit covariance**.\n",
    "\n",
    "Removing the mean is easy. The difficult bit is computing the inverse square root of the covariance matrix. N.B. this is *definitely not* the inverse square root of the elements of the covariance matrix!\n",
    "\n",
    "Fortunately, we can compute it easily by taking the SVD of the covariance matrix. We compute the inverse square root in one step, by taking the reciprocal of the square root of each of the singular values and then reconstructing. \n",
    "\n",
    "The whitened version of the data set will have all linear correlations removed. This is an important preprocessing step when applying machine learning and statistical analysis algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jhwutils import ellipse as ellipse\n",
    "\n",
    "fig = plt.figure(figsize = (8, 8))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "\n",
    "# generate a random data set with some correlations\n",
    "x = np.random.normal(0, 1, (200, 2)) @ np.array([[0.1, 0.5], [-0.9, 1.0]]) + np.array([2, 3])\n",
    "\n",
    "# plot the data set with some ellipses to indicate the covariance\n",
    "ax.scatter(x[:, 0], x[:, 1], c='C0', label=\"Original\", s=10)\n",
    "cov_ellipse(ax, x[:, 0:2], 1, facecolor='none', edgecolor='C0')\n",
    "cov_ellipse(ax, x[:, 0:2], 2, facecolor='none', edgecolor='C0')\n",
    "cov_ellipse(ax, x[:, 0:2], 3, facecolor='none', edgecolor='C0')\n",
    "\n",
    "###################\n",
    "\n",
    "# center around the mean\n",
    "center_x = x - np.mean(x, axis=0)\n",
    "\n",
    "# remove covariance via the SVD\n",
    "u, sigma, vt = np.linalg.svd(np.cov(center_x, rowvar=False))\n",
    "# here's the magic trick:\n",
    "sigma_inv_sqr = vt.T @ np.diag(1.0 / np.sqrt(sigma)) @ u.T\n",
    "white_x = center_x @ sigma_inv_sqr\n",
    "\n",
    "####################\n",
    "\n",
    "# plot the whitened data with some ellipses to indicate the covariance\n",
    "ax.scatter(white_x[:, 0], white_x[:, 1], c='C1', label=\"Whitened\", s=10)\n",
    "cov_ellipse(\n",
    "    ax, white_x[:, 0:2], 1, facecolor='none', edgecolor='C1')\n",
    "cov_ellipse(\n",
    "    ax, white_x[:, 0:2], 2, facecolor='none', edgecolor='C1')\n",
    "cov_ellipse(\n",
    "    ax, white_x[:, 0:2], 3, facecolor='none', edgecolor='C1')\n",
    "\n",
    "# fix details\n",
    "ax.set_xlim(-6, 6)\n",
    "ax.set_ylim(-6, 8)\n",
    "\n",
    "ax.axhline(1, ls=':', alpha=0.5)\n",
    "ax.axvline(1, ls=':', alpha=0.5)\n",
    "ax.axhline(-1, ls=':', alpha=0.5)\n",
    "ax.axvline(-1, ls=':', alpha=0.5)\n",
    "\n",
    "ax.axhline(0)\n",
    "ax.axvline(0)\n",
    "ax.set_aspect(1.0)\n",
    "ax.legend()\n",
    "ax.set_frame_on(False)\n",
    "ax.set_title(\"Whitening a dataset\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "* [**Matrix decompositions**](http://nicolas-hug.com/blog/matrix_facto_1) [**Recommended**]\n",
    "\n",
    "* [**Eigenvectors and eigenvalues**](http://setosa.io/ev/eigenvectors-and-eigenvalues/)\n",
    "\n",
    "* [**The Singular Value Decomposition**](http://theory.stanford.edu/~tim/s15/l/l9.pdf)\n",
    "* [**A tutorial on principal components analysis**](https://arxiv.org/pdf/1404.1100.pdf)\n",
    "\n",
    "* [**A tutorial on the singular value decomposition**](https://blog.statsbot.co/singular-value-decomposition-tutorial-52c695315254)\n",
    "### Beyond the course\n",
    "\n",
    "* [**A tutorial on spectral graph theory and graph Laplacians**](https://csustan.csustan.edu/~tom/Clustering/GraphLaplacian-tutorial.pdf)\n",
    "\n",
    "* [**Introduction to Linear Algebra**](http://math.mit.edu/~gs/linearalgebra/) by Gilbert Strang. The standard reference text book for linear algebra.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext_format_version": "1.0",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
