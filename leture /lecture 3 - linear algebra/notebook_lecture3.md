## Linear algebra
**Adjancency matrix é‚»æ¥çŸ©é˜µ** : with edges being the weights of the connection between sites.   

**Matrix powers**: Since we have already defined matrix multiplication, we can now define $A^2=AA$, $A^3=AAA$, $A^4=AAAA$, etc. These are the **powers** of a matrix, and are only defined for square matrices(æ­£æ–¹å½¢çŸ©é˜µ).

# Eigenvalues and eigenvectors
A matrix represents a special kind of function: a **linear transform**; an operation that performs rotation and scaling on vectors. However, there are certain vectors which don't get rotated when multiplied by the matrix. çŸ©é˜µä»£è¡¨ä¸€ç§ç‰¹æ®Šçš„å‡½æ•°ï¼š**çº¿æ€§å˜æ¢**ï¼›ä¸€ç§å¯¹å‘é‡è¿›è¡Œæ—‹è½¬å’Œç¼©æ”¾çš„è¿ç®—ã€‚ä¸è¿‡ï¼Œæœ‰äº›å‘é‡ä¸çŸ©é˜µç›¸ä¹˜æ—¶ä¸ä¼šå‘ç”Ÿæ—‹è½¬ã€‚

Special vectors: They only get scaled (stretched or compressed). These vectors are called **eigenvectors**, and they can be thought of as the "fundamental" or "characteristic" vectors of the matrix, as they have some stability. The prefix **eigen** just means **characteristic** (from the German for "own"). The scaling factors that the matrix applies to its eigenvectors are called **eigenvalues**. ç‰¹æ®ŠçŸ¢é‡ï¼š å®ƒä»¬åªä¼šè¢«ç¼©æ”¾ï¼ˆæ‹‰ä¼¸æˆ–å‹ç¼©ï¼‰ã€‚è¿™äº›å‘é‡è¢«ç§°ä¸º**ç‰¹å¾å‘é‡**ï¼Œå®ƒä»¬å¯ä»¥è¢«è§†ä¸ºçŸ©é˜µçš„ "åŸºæœ¬ "æˆ– "ç‰¹å¾ "å‘é‡ï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰ä¸€å®šçš„ç¨³å®šæ€§ã€‚çŸ©é˜µåº”ç”¨äºå…¶ç‰¹å¾å‘é‡çš„ç¼©æ”¾å› å­ç§°ä¸º**ç‰¹å¾å€¼**ã€‚
`evals,evecs = np.linalg.eig(A)`

#### npçš„eigenvaluesç¼ºé™·
* æ•°å€¼ç¨³å®šæ€§é—®é¢˜ï¼šåœ¨å¤„ç†éå¸¸å¤§æˆ–éå¸¸å°çš„æ•°å€¼æ—¶ï¼Œè®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡å¯èƒ½ä¼šå‡ºç°æ•°å€¼ç¨³å®šæ€§é—®é¢˜ã€‚è¿™å¯èƒ½å¯¼è‡´ç»“æœçš„ç²¾åº¦ä¸é«˜æˆ–è€…è®¡ç®—è¿‡ç¨‹ä¸­å‡ºç°æ•°å€¼ä¸Šçš„ä¸ç¨³å®šã€‚
  
* è®¡ç®—å¤æ‚æ€§ï¼šå¯¹äºéå¸¸å¤§çš„çŸ©é˜µï¼Œè®¡ç®—ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡çš„ç®—æ³•å¯èƒ½éå¸¸è€—æ—¶ã€‚å°¤å…¶æ˜¯åœ¨éœ€è¦é«˜ç²¾åº¦ç»“æœçš„æƒ…å†µä¸‹ï¼Œè®¡ç®—é‡ä¼šæ˜¾è‘—å¢åŠ ã€‚

* Numerical stability issues: Numerical stability issues may arise in the calculation of eigenvalues and eigenvectors when dealing with very large or very small values. This may lead to poor accuracy of the results or numerical instability in the computation.
Computational complexity:

* For very large matrices, the algorithms for computing eigenvalues and eigenvectors can be very time consuming. Especially if high precision results are required, the computational effort can increase significantly.
  
### PCA ä¸»æˆåˆ†åˆ†æ Principal Component Analysis
```
# Step 1: æ•°æ®æ ‡å‡†åŒ–
# è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å‡å€¼å’Œæ ‡å‡†å·®
mean = np.mean(X, axis=0)
std = np.std(X, axis=0)

# æ ‡å‡†åŒ–æ•°æ®
X_normalized = (X - mean) / std

# Step 2: è®¡ç®—åæ–¹å·®çŸ©é˜µ
cov_matrix = np.cov(X_normalized.T)

# Step 3: è®¡ç®—åæ–¹å·®çŸ©é˜µçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)

# Step 4: å¯¹ç‰¹å¾å‘é‡è¿›è¡Œæ’åºï¼Œå¹¶é€‰æ‹©ä¸»æˆåˆ†
# å¯¹ç‰¹å¾å€¼ä»å¤§åˆ°å°æ’åºï¼Œè·å–æ’åºåçš„ç‰¹å¾å€¼çš„ç´¢å¼•
sorted_indices = np.argsort(eigenvalues)[::-1]

# é€‰æ‹©å‰kä¸ªç‰¹å¾å‘é‡ï¼Œkæ˜¯ä½ æƒ³è¦çš„ä¸»æˆåˆ†æ•°é‡
k = 2  # ä¾‹å¦‚ï¼Œé€‰æ‹©å‰2ä¸ªä¸»æˆåˆ†
principal_components = eigenvectors[:, sorted_indices[:k]]

# Step 5: å°†åŸå§‹æ•°æ®è½¬æ¢åˆ°æ–°çš„ç‰¹å¾ç©ºé—´
X_pca = X_normalized.dot(principal_components)

# X_pca æ˜¯é™ç»´åçš„æ•°æ®
```
### ç‰¹å¾åˆ†è§£
- **å®šä¹‰**ï¼šç‰¹å¾åˆ†è§£æ˜¯å°†ä¸€ä¸ªçŸ©é˜µåˆ†è§£ä¸ºç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ã€‚å®ƒåªé€‚ç”¨äºæ–¹é˜µã€‚It only applies to square matrices.
- **ä¼˜ç‚¹**ï¼š
  - æä¾›äº†çŸ©é˜µçš„ç›´æ¥åˆ†è§£ï¼Œå¯ä»¥æ­ç¤ºçŸ©é˜µçš„åŸºæœ¬ç‰¹æ€§ï¼Œå¦‚å¯é€†æ€§ã€ç§©ç­‰ã€‚
  - ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡çš„æ¦‚å¿µåœ¨ç†è§£çº¿æ€§å˜æ¢ä¸­éå¸¸é‡è¦ã€‚
  - Provides a direct decomposition of a matrix that reveals the basic properties of a matrix such as invertibility, rank, etc.
  - The concepts of eigenvalues and eigenvectors are important in understanding linear transformations.
- **ç¼ºç‚¹**ï¼š
  - ä»…é™äºæ–¹é˜µï¼Œä¸èƒ½åº”ç”¨äºéæ–¹é˜µï¼ˆä¾‹å¦‚ï¼Œå¤§å¤šæ•°ç°å®ä¸–ç•Œçš„æ•°æ®é›†æ˜¯éæ–¹é˜µï¼‰ã€‚
  - è®¡ç®—ä¸Šå¯èƒ½ä¸å¤Ÿç¨³å®šï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§å‹çŸ©é˜µã€‚
  - Limited to square matrices, cannot be applied to non-square matrices (e.g., most real-world datasets are non-square).
  - May not be computationally stable, especially for large matrices.

### ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰
- **å®šä¹‰**ï¼šPCAæ˜¯ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œé€šè¿‡æ­£äº¤å˜æ¢å°†å¯èƒ½ç›¸å…³çš„å˜é‡è½¬æ¢ä¸ºçº¿æ€§ä¸ç›¸å…³çš„å˜é‡é›†åˆã€‚å®ƒä¸é™äºæ–¹é˜µï¼Œå¹¶é€šå¸¸ç”¨äºé™ç»´ã€‚
- **ä¼˜ç‚¹**ï¼š
  - å¯ä»¥åº”ç”¨äºä»»ä½•å¤§å°çš„çŸ©é˜µï¼Œç‰¹åˆ«é€‚ç”¨äºé«˜ç»´æ•°æ®é›†ã€‚
  - æœ‰æ•ˆçš„æ•°æ®å‹ç¼©å·¥å…·ï¼Œå¯ä»¥å‡å°‘æ•°æ®é›†çš„ç»´åº¦ï¼ŒåŒæ—¶ä¿ç•™æœ€é‡è¦çš„ä¿¡æ¯ã€‚
  - æœ‰åŠ©äºå»é™¤å™ªå£°ï¼Œå¼ºåŒ–æ•°æ®é›†ä¸­æœ€é‡è¦çš„ä¿¡å·ã€‚
  - Can be applied to matrices of any size and is particularly suitable for high-dimensional datasets.
  - Effective data compression tool that reduces the dimensionality of a dataset while retaining the most important information.
  - Helps to remove noise and enhance the most important signals in the dataset.
- **ç¼ºç‚¹**ï¼š
  - PCAä¾èµ–äºçº¿æ€§å‡è®¾ï¼Œå¯¹äºéçº¿æ€§æ•°æ®ç»“æ„ä¸æ˜¯æœ€ä½³é€‰æ‹©ã€‚
  - ç»“æœçš„è§£é‡Šå¯èƒ½ä¸ç›´è§‚ï¼Œç‰¹åˆ«æ˜¯åœ¨é«˜ç»´æ•°æ®ä¸Šã€‚
  - å¯¹æ•°æ®çš„æ ‡å‡†åŒ–æˆ–è§„èŒƒåŒ–é«˜åº¦æ•æ„Ÿã€‚
  - PCA relies on linear assumptions and is not optimal for non-linear data structures.
  - Interpretation of results may not be intuitive, especially on high-dimensional data.
  - Highly sensitive to standardisation or normalisation of data.

### æ¯”è¾ƒ
- ç‰¹å¾åˆ†è§£æ˜¯PCAçš„æ•°å­¦åŸºç¡€ã€‚å®é™…ä¸Šï¼ŒPCAæ¶‰åŠåˆ°åæ–¹å·®çŸ©é˜µæˆ–æ•°æ®çŸ©é˜µçš„å¥‡å¼‚å€¼åˆ†è§£ï¼ˆSVDï¼‰ï¼Œè¿™å¯ä»¥è§†ä¸ºç‰¹å¾åˆ†è§£çš„ä¸€ç§å½¢å¼ã€‚
- PCAé€šå¸¸è¢«è§†ä¸ºç‰¹å¾åˆ†è§£åœ¨æ•°æ®åˆ†æå’Œé™ç»´æ–¹é¢çš„å®é™…åº”ç”¨ã€‚
- åœ¨å®é™…æ“ä½œä¸­ï¼ŒPCAæ›´åŠ æ™®éï¼Œå› ä¸ºå®ƒé€‚ç”¨äºéæ–¹é˜µï¼Œå¹¶ä¸”ä¸ç‰¹å®šçš„åº”ç”¨ï¼ˆå¦‚æ•°æ®å‹ç¼©ã€ç‰¹å¾æå–ã€å™ªå£°å‡å°‘ï¼‰å¯†åˆ‡ç›¸å…³ã€‚In practice, PCA is more common because it is applicable to non-square matrices and is closely related to specific applications (e.g., data compression, feature extraction, noise reduction).

### Trace(è¿¹)
The trace of a square matrix can be computed from the sum of its diagonal values:

$$ \text{Tr}(A) = a_{1,1} + a_{2,2} + \dots + a_{n,n} $$

It is also equal to the sum of the eigenvalues of $A$

$$ \text{Tr}(A) = \sum_{i=1}^n \lambda_i $$

The trace can be thought of as measuring the  **perimeter** of the parallelotope of a unit cube transformed by the matrix. [Strictly, it is *proportional* to the perimeter, with the constant of proportionality being $\text{Perimiter}(A)=2^{n-1} \text{Tr}(A)$].

### Determinant è¡Œåˆ—å¼
The determinant $\text{det}(A)$ is an important property of square matrices. It can be thought of as the **volume** of the parallelotope  of a unit cube transformed by the matrix -- it measures how much the space expands or contracts after the linear transform.

It is equal to the product of the eigenvalues of the matrix.

$$ \text{det}(A) = \prod_{i=1}^n \lambda_i  $$

If any eigenvalue $\lambda_i$ of $A$ is 0, the determinant $\det(A)=0$, and the transformation collapses at least one dimension to be completely flat. This means that the transformation **cannot be reversed**; information has been lost.

### Definite and semi-definite matrices

A matrix is called 

- **positive definite** if all of its eigenvalues are greater than zero: $\lambda_i > 0$. æ­£å®šçŸ©é˜µ

- **positive semi-definite** if all of its eigenvalues are greater than or equal to zero: $\lambda_i \geq 0$.  åŠæ­£å®šçŸ©é˜µ

- **negative definite** if all of the eigenvalues are less than zero: $\lambda_i < 0$, -  è´Ÿå®šçŸ©é˜µ

- **negative semi-definite** if all the eigenvalues are less than or equal to zero: $\lambda_i \leq 0$. åŠè´Ÿå®šçŸ©é˜µ

åœ¨ç»Ÿè®¡æ•°æ®åˆ†æä¸­ï¼Œåæ–¹å·®çŸ©é˜µé€šå¸¸æ˜¯æ­£å®šçš„ï¼Œè¿™æ„å‘³ç€å®ƒä¿ç•™äº†æ•°æ®çš„æŸäº›åŸºæœ¬æ€§è´¨å’Œç»“æ„ã€‚æ¢å¥è¯è¯´ï¼Œæ­£å®šçŸ©é˜µåœ¨å˜æ¢ç©ºé—´ä¸­ä¿æŒäº†å‘é‡çš„â€œæ–¹å‘â€ã€‚åœ¨æ•°å€¼ä¼˜åŒ–ä¸­ï¼Œæµ·æ£®çŸ©é˜µè¢«ç”¨äºåˆ¤æ–­å‡½æ•°çš„å‡¸æ€§ï¼Œå…¶æ­£å®šæ€§æ˜¯æ‰¾åˆ°æœ€ä¼˜è§£çš„é‡è¦æ¡ä»¶ã€‚In statistical data analysis, the covariance matrix is usually positive definite, which means that it preserves some of the fundamental properties and structure of the data. In other words, a positive definite matrix maintains the "direction" of the vectors in the transformation space. In numerical optimisation, the Hessian matrix is used to determine the convexity of a function, and its positive definiteness is important for finding the optimal solution.

### Matrix Inversion
We have seen four basic algebraic operations on matrices:
* scalar multiplication $cA$;
* matrix addition $A+B$;
* matrix multiplication $BA$
* matrix transposition $A^T$

There is a further important operation: **inversion** $A^{-1}$, defined such that:
* $A^{-1}(A\vec{x}) = \vec{x}$, 
* $A^{-1}A = I$ 
* $(A^{-1})^{-1} = A$
* $(AB)^{-1} = B^{-1}A^{-1}$

`np.linalg.inv()`  çŸ©é˜µå¯é€†çš„æ¡ä»¶ä¸ºæ˜¯æ–¹é˜µä¸”det(A)ä¹Ÿå°±æ˜¯è¡Œåˆ—å¼ä¸ä¸º0 det(ğ´)â‰ 0ã€‚å¦‚æœ det(A)=0 ï¼Œé‚£ä¹ˆå˜æ¢ A è‡³å°‘æŠ˜å äº†ä¸€ä¸ªç»´åº¦ï¼Œè¿™æ„å‘³ç€å®ƒä¸æ˜¯åŒå°„çš„ã€‚ If det(A)=0, then the transformation A collapses at least one dimension, whichmeans it's not bijective.

### å¥‡å¼‚çŸ©é˜µ ä¸ éå¥‡å¼‚çŸ©é˜µ Singular and non-singular matrices

A matrix with $\det(A)=0$ is called **singular** and has no inverse.

A matrix which is invertible is called **non-singular**. 

The geometric intuition for this is simple. Going back to the paralleogram model, a matrix with zero determinant has at least one zero eigenvalue. This means that at least one of the dimensions of the parallelepiped has been squashed to nothing at all. Therefore it is impossible to reverse the transformation, because information was lost in the forward transform. 

All of the original dimensions must be preserved in a linear map for inversion to be meaningful; this is the same as saying $\det(A) \neq 0$.


## BUT only square matrices can be inverted !

Inversion is only defined for square matrices, representing a linear transform $\real^n \rightarrow \real^n$. This is equivalent to saying that the determinant of the matrix must be non-zero: $\det(A) \neq 0$. Why?

A matrix which is non-square maps vectors of dimension $m$ to dimension $n$. This means the transformation collapses or creates dimensions. Such a transformation is not uniquely reversible.

For a matrix to be invertible it must represent a **bijection** (a function that maps every member of a set onto exactly one member of another set).

## Singular and non-singular matrices

A matrix with $\det(A)=0$ is called **singular** and has no inverse.

A matrix which is invertible is called **non-singular**. 

The geometric intuition for this is simple. Going back to the paralleogram model, a matrix with zero determinant has at least one zero eigenvalue. This means that at least one of the dimensions of the parallelepiped has been squashed to nothing at all. Therefore it is impossible to reverse the transformation, because information was lost in the forward transform. 

All of the original dimensions must be preserved in a linear map for inversion to be meaningful; this is the same as saying $\det(A) \neq 0$.

<img src="imgs/inversion.png" width=50%>

### Time complexity
Matrix inversion, for a general $n \times n$ matrix, takes $O(n^3)$ time. It is *provable* that no general matrix inversion algorithm can ever be faster than $O(n^3)$ (one of the few problems for which a tight polynomial time bound is known). çŸ©é˜µè¿ç®—æ¶‰åŠå¤§é‡é‡å¤çš„æµ®ç‚¹è¿ç®—ï¼ˆèˆå…¥ç´¯åŠ ï¼‰--åæ¼”ç‰¹åˆ«éš¾ä»¥ç›´æ¥ä»¥ç¨³å®šçš„å½¢å¼è®¡ç®—ï¼Œè®¸å¤šç†è®ºä¸Šå¯ä»¥åæ¼”çš„çŸ©é˜µæ— æ³•ä½¿ç”¨æµ®ç‚¹è¡¨ç¤ºæ³•è¿›è¡Œåæ¼”ã€‚æ—¶é—´å¤æ‚æ€§--å¯¹äºä¸€èˆ¬çš„ğ‘›Ã—ğ‘›çŸ©é˜µï¼ŒçŸ©é˜µåæ¼”éœ€è¦ğ‘‚(ğ‘›3) æ—¶é—´ã€‚å¯ä»¥è¯æ˜ï¼Œæ²¡æœ‰ä¸€ç§é€šç”¨çŸ©é˜µåæ¼”ç®—æ³•èƒ½æ¯” ğ‘‚(ğ‘›3)æ›´å¿«ã€‚

### Special cases
* orthogonal matrix (rows and columns are all orthogonal unit vectors): $O(1)$, $A^{-1}= A^T$ æ­£äº¤çŸ©é˜µ
* diagonal matrix (all non-diagonal elements are zero): $O(n)$, $A^{-1} = \frac{1}{A}$ (i.e. the reciprocal of the diagonal elements of $A$). å¯¹è§’çŸ©é˜µ
* positive-definite matrix: $O(n^2)$ via the *Cholesky decomposition*. We won't discuss this further.æ­£å®šçŸ©é˜µ
* triangular matrix (all elements either above or below the main diagonal are zero): $O(n^2)$, trivially invertible by **elimination algorithms**.  ä¸‰è§’å½¢çŸ©é˜µï¼ˆä¸»å¯¹è§’çº¿ä¸Šä¸‹çš„æ‰€æœ‰å…ƒç´ å‡ä¸ºé›¶ï¼‰ï¼š O(n^2)$ï¼Œé€šè¿‡**æ¶ˆé™¤ç®—æ³•**å¯åè½¬ã€‚

## SVD Singular Value Decomposition å¥‡å¼‚å€¼åˆ†è§£
The **singular value decomposition** (SVD) is a general approach to decomposing any matrix $A$. It is the powerhouse of computational linear algebra.

The SVD produces a decomposition which splits ***ANY*** matrix up into three matrices:
$$A = U \Sigma V^T $$
where 
* $A$ is any $m \times n$ matrix, 
* $U$ is a **square unitary** $m \times m$ matrix, whose columns contain the **left singular vectors**, å·¦å¥‡å¼‚å‘é‡
* $V$ is an **square unitary** $n \times n$ matrix, whose columns contain the **right singular vectors**, å³å¥‡å¼‚å‘é‡
* $\Sigma$ is a diagonal $m \times n$ matrix, whose diagonal contains the **singular values**. å…¶å¯¹è§’çº¿ä¸Šçš„å…ƒç´ æ˜¯å¥‡å¼‚å€¼ï¼Œè¿™äº›å¥‡å¼‚å€¼æ˜¯çŸ©é˜µ A çš„éè´Ÿå®æ•°ï¼Œé€šå¸¸æŒ‰é™åºæ’åˆ—ã€‚

ä¸€ä¸ª**å•å…ƒ**çŸ©é˜µçš„å…±è½­è½¬ç½®ç­‰äºå…¶é€†çŸ©é˜µã€‚å¦‚æœ $A$ ä¸ºå®æ•°ï¼Œé‚£ä¹ˆ $U$ å’Œ $V$ å°†æ˜¯**æ­£äº¤**çŸ©é˜µï¼ˆ$U^T = U^{-1}$ï¼‰ï¼Œå…¶è¡Œéƒ½å…·æœ‰å•ä½çŸ©ï¼Œå…¶åˆ—ä¹Ÿéƒ½å…·æœ‰å•ä½çŸ©ã€‚A **unitary** matrix is one whose conjugate transpose is equal to its inverse. If $A$ is real, then $U$ and $V$ will be **orthogonal** matrices ($U^T = U^{-1}$), whose rows all have unit norm and whose columns also all have unit norm. 

çŸ©é˜µ $\Sigma$ çš„å¯¹è§’çº¿æ˜¯**å¥‡å¼‚å€¼**çš„é›†åˆï¼Œä¸ç‰¹å¾å€¼å¯†åˆ‡ç›¸å…³ï¼Œä½†**å¹¶ä¸å®Œå…¨ç›¸åŒï¼ˆç‰¹æ®Šæƒ…å†µé™¤å¤–ï¼Œå¦‚å½“ $A$ æ˜¯æ­£åŠæœ‰é™å¯¹ç§°çŸ©é˜µæ—¶ï¼‰ï¼å¥‡å¼‚å€¼**æ€»æ˜¯æ­£å®æ•°ã€‚The diagonal of the matrix $\Sigma$ is the set of **singular values**, which are closely related to the eigenvalues, but are *not* quite the same thing (except for special cases like when $A$ is a positive semi-definite symmetric matrix)! The **singular values** are always positive real numbers.

We can compute the SVD with `np.linalg.svd`:

### ä¸PCAçš„å…³ç³»
PCAæ˜¯ä¸€ç§ç»Ÿè®¡æ–¹æ³•ï¼Œç”¨äºé€šè¿‡æ­£äº¤å˜æ¢å°†å¯èƒ½ç›¸å…³çš„å˜é‡è½¬æ¢ä¸ºçº¿æ€§ä¸ç›¸å…³çš„å˜é‡é›†åˆï¼Œé€šå¸¸ç”¨äºé™ç»´ã€‚
  - PCAå¯ä»¥é€šè¿‡SVDå®ç°ã€‚å…·ä½“æ¥è¯´ï¼Œå¯¹æ•°æ®çŸ©é˜µ \( A \) è¿›è¡ŒSVDï¼Œå¾—åˆ° $( A = U\Sigma V^T )$ã€‚
  - åœ¨PCAä¸­ï¼Œä¸»æˆåˆ†å°±æ˜¯SVDä¸­çš„å³å¥‡å¼‚å‘é‡ \( V \)ï¼ˆæˆ– \( V^T \) çš„è¡Œï¼‰ã€‚
  - é€šè¿‡SVDï¼Œå¯ä»¥æ›´ç¨³å®šå’Œé«˜æ•ˆåœ°è®¡ç®—PCAï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†å¤§å‹æ•°æ®é›†æ—¶ã€‚

###  Fractional powers
We can use the SVD to compute interesting matrix functions like the square root of a matrix $A^{1/2}$. 
$$ A^n = U \Sigma^n V^T $$

**Note: $A^{1/2}$ is not the elementwise square root of each element of A!** 

Rather, we must compute the elementwise square root of $\Sigma$, then compute $A^{1/2} = U \Sigma^{1/2} V^T$.

### Inversion - relation to SVD
We can efficiently invert a matrix once it is in SVD form. For a non-symmetric matrix, we use:
$$A ^{-1} = V \Sigma^{-1} U^T$$

### Rank of a matrix çŸ©é˜µçš„ç§©

çŸ©é˜µçš„**ç§©**ç­‰äºéé›¶å¥‡å¼‚å€¼çš„ä¸ªæ•°ã€‚The **rank** of a matrix is equal to the number of non-zero singular values. 

* å¦‚æœéé›¶å¥‡å¼‚å€¼çš„ä¸ªæ•°ç­‰äºçŸ©é˜µçš„å¤§å°ï¼Œé‚£ä¹ˆçŸ©é˜µå°±æ˜¯**å…¨ç§©**ã€‚
* å…¨ç§©çŸ©é˜µçš„è¡Œåˆ—å¼ä¸ä¸ºé›¶ï¼Œå¹¶ä¸”å¯ä»¥åè½¬ã€‚
* ç§©å‘Šè¯‰æˆ‘ä»¬å˜æ¢æ‰€ä»£è¡¨çš„å¹³è¡ŒçŸ©é˜µæœ‰å¤šå°‘ç»´ã€‚
* å¦‚æœçŸ©é˜µæ²¡æœ‰æ»¡ç§©ï¼Œå®ƒå°±æ˜¯**å¥‡æ•°**ï¼ˆä¸å¯åè½¬ï¼‰ï¼Œå…·æœ‰**ç¼ºç§©**ã€‚
* å¦‚æœéé›¶å¥‡å¼‚å€¼çš„æ•°é‡è¿œå°äºçŸ©é˜µçš„å¤§å°ï¼Œåˆ™è¯¥çŸ©é˜µä¸º**ä½é˜¶çŸ©é˜µ**ã€‚

### Condition number of a matrix çŸ©é˜µçš„æ¡ä»¶æ•°
çŸ©é˜µçš„**æ¡ä»¶æ•°**æ˜¯æœ€å¤§å¥‡å¼‚å€¼ä¸æœ€å°å¥‡å¼‚å€¼çš„æ¯”å€¼ã€‚The **condition number** number of a matrix is the ratio of the largest singular value to the smallest. 
* è¿™åªé’ˆå¯¹å…¨ç§©çŸ©é˜µã€‚
* æ¡ä»¶æ•°è¡¡é‡çŸ©é˜µçš„åè½¬å¯¹å¾®å°å˜åŒ–çš„æ•æ„Ÿç¨‹åº¦ã€‚
* æ¡ä»¶æ•°å°çš„çŸ©é˜µç§°ä¸º**æ¡ä»¶è‰¯å¥½**ï¼Œä¸å¤ªå¯èƒ½å¼•èµ·æ•°å€¼é—®é¢˜ã€‚
* æ¡ä»¶æ•°å¤§çš„çŸ©é˜µæ˜¯**æ¡ä»¶å·®çš„**ï¼Œæ•°å€¼é—®é¢˜å¯èƒ½ä¼šå¾ˆä¸¥é‡ã€‚
  
æ¡ä»¶ä¸è‰¯çš„çŸ©é˜µå‡ ä¹æ˜¯å¥‡å¼‚çš„ï¼Œå› æ­¤å¯¹å…¶è¿›è¡Œåæ¼”å°†å¯¼è‡´æ— æ•ˆç»“æœï¼Œå› ä¸ºæµ®ç‚¹èˆå…¥é”™è¯¯ã€‚

### ç™½åŒ– Whitening
### ä½¿ç”¨SVDè¿›è¡Œç™½åŒ–çš„æ­¥éª¤
1. **ä¸­å¿ƒåŒ–æ•°æ®**ï¼š
   - å¯¹æ•°æ®è¿›è¡Œä¸­å¿ƒåŒ–ï¼Œå³ä»æ¯ä¸ªç‰¹å¾ä¸­å‡å»å…¶å‡å€¼ã€‚è¿™æ ·åšçš„ç›®çš„æ˜¯ä½¿æ•°æ®åœ¨å„ä¸ªç»´åº¦ä¸Šçš„å¹³å‡å€¼ä¸ºé›¶ã€‚

2. **è®¡ç®—SVD**ï¼š
   - å¯¹ä¸­å¿ƒåŒ–åçš„æ•°æ®çŸ©é˜µ $X$ è¿›è¡Œå¥‡å¼‚å€¼åˆ†è§£ã€‚SVDå°†æ•°æ®çŸ©é˜µåˆ†è§£ä¸ºä¸‰ä¸ªçŸ©é˜µçš„ä¹˜ç§¯ï¼š$X = U \Sigma V^T 
   $
   - å…¶ä¸­ï¼Œ$U$ å’Œ $V$ æ˜¯æ­£äº¤çŸ©é˜µï¼Œè€Œ $\Sigma$ æ˜¯å¯¹è§’çŸ©é˜µï¼Œå¯¹è§’çº¿ä¸Šçš„å…ƒç´ æ˜¯å¥‡å¼‚å€¼ã€‚

1. **æ„å»ºç™½åŒ–çŸ©é˜µ**ï¼š
   - ç™½åŒ–çŸ©é˜µé€šå¸¸æ˜¯é€šè¿‡å–  $\Sigma$  ä¸­å¥‡å¼‚å€¼çš„é€†å¹³æ–¹æ ¹æ¥æ„å»ºçš„ï¼Œè®°ä¸º  $\Sigma^{-\frac{1}{2}}$ ã€‚

2. **åº”ç”¨ç™½åŒ–å˜æ¢**ï¼š
   - ä½¿ç”¨ç™½åŒ–çŸ©é˜µå¯¹åŸå§‹æ•°æ®çŸ©é˜µ $X$ è¿›è¡Œå˜æ¢ï¼Œå¾—åˆ°ç™½åŒ–åçš„æ•°æ® $X_{\text{whitened}} = U \Sigma^{-1/2} U^T X 
    $

### ç™½åŒ–çš„ç›®çš„
- **å»é™¤ç›¸å…³æ€§** **Removal of correlation**ï¼š
  - ç™½åŒ–çš„ä¸»è¦ç›®çš„æ˜¯å»é™¤æ•°æ®ç‰¹å¾é—´çš„ç›¸å…³æ€§ã€‚é€šè¿‡è¿™ç§å˜æ¢ï¼Œæ•°æ®çš„åæ–¹å·®çŸ©é˜µå°†å˜ä¸ºå•ä½çŸ©é˜µï¼Œæ„å‘³ç€å˜æ¢åçš„ç‰¹å¾å½¼æ­¤ç»Ÿè®¡ç‹¬ç«‹ã€‚The main purpose of whitening is to remove the correlation between the features of the data. With this transformation, the covariance matrix of the data is changed to a unit matrix, meaning that the transformed features are statistically independent of each other.

- **æ–¹å·®å½’ä¸€åŒ–** **Covariance Normalisation**ï¼š
  - ç™½åŒ–è¿‡ç¨‹ä¸­ï¼Œæ¯ä¸ªç‰¹å¾çš„æ–¹å·®éƒ½è¢«æ ‡å‡†åŒ–ä¸º1ã€‚è¿™æ ·åšå¯ä»¥ç¡®ä¿æ²¡æœ‰ä»»ä½•ä¸€ä¸ªç‰¹å¾åœ¨æ•°å€¼ä¸Šä¸»å¯¼æ•´ä¸ªæ•°æ®é›†ã€‚During whitening, the variance of each feature is normalised to 1. This ensures that no single feature numerically dominates the entire data set.

- **æ”¹å–„ç®—æ³•æ€§èƒ½** **Improved algorithm performance**ï¼š
  - ç™½åŒ–åçš„æ•°æ®æœ‰åŠ©äºæ”¹å–„è®¸å¤šæœºå™¨å­¦ä¹ ç®—æ³•çš„æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ¶‰åŠè·ç¦»è®¡ç®—çš„ç®—æ³•ï¼ˆå¦‚k-meansèšç±»ï¼‰å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ã€‚Whitened data helps to improve the performance of many machine learning algorithms, especially in algorithms that involve distance calculations (e.g. k-means clustering) and deep learning models.

- **æ•°æ®å¯è§†åŒ–å’Œè¿›ä¸€æ­¥å¤„ç†** **Data visualisation and further processing**ï¼š
  - ç™½åŒ–å¤„ç†å¯ä»¥ä½¿æ•°æ®æ›´é€‚åˆè¿›è¡Œå¯è§†åŒ–å’Œè¿›ä¸€æ­¥çš„åˆ†æå¤„ç†ï¼Œå› ä¸ºå®ƒç¡®ä¿äº†æ•°æ®åœ¨æ‰€æœ‰ç»´åº¦ä¸Šå…·æœ‰ç›¸ä¼¼çš„è§„æ¨¡ã€‚Whitening makes the data more suitable for visualisation and further analytical processing as it ensures that the data has a similar scale in all dimensions.
